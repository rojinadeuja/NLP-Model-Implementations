{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x'''\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def softmax_1(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x'''\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / np.sum(e_x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(self, word):\n",
    "    '''Function to covert a word to one-hot-encoded value'''\n",
    "    word_vec = [0 for i in range(0, self.v_count)]\n",
    "    word_index = self.word_index[word]\n",
    "    word_vec[word_index] = 1\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec(object):\n",
    "    def __init__(self):\n",
    "        self.N = 10\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.001\n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "    \n",
    "    def initialize(self,V,data):\n",
    "        self.V = V\n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "            \n",
    "    def feed_forward(self, X): \n",
    "        self.h = np.dot(self.W.T, X).reshape(self.N, 1) \n",
    "        self.u = np.dot(self.W1.T, self.h)\n",
    "        self.y = softmax(self.u)   \n",
    "        return self.y\n",
    "    \n",
    "    def backpropagate(self, x, t):\n",
    "        e = self.y - np.asarray(t).reshape(self.V, 1) #e.shape is V X 1\n",
    "        # Calculate partial derivative of loss function wrt W1\n",
    "        dEdW1 = np.dot(self.h, e.T)\n",
    "        \n",
    "        X = np.array(x).reshape(self.V, 1)\n",
    "        # Calculate partial derivative of loss function wrt W\n",
    "        dEdW = np.dot(X, np.dot(self.W1, e).T)\n",
    "        \n",
    "        # Update the weights\n",
    "        self.W1 = self.W1 - self.alpha * dEdW1\n",
    "        self.W = self.W - self.alpha * dEdW\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        # Loop through each epoch\n",
    "        for x in range(1,epochs):\n",
    "            \n",
    "            # Initialize Loss\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Loop through each training sample\n",
    "            for j in range(len(self.X_train)):\n",
    "                # Forward Pass\n",
    "                self.feed_forward(self.X_train[j])\n",
    "                \n",
    "                # Backpropagation\n",
    "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                        \n",
    "                # Calculate Loss        \n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u)))\n",
    "                \n",
    "            print(\"Epoch: \", x, \" Loss: \", self.loss)\n",
    "            self.alpha *= 1/((1+self.alpha*x))\n",
    "            \n",
    "    def predict(self, word, number_of_predictions):       \n",
    "        # Check if word is contained in the dictionary\n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X)\n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i\n",
    "                \n",
    "            # Sort top context words in the output    \n",
    "            sorted_output = [] \n",
    "            for k in sorted(output, reverse=True): \n",
    "                sorted_output.append(self.words[output[k]]) \n",
    "                if(len(sorted_output)>=number_of_predictions): \n",
    "                    break\n",
    "            return sorted_output \n",
    "        else: \n",
    "            print(\"Error: Word not found in dicitonary\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    processed = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    # Split text corpus into sentences\n",
    "    sentences = corpus.split(\".\")\n",
    "    # Loop through each sentence\n",
    "    for i in range(len(sentences)):\n",
    "        # Remove leading and trailing characters\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        # Split sentence into list of words\n",
    "        sentence = sentences[i].split()\n",
    "        # Remove punctuations\n",
    "        x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "        # Convert to lowe\n",
    "        x = [word.lower() for word in x] \n",
    "        processed.append(x) \n",
    "    return processed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sentences, w2v):\n",
    "    data = {}\n",
    "    # Loop throuch each sentence\n",
    "    for sentence in sentences:\n",
    "        # Loop through each word\n",
    "        for word in sentence: \n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) # Size of Vocabulary\n",
    "    data = sorted(list(data.keys()))\n",
    "    \n",
    "    vocab = {}\n",
    "    # Store words into vocabulary\n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "       \n",
    "    # Loop through each sentence \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)] \n",
    "              \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context) \n",
    "    w2v.initialize(V,data) \n",
    "    return w2v.X_train,w2v.y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  54.72114233862495\n",
      "Epoch:  2  Loss:  54.6483665924774\n",
      "Epoch:  3  Loss:  54.57589018858346\n",
      "Epoch:  4  Loss:  54.503782754863494\n",
      "Epoch:  5  Loss:  54.432112203572494\n",
      "Epoch:  6  Loss:  54.36094435345277\n",
      "Epoch:  7  Loss:  54.290342579033975\n",
      "Epoch:  8  Loss:  54.22036749155833\n",
      "Epoch:  9  Loss:  54.1510766552335\n",
      "Epoch:  10  Loss:  54.08252434166894\n",
      "Epoch:  11  Loss:  54.01476132446768\n",
      "Epoch:  12  Loss:  53.947834715054206\n",
      "Epoch:  13  Loss:  53.8817878399533\n",
      "Epoch:  14  Loss:  53.81666015891921\n",
      "Epoch:  15  Loss:  53.75248722257378\n",
      "Epoch:  16  Loss:  53.68930066756233\n",
      "Epoch:  17  Loss:  53.62712824669158\n",
      "Epoch:  18  Loss:  53.565993891080275\n",
      "Epoch:  19  Loss:  53.50591780103166\n",
      "Epoch:  20  Loss:  53.44691656212924\n",
      "Epoch:  21  Loss:  53.3890032829488\n",
      "Epoch:  22  Loss:  53.33218775077104\n",
      "Epoch:  23  Loss:  53.27647660174998\n",
      "Epoch:  24  Loss:  53.22187350213465\n",
      "Epoch:  25  Loss:  53.16837933734076\n",
      "Epoch:  26  Loss:  53.11599240591044\n",
      "Epoch:  27  Loss:  53.06470861567084\n",
      "Epoch:  28  Loss:  53.014521679693736\n",
      "Epoch:  29  Loss:  52.96542330995702\n",
      "Epoch:  30  Loss:  52.91740340690812\n",
      "Epoch:  31  Loss:  52.870450243420436\n",
      "Epoch:  32  Loss:  52.824550641909674\n",
      "Epoch:  33  Loss:  52.77969014363768\n",
      "Epoch:  34  Loss:  52.73585316946669\n",
      "Epoch:  35  Loss:  52.693023171543395\n",
      "Epoch:  36  Loss:  52.65118277558114\n",
      "Epoch:  37  Loss:  52.610313913576036\n",
      "Epoch:  38  Loss:  52.570397946935195\n",
      "Epoch:  39  Loss:  52.53141578011656\n",
      "Epoch:  40  Loss:  52.49334796497871\n",
      "Epoch:  41  Loss:  52.456174796120784\n",
      "Epoch:  42  Loss:  52.41987639755517\n",
      "Epoch:  43  Loss:  52.38443280110407\n",
      "Epoch:  44  Loss:  52.349824016945064\n",
      "Epoch:  45  Loss:  52.316030096753735\n",
      "Epoch:  46  Loss:  52.283031189904136\n",
      "Epoch:  47  Loss:  52.25080759319194\n",
      "Epoch:  48  Loss:  52.219339794542634\n",
      "Epoch:  49  Loss:  52.188608511158954\n",
      "Epoch:  50  Loss:  52.158594722548735\n",
      "Epoch:  51  Loss:  52.12927969885853\n",
      "Epoch:  52  Loss:  52.10064502491942\n",
      "Epoch:  53  Loss:  52.072672620390875\n",
      "Epoch:  54  Loss:  52.04534475636707\n",
      "Epoch:  55  Loss:  52.01864406878764\n",
      "Epoch:  56  Loss:  51.99255356897203\n",
      "Epoch:  57  Loss:  51.96705665157512\n",
      "Epoch:  58  Loss:  51.94213710023862\n",
      "Epoch:  59  Loss:  51.917779091192884\n",
      "Epoch:  60  Loss:  51.89396719504197\n",
      "Epoch:  61  Loss:  51.87068637694672\n",
      "Epoch:  62  Loss:  51.84792199540066\n",
      "Epoch:  63  Loss:  51.82565979977744\n",
      "Epoch:  64  Loss:  51.803885926811205\n",
      "Epoch:  65  Loss:  51.7825868961568\n",
      "Epoch:  66  Loss:  51.76174960516187\n",
      "Epoch:  67  Loss:  51.74136132297056\n",
      "Epoch:  68  Loss:  51.7214096840656\n",
      "Epoch:  69  Loss:  51.701882681345054\n",
      "Epoch:  70  Loss:  51.68276865881973\n",
      "Epoch:  71  Loss:  51.66405630400736\n",
      "Epoch:  72  Loss:  51.645734640092115\n",
      "Epoch:  73  Loss:  51.62779301790895\n",
      "Epoch:  74  Loss:  51.61022110780689\n",
      "Epoch:  75  Loss:  51.593008891437215\n",
      "Epoch:  76  Loss:  51.576146653508204\n",
      "Epoch:  77  Loss:  51.5596249735418\n",
      "Epoch:  78  Loss:  51.543434717663885\n",
      "Epoch:  79  Loss:  51.52756703045442\n",
      "Epoch:  80  Loss:  51.51201332688115\n",
      "Epoch:  81  Loss:  51.49676528433645\n",
      "Epoch:  82  Loss:  51.48181483479368\n",
      "Epoch:  83  Loss:  51.46715415709775\n",
      "Epoch:  84  Loss:  51.452775669400964\n",
      "Epoch:  85  Loss:  51.43867202175369\n",
      "Epoch:  86  Loss:  51.42483608885771\n",
      "Epoch:  87  Loss:  51.41126096298811\n",
      "Epoch:  88  Loss:  51.397939947087735\n",
      "Epoch:  89  Loss:  51.384866548037955\n",
      "Epoch:  90  Loss:  51.37203447010761\n",
      "Epoch:  91  Loss:  51.35943760858064\n",
      "Epoch:  92  Loss:  51.3470700435635\n",
      "Epoch:  93  Loss:  51.33492603397093\n",
      "Epoch:  94  Loss:  51.32300001168944\n",
      "Epoch:  95  Loss:  51.3112865759164\n",
      "Epoch:  96  Loss:  51.299780487672756\n",
      "Epoch:  97  Loss:  51.28847666448658\n",
      "Epoch:  98  Loss:  51.27737017524457\n",
      "Epoch:  99  Loss:  51.26645623520821\n",
      "Epoch:  100  Loss:  51.25573020119137\n",
      "Epoch:  101  Loss:  51.24518756689527\n",
      "Epoch:  102  Loss:  51.23482395839764\n",
      "Epoch:  103  Loss:  51.22463512979157\n",
      "Epoch:  104  Loss:  51.214616958970765\n",
      "Epoch:  105  Loss:  51.204765443556575\n",
      "Epoch:  106  Loss:  51.19507669696332\n",
      "Epoch:  107  Loss:  51.185546944597625\n",
      "Epoch:  108  Loss:  51.17617252018781\n",
      "Epoch:  109  Loss:  51.166949862239335\n",
      "Epoch:  110  Loss:  51.1578755106126\n",
      "Epoch:  111  Loss:  51.14894610321876\n",
      "Epoch:  112  Loss:  51.14015837283034\n",
      "Epoch:  113  Loss:  51.13150914400226\n",
      "Epoch:  114  Loss:  51.12299533010021\n",
      "Epoch:  115  Loss:  51.11461393043237\n",
      "Epoch:  116  Loss:  51.106362027481005\n",
      "Epoch:  117  Loss:  51.09823678423075\n",
      "Epoch:  118  Loss:  51.09023544159\n",
      "Epoch:  119  Loss:  51.08235531590239\n",
      "Epoch:  120  Loss:  51.074593796544804\n",
      "Epoch:  121  Loss:  51.06694834360949\n",
      "Epoch:  122  Loss:  51.05941648566664\n",
      "Epoch:  123  Loss:  51.05199581760516\n",
      "Epoch:  124  Loss:  51.04468399854824\n",
      "Epoch:  125  Loss:  51.03747874984151\n",
      "Epoch:  126  Loss:  51.030377853111204\n",
      "Epoch:  127  Loss:  51.0233791483891\n",
      "Epoch:  128  Loss:  51.01648053230285\n",
      "Epoch:  129  Loss:  51.00967995632838\n",
      "Epoch:  130  Loss:  51.00297542510283\n",
      "Epoch:  131  Loss:  50.996364994795286\n",
      "Epoch:  132  Loss:  50.989846771533585\n",
      "Epoch:  133  Loss:  50.98341890988493\n",
      "Epoch:  134  Loss:  50.97707961138835\n",
      "Epoch:  135  Loss:  50.97082712313728\n",
      "Epoch:  136  Loss:  50.964659736410404\n",
      "Epoch:  137  Loss:  50.95857578534857\n",
      "Epoch:  138  Loss:  50.95257364567687\n",
      "Epoch:  139  Loss:  50.94665173346953\n",
      "Epoch:  140  Loss:  50.94080850395647\n",
      "Epoch:  141  Loss:  50.93504245036983\n",
      "Epoch:  142  Loss:  50.929352102829\n",
      "Epoch:  143  Loss:  50.92373602726297\n",
      "Epoch:  144  Loss:  50.91819282436829\n",
      "Epoch:  145  Loss:  50.912721128601774\n",
      "Epoch:  146  Loss:  50.90731960720617\n",
      "Epoch:  147  Loss:  50.901986959268164\n",
      "Epoch:  148  Loss:  50.89672191480721\n",
      "Epoch:  149  Loss:  50.891523233894034\n",
      "Epoch:  150  Loss:  50.88638970579796\n",
      "Epoch:  151  Loss:  50.88132014816202\n",
      "Epoch:  152  Loss:  50.876313406204524\n",
      "Epoch:  153  Loss:  50.87136835194664\n",
      "Epoch:  154  Loss:  50.86648388346459\n",
      "Epoch:  155  Loss:  50.861658924165894\n",
      "Epoch:  156  Loss:  50.85689242208879\n",
      "Epoch:  157  Loss:  50.85218334922374\n",
      "Epoch:  158  Loss:  50.84753070085663\n",
      "Epoch:  159  Loss:  50.842933494932566\n",
      "Epoch:  160  Loss:  50.83839077143975\n",
      "Epoch:  161  Loss:  50.83390159181269\n",
      "Epoch:  162  Loss:  50.82946503835404\n",
      "Epoch:  163  Loss:  50.825080213674276\n",
      "Epoch:  164  Loss:  50.82074624014899\n",
      "Epoch:  165  Loss:  50.81646225939288\n",
      "Epoch:  166  Loss:  50.81222743174965\n",
      "Epoch:  167  Loss:  50.808040935798054\n",
      "Epoch:  168  Loss:  50.803901967872505\n",
      "Epoch:  169  Loss:  50.79980974159866\n",
      "Epoch:  170  Loss:  50.79576348744278\n",
      "Epoch:  171  Loss:  50.79176245227505\n",
      "Epoch:  172  Loss:  50.7878058989457\n",
      "Epoch:  173  Loss:  50.78389310587389\n",
      "Epoch:  174  Loss:  50.78002336664913\n",
      "Epoch:  175  Loss:  50.776195989644194\n",
      "Epoch:  176  Loss:  50.77241029763976\n",
      "Epoch:  177  Loss:  50.76866562746007\n",
      "Epoch:  178  Loss:  50.76496132961914\n",
      "Epoch:  179  Loss:  50.761296767977605\n",
      "Epoch:  180  Loss:  50.75767131940941\n",
      "Epoch:  181  Loss:  50.754084373478065\n",
      "Epoch:  182  Loss:  50.75053533212274\n",
      "Epoch:  183  Loss:  50.747023609352695\n",
      "Epoch:  184  Loss:  50.74354863095132\n",
      "Epoch:  185  Loss:  50.74010983418783\n",
      "Epoch:  186  Loss:  50.73670666753791\n",
      "Epoch:  187  Loss:  50.7333385904117\n",
      "Epoch:  188  Loss:  50.73000507288985\n",
      "Epoch:  189  Loss:  50.72670559546698\n",
      "Epoch:  190  Loss:  50.72343964880206\n",
      "Epoch:  191  Loss:  50.720206733476196\n",
      "Epoch:  192  Loss:  50.71700635975675\n",
      "Epoch:  193  Loss:  50.71383804736838\n",
      "Epoch:  194  Loss:  50.71070132527014\n",
      "Epoch:  195  Loss:  50.707595731438815\n",
      "Epoch:  196  Loss:  50.70452081265829\n",
      "Epoch:  197  Loss:  50.70147612431443\n",
      "Epoch:  198  Loss:  50.69846123019594\n",
      "Epoch:  199  Loss:  50.695475702300165\n",
      "Epoch:  200  Loss:  50.69251912064454\n",
      "Epoch:  201  Loss:  50.68959107308291\n",
      "Epoch:  202  Loss:  50.686691155126745\n",
      "Epoch:  203  Loss:  50.68381896977133\n",
      "Epoch:  204  Loss:  50.68097412732631\n",
      "Epoch:  205  Loss:  50.67815624525103\n",
      "Epoch:  206  Loss:  50.67536494799399\n",
      "Epoch:  207  Loss:  50.672599866836606\n",
      "Epoch:  208  Loss:  50.669860639741124\n",
      "Epoch:  209  Loss:  50.6671469112024\n",
      "Epoch:  210  Loss:  50.664458332103656\n",
      "Epoch:  211  Loss:  50.661794559575874\n",
      "Epoch:  212  Loss:  50.65915525686085\n",
      "Epoch:  213  Loss:  50.656540093177895\n",
      "Epoch:  214  Loss:  50.65394874359369\n",
      "Epoch:  215  Loss:  50.65138088889577\n",
      "Epoch:  216  Loss:  50.648836215469004\n",
      "Epoch:  217  Loss:  50.646314415175276\n",
      "Epoch:  218  Loss:  50.64381518523632\n",
      "Epoch:  219  Loss:  50.64133822811928\n",
      "Epoch:  220  Loss:  50.638883251425376\n",
      "Epoch:  221  Loss:  50.63644996778111\n",
      "Epoch:  222  Loss:  50.634038094732446\n",
      "Epoch:  223  Loss:  50.63164735464142\n",
      "Epoch:  224  Loss:  50.629277474585344\n",
      "Epoch:  225  Loss:  50.626928186258574\n",
      "Epoch:  226  Loss:  50.624599225876615\n",
      "Epoch:  227  Loss:  50.62229033408256\n",
      "Epoch:  228  Loss:  50.62000125585582\n",
      "Epoch:  229  Loss:  50.61773174042321\n",
      "Epoch:  230  Loss:  50.61548154117187\n",
      "Epoch:  231  Loss:  50.61325041556468\n",
      "Epoch:  232  Loss:  50.61103812505732\n",
      "Epoch:  233  Loss:  50.60884443501767\n",
      "Epoch:  234  Loss:  50.60666911464681\n",
      "Epoch:  235  Loss:  50.60451193690213\n",
      "Epoch:  236  Loss:  50.602372678422206\n",
      "Epoch:  237  Loss:  50.60025111945333\n",
      "Epoch:  238  Loss:  50.59814704377794\n",
      "Epoch:  239  Loss:  50.596060238644625\n",
      "Epoch:  240  Loss:  50.59399049469977\n",
      "Epoch:  241  Loss:  50.59193760592078\n",
      "Epoch:  242  Loss:  50.589901369551\n",
      "Epoch:  243  Loss:  50.587881586035884\n",
      "Epoch:  244  Loss:  50.585878058960795\n",
      "Epoch:  245  Loss:  50.58389059499029\n",
      "Epoch:  246  Loss:  50.58191900380858\n",
      "Epoch:  247  Loss:  50.579963098061604\n",
      "Epoch:  248  Loss:  50.57802269330022\n",
      "Epoch:  249  Loss:  50.5760976079247\n",
      "Epoch:  250  Loss:  50.5741876631307\n",
      "Epoch:  251  Loss:  50.57229268285601\n",
      "Epoch:  252  Loss:  50.57041249372905\n",
      "Epoch:  253  Loss:  50.56854692501802\n",
      "Epoch:  254  Loss:  50.566695808581436\n",
      "Epoch:  255  Loss:  50.56485897881974\n",
      "Epoch:  256  Loss:  50.56303627262802\n",
      "Epoch:  257  Loss:  50.561227529349466\n",
      "Epoch:  258  Loss:  50.55943259073038\n",
      "Epoch:  259  Loss:  50.55765130087568\n",
      "Epoch:  260  Loss:  50.55588350620563\n",
      "Epoch:  261  Loss:  50.55412905541347\n",
      "Epoch:  262  Loss:  50.55238779942389\n",
      "Epoch:  263  Loss:  50.55065959135254\n",
      "Epoch:  264  Loss:  50.548944286466224\n",
      "Epoch:  265  Loss:  50.5472417421441\n",
      "Epoch:  266  Loss:  50.54555181783966\n",
      "Epoch:  267  Loss:  50.54387437504353\n",
      "Epoch:  268  Loss:  50.54220927724702\n",
      "Epoch:  269  Loss:  50.5405563899065\n",
      "Epoch:  270  Loss:  50.538915580408464\n",
      "Epoch:  271  Loss:  50.53728671803539\n",
      "Epoch:  272  Loss:  50.53566967393227\n",
      "Epoch:  273  Loss:  50.53406432107395\n",
      "Epoch:  274  Loss:  50.53247053423295\n",
      "Epoch:  275  Loss:  50.53088818994809\n",
      "Epoch:  276  Loss:  50.52931716649377\n",
      "Epoch:  277  Loss:  50.52775734384984\n",
      "Epoch:  278  Loss:  50.52620860367207\n",
      "Epoch:  279  Loss:  50.52467082926333\n",
      "Epoch:  280  Loss:  50.52314390554513\n",
      "Epoch:  281  Loss:  50.52162771903019\n",
      "Epoch:  282  Loss:  50.5201221577949\n",
      "Epoch:  283  Loss:  50.51862711145298\n",
      "Epoch:  284  Loss:  50.517142471129304\n",
      "Epoch:  285  Loss:  50.51566812943436\n",
      "Epoch:  286  Loss:  50.51420398043919\n",
      "Epoch:  287  Loss:  50.51274991965082\n",
      "Epoch:  288  Loss:  50.511305843988396\n",
      "Epoch:  289  Loss:  50.509871651759305\n",
      "Epoch:  290  Loss:  50.50844724263632\n",
      "Epoch:  291  Loss:  50.507032517634826\n",
      "Epoch:  292  Loss:  50.50562737909063\n",
      "Epoch:  293  Loss:  50.50423173063827\n",
      "Epoch:  294  Loss:  50.502845477189524\n",
      "Epoch:  295  Loss:  50.50146852491267\n",
      "Epoch:  296  Loss:  50.50010078121174\n",
      "Epoch:  297  Loss:  50.49874215470664\n",
      "Epoch:  298  Loss:  50.497392555213175\n",
      "Epoch:  299  Loss:  50.496051893723894\n",
      "Epoch:  300  Loss:  50.494720082389\n",
      "Epoch:  301  Loss:  50.493397034497754\n",
      "Epoch:  302  Loss:  50.49208266446024\n",
      "Epoch:  303  Loss:  50.490776887789366\n",
      "Epoch:  304  Loss:  50.4894796210834\n",
      "Epoch:  305  Loss:  50.48819078200863\n",
      "Epoch:  306  Loss:  50.4869102892825\n",
      "Epoch:  307  Loss:  50.485638062657024\n",
      "Epoch:  308  Loss:  50.484374022902394\n",
      "Epoch:  309  Loss:  50.48311809179117\n",
      "Epoch:  310  Loss:  50.48187019208239\n",
      "Epoch:  311  Loss:  50.48063024750637\n",
      "Epoch:  312  Loss:  50.47939818274945\n",
      "Epoch:  313  Loss:  50.47817392343918\n",
      "Epoch:  314  Loss:  50.4769573961299\n",
      "Epoch:  315  Loss:  50.47574852828828\n",
      "Epoch:  316  Loss:  50.47454724827936\n",
      "Epoch:  317  Loss:  50.473353485352774\n",
      "Epoch:  318  Loss:  50.472167169629294\n",
      "Epoch:  319  Loss:  50.47098823208744\n",
      "Epoch:  320  Loss:  50.46981660455051\n",
      "Epoch:  321  Loss:  50.468652219673736\n",
      "Epoch:  322  Loss:  50.46749501093186\n",
      "Epoch:  323  Loss:  50.46634491260655\n",
      "Epoch:  324  Loss:  50.465201859774446\n",
      "Epoch:  325  Loss:  50.464065788295265\n",
      "Epoch:  326  Loss:  50.46293663480003\n",
      "Epoch:  327  Loss:  50.46181433667954\n",
      "Epoch:  328  Loss:  50.46069883207328\n",
      "Epoch:  329  Loss:  50.4595900598581\n",
      "Epoch:  330  Loss:  50.45848795963751\n",
      "Epoch:  331  Loss:  50.4573924717309\n",
      "Epoch:  332  Loss:  50.45630353716299\n",
      "Epoch:  333  Loss:  50.455221097653634\n",
      "Epoch:  334  Loss:  50.45414509560751\n",
      "Epoch:  335  Loss:  50.45307547410432\n",
      "Epoch:  336  Loss:  50.4520121768888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  337  Loss:  50.45095514836132\n",
      "Epoch:  338  Loss:  50.44990433356813\n",
      "Epoch:  339  Loss:  50.44885967819238\n",
      "Epoch:  340  Loss:  50.44782112854474\n",
      "Epoch:  341  Loss:  50.4467886315545\n",
      "Epoch:  342  Loss:  50.44576213476074\n",
      "Epoch:  343  Loss:  50.44474158630364\n",
      "Epoch:  344  Loss:  50.44372693491595\n",
      "Epoch:  345  Loss:  50.442718129914596\n",
      "Epoch:  346  Loss:  50.4417151211924\n",
      "Epoch:  347  Loss:  50.44071785921002\n",
      "Epoch:  348  Loss:  50.43972629498796\n",
      "Epoch:  349  Loss:  50.43874038009866\n",
      "Epoch:  350  Loss:  50.43776006665891\n",
      "Epoch:  351  Loss:  50.43678530732218\n",
      "Epoch:  352  Loss:  50.43581605527115\n",
      "Epoch:  353  Loss:  50.434852264210406\n",
      "Epoch:  354  Loss:  50.433893888359314\n",
      "Epoch:  355  Loss:  50.432940882444726\n",
      "Epoch:  356  Loss:  50.43199320169417\n",
      "Epoch:  357  Loss:  50.431050801828945\n",
      "Epoch:  358  Loss:  50.430113639057296\n",
      "Epoch:  359  Loss:  50.42918167006789\n",
      "Epoch:  360  Loss:  50.42825485202321\n",
      "Epoch:  361  Loss:  50.42733314255311\n",
      "Epoch:  362  Loss:  50.42641649974856\n",
      "Epoch:  363  Loss:  50.42550488215532\n",
      "Epoch:  364  Loss:  50.42459824876797\n",
      "Epoch:  365  Loss:  50.42369655902371\n",
      "Epoch:  366  Loss:  50.42279977279657\n",
      "Epoch:  367  Loss:  50.421907850391484\n",
      "Epoch:  368  Loss:  50.42102075253862\n",
      "Epoch:  369  Loss:  50.42013844038766\n",
      "Epoch:  370  Loss:  50.41926087550235\n",
      "Epoch:  371  Loss:  50.41838801985488\n",
      "Epoch:  372  Loss:  50.41751983582057\n",
      "Epoch:  373  Loss:  50.41665628617264\n",
      "Epoch:  374  Loss:  50.415797334076856\n",
      "Epoch:  375  Loss:  50.41494294308648\n",
      "Epoch:  376  Loss:  50.414093077137196\n",
      "Epoch:  377  Loss:  50.41324770054209\n",
      "Epoch:  378  Loss:  50.41240677798686\n",
      "Epoch:  379  Loss:  50.41157027452482\n",
      "Epoch:  380  Loss:  50.410738155572304\n",
      "Epoch:  381  Loss:  50.4099103869039\n",
      "Epoch:  382  Loss:  50.409086934647895\n",
      "Epoch:  383  Loss:  50.40826776528169\n",
      "Epoch:  384  Loss:  50.40745284562736\n",
      "Epoch:  385  Loss:  50.40664214284726\n",
      "Epoch:  386  Loss:  50.405835624439675\n",
      "Epoch:  387  Loss:  50.405033258234596\n",
      "Epoch:  388  Loss:  50.404235012389464\n",
      "Epoch:  389  Loss:  50.403440855385064\n",
      "Epoch:  390  Loss:  50.40265075602143\n",
      "Epoch:  391  Loss:  50.401864683413855\n",
      "Epoch:  392  Loss:  50.40108260698894\n",
      "Epoch:  393  Loss:  50.40030449648064\n",
      "Epoch:  394  Loss:  50.39953032192651\n",
      "Epoch:  395  Loss:  50.398760053663864\n",
      "Epoch:  396  Loss:  50.39799366232611\n",
      "Epoch:  397  Loss:  50.39723111883898\n",
      "Epoch:  398  Loss:  50.39647239441706\n",
      "Epoch:  399  Loss:  50.39571746056006\n",
      "Epoch:  400  Loss:  50.39496628904946\n",
      "Epoch:  401  Loss:  50.394218851944956\n",
      "Epoch:  402  Loss:  50.39347512158111\n",
      "Epoch:  403  Loss:  50.39273507056392\n",
      "Epoch:  404  Loss:  50.391998671767595\n",
      "Epoch:  405  Loss:  50.391265898331255\n",
      "Epoch:  406  Loss:  50.39053672365573\n",
      "Epoch:  407  Loss:  50.38981112140032\n",
      "Epoch:  408  Loss:  50.38908906547982\n",
      "Epoch:  409  Loss:  50.38837053006133\n",
      "Epoch:  410  Loss:  50.387655489561254\n",
      "Epoch:  411  Loss:  50.38694391864229\n",
      "Epoch:  412  Loss:  50.38623579221056\n",
      "Epoch:  413  Loss:  50.385531085412616\n",
      "Epoch:  414  Loss:  50.38482977363263\n",
      "Epoch:  415  Loss:  50.384131832489516\n",
      "Epoch:  416  Loss:  50.3834372378343\n",
      "Epoch:  417  Loss:  50.38274596574715\n",
      "Epoch:  418  Loss:  50.38205799253483\n",
      "Epoch:  419  Loss:  50.38137329472804\n",
      "Epoch:  420  Loss:  50.38069184907867\n",
      "Epoch:  421  Loss:  50.38001363255731\n",
      "Epoch:  422  Loss:  50.379338622350694\n",
      "Epoch:  423  Loss:  50.37866679585912\n",
      "Epoch:  424  Loss:  50.377998130693996\n",
      "Epoch:  425  Loss:  50.3773326046754\n",
      "Epoch:  426  Loss:  50.3766701958297\n",
      "Epoch:  427  Loss:  50.376010882387014\n",
      "Epoch:  428  Loss:  50.37535464277909\n",
      "Epoch:  429  Loss:  50.374701455636774\n",
      "Epoch:  430  Loss:  50.374051299787894\n",
      "Epoch:  431  Loss:  50.37340415425489\n",
      "Epoch:  432  Loss:  50.37275999825266\n",
      "Epoch:  433  Loss:  50.37211881118628\n",
      "Epoch:  434  Loss:  50.37148057264896\n",
      "Epoch:  435  Loss:  50.37084526241984\n",
      "Epoch:  436  Loss:  50.370212860461876\n",
      "Epoch:  437  Loss:  50.36958334691976\n",
      "Epoch:  438  Loss:  50.36895670211794\n",
      "Epoch:  439  Loss:  50.36833290655851\n",
      "Epoch:  440  Loss:  50.36771194091923\n",
      "Epoch:  441  Loss:  50.36709378605167\n",
      "Epoch:  442  Loss:  50.3664784229791\n",
      "Epoch:  443  Loss:  50.365865832894684\n",
      "Epoch:  444  Loss:  50.36525599715958\n",
      "Epoch:  445  Loss:  50.36464889730103\n",
      "Epoch:  446  Loss:  50.36404451501056\n",
      "Epoch:  447  Loss:  50.363442832142155\n",
      "Epoch:  448  Loss:  50.36284383071045\n",
      "Epoch:  449  Loss:  50.36224749288901\n",
      "Epoch:  450  Loss:  50.36165380100845\n",
      "Epoch:  451  Loss:  50.361062737554946\n",
      "Epoch:  452  Loss:  50.3604742851683\n",
      "Epoch:  453  Loss:  50.359888426640474\n",
      "Epoch:  454  Loss:  50.35930514491367\n",
      "Epoch:  455  Loss:  50.358724423078996\n",
      "Epoch:  456  Loss:  50.35814624437464\n",
      "Epoch:  457  Loss:  50.357570592184324\n",
      "Epoch:  458  Loss:  50.35699745003579\n",
      "Epoch:  459  Loss:  50.35642680159924\n",
      "Epoch:  460  Loss:  50.355858630685695\n",
      "Epoch:  461  Loss:  50.35529292124568\n",
      "Epoch:  462  Loss:  50.3547296573675\n",
      "Epoch:  463  Loss:  50.35416882327593\n",
      "Epoch:  464  Loss:  50.35361040333077\n",
      "Epoch:  465  Loss:  50.353054382025284\n",
      "Epoch:  466  Loss:  50.35250074398486\n",
      "Epoch:  467  Loss:  50.35194947396563\n",
      "Epoch:  468  Loss:  50.35140055685305\n",
      "Epoch:  469  Loss:  50.35085397766055\n",
      "Epoch:  470  Loss:  50.35030972152815\n",
      "Epoch:  471  Loss:  50.3497677737212\n",
      "Epoch:  472  Loss:  50.34922811962905\n",
      "Epoch:  473  Loss:  50.34869074476363\n",
      "Epoch:  474  Loss:  50.34815563475839\n",
      "Epoch:  475  Loss:  50.34762277536686\n",
      "Epoch:  476  Loss:  50.34709215246141\n",
      "Epoch:  477  Loss:  50.34656375203215\n",
      "Epoch:  478  Loss:  50.34603756018559\n",
      "Epoch:  479  Loss:  50.345513563143435\n",
      "Epoch:  480  Loss:  50.34499174724145\n",
      "Epoch:  481  Loss:  50.3444720989283\n",
      "Epoch:  482  Loss:  50.34395460476432\n",
      "Epoch:  483  Loss:  50.343439251420385\n",
      "Epoch:  484  Loss:  50.342926025676896\n",
      "Epoch:  485  Loss:  50.342414914422434\n",
      "Epoch:  486  Loss:  50.34190590465292\n",
      "Epoch:  487  Loss:  50.341398983470285\n",
      "Epoch:  488  Loss:  50.34089413808154\n",
      "Epoch:  489  Loss:  50.34039135579773\n",
      "Epoch:  490  Loss:  50.339890624032705\n",
      "Epoch:  491  Loss:  50.33939193030233\n",
      "Epoch:  492  Loss:  50.338895262223254\n",
      "Epoch:  493  Loss:  50.338400607512014\n",
      "Epoch:  494  Loss:  50.33790795398391\n",
      "Epoch:  495  Loss:  50.33741728955222\n",
      "Epoch:  496  Loss:  50.336928602227005\n",
      "Epoch:  497  Loss:  50.33644188011429\n",
      "Epoch:  498  Loss:  50.335957111415055\n",
      "Epoch:  499  Loss:  50.33547428442425\n",
      "Epoch:  500  Loss:  50.33499338752995\n",
      "Epoch:  501  Loss:  50.33451440921236\n",
      "Epoch:  502  Loss:  50.334037338043\n",
      "Epoch:  503  Loss:  50.333562162683656\n",
      "Epoch:  504  Loss:  50.33308887188565\n",
      "Epoch:  505  Loss:  50.3326174544888\n",
      "Epoch:  506  Loss:  50.33214789942079\n",
      "Epoch:  507  Loss:  50.331680195696016\n",
      "Epoch:  508  Loss:  50.33121433241493\n",
      "Epoch:  509  Loss:  50.33075029876319\n",
      "Epoch:  510  Loss:  50.33028808401076\n",
      "Epoch:  511  Loss:  50.329827677511155\n",
      "Epoch:  512  Loss:  50.32936906870066\n",
      "Epoch:  513  Loss:  50.3289122470973\n",
      "Epoch:  514  Loss:  50.32845720230041\n",
      "Epoch:  515  Loss:  50.32800392398964\n",
      "Epoch:  516  Loss:  50.32755240192411\n",
      "Epoch:  517  Loss:  50.32710262594185\n",
      "Epoch:  518  Loss:  50.326654585958906\n",
      "Epoch:  519  Loss:  50.32620827196865\n",
      "Epoch:  520  Loss:  50.325763674040985\n",
      "Epoch:  521  Loss:  50.32532078232172\n",
      "Epoch:  522  Loss:  50.32487958703174\n",
      "Epoch:  523  Loss:  50.32444007846635\n",
      "Epoch:  524  Loss:  50.324002246994546\n",
      "Epoch:  525  Loss:  50.32356608305831\n",
      "Epoch:  526  Loss:  50.32313157717208\n",
      "Epoch:  527  Loss:  50.32269871992171\n",
      "Epoch:  528  Loss:  50.322267501964205\n",
      "Epoch:  529  Loss:  50.32183791402677\n",
      "Epoch:  530  Loss:  50.321409946906265\n",
      "Epoch:  531  Loss:  50.32098359146857\n",
      "Epoch:  532  Loss:  50.320558838647884\n",
      "Epoch:  533  Loss:  50.320135679446125\n",
      "Epoch:  534  Loss:  50.31971410493223\n",
      "Epoch:  535  Loss:  50.31929410624175\n",
      "Epoch:  536  Loss:  50.31887567457588\n",
      "Epoch:  537  Loss:  50.31845880120119\n",
      "Epoch:  538  Loss:  50.318043477448825\n",
      "Epoch:  539  Loss:  50.31762969471396\n",
      "Epoch:  540  Loss:  50.31721744445523\n",
      "Epoch:  541  Loss:  50.31680671819412\n",
      "Epoch:  542  Loss:  50.31639750751444\n",
      "Epoch:  543  Loss:  50.31598980406166\n",
      "Epoch:  544  Loss:  50.31558359954246\n",
      "Epoch:  545  Loss:  50.31517888572409\n",
      "Epoch:  546  Loss:  50.31477565443383\n",
      "Epoch:  547  Loss:  50.31437389755848\n",
      "Epoch:  548  Loss:  50.31397360704382\n",
      "Epoch:  549  Loss:  50.31357477489403\n",
      "Epoch:  550  Loss:  50.3131773931712\n",
      "Epoch:  551  Loss:  50.31278145399479\n",
      "Epoch:  552  Loss:  50.312386949541136\n",
      "Epoch:  553  Loss:  50.31199387204292\n",
      "Epoch:  554  Loss:  50.31160221378867\n",
      "Epoch:  555  Loss:  50.31121196712225\n",
      "Epoch:  556  Loss:  50.31082312444236\n",
      "Epoch:  557  Loss:  50.31043567820214\n",
      "Epoch:  558  Loss:  50.310049620908494\n",
      "Epoch:  559  Loss:  50.30966494512186\n",
      "Epoch:  560  Loss:  50.30928164345546\n",
      "Epoch:  561  Loss:  50.30889970857504\n",
      "Epoch:  562  Loss:  50.30851913319838\n",
      "Epoch:  563  Loss:  50.30813991009471\n",
      "Epoch:  564  Loss:  50.307762032084426\n",
      "Epoch:  565  Loss:  50.307385492038435\n",
      "Epoch:  566  Loss:  50.307010282877975\n",
      "Epoch:  567  Loss:  50.306636397573925\n",
      "Epoch:  568  Loss:  50.306263829146495\n",
      "Epoch:  569  Loss:  50.30589257066482\n",
      "Epoch:  570  Loss:  50.30552261524641\n",
      "Epoch:  571  Loss:  50.30515395605687\n",
      "Epoch:  572  Loss:  50.30478658630942\n",
      "Epoch:  573  Loss:  50.304420499264424\n",
      "Epoch:  574  Loss:  50.30405568822912\n",
      "Epoch:  575  Loss:  50.30369214655709\n",
      "Epoch:  576  Loss:  50.30332986764785\n",
      "Epoch:  577  Loss:  50.302968844946584\n",
      "Epoch:  578  Loss:  50.30260907194366\n",
      "Epoch:  579  Loss:  50.30225054217423\n",
      "Epoch:  580  Loss:  50.30189324921794\n",
      "Epoch:  581  Loss:  50.30153718669835\n",
      "Epoch:  582  Loss:  50.30118234828284\n",
      "Epoch:  583  Loss:  50.30082872768199\n",
      "Epoch:  584  Loss:  50.300476318649345\n",
      "Epoch:  585  Loss:  50.30012511498097\n",
      "Epoch:  586  Loss:  50.29977511051526\n",
      "Epoch:  587  Loss:  50.2994262991323\n",
      "Epoch:  588  Loss:  50.29907867475371\n",
      "Epoch:  589  Loss:  50.29873223134232\n",
      "Epoch:  590  Loss:  50.2983869629017\n",
      "Epoch:  591  Loss:  50.29804286347585\n",
      "Epoch:  592  Loss:  50.29769992714891\n",
      "Epoch:  593  Loss:  50.29735814804481\n",
      "Epoch:  594  Loss:  50.29701752032691\n",
      "Epoch:  595  Loss:  50.296678038197655\n",
      "Epoch:  596  Loss:  50.296339695898304\n",
      "Epoch:  597  Loss:  50.29600248770855\n",
      "Epoch:  598  Loss:  50.29566640794627\n",
      "Epoch:  599  Loss:  50.29533145096711\n",
      "Epoch:  600  Loss:  50.294997611164334\n",
      "Epoch:  601  Loss:  50.294664882968235\n",
      "Epoch:  602  Loss:  50.29433326084616\n",
      "Epoch:  603  Loss:  50.29400273930199\n",
      "Epoch:  604  Loss:  50.29367331287583\n",
      "Epoch:  605  Loss:  50.29334497614388\n",
      "Epoch:  606  Loss:  50.293017723717945\n",
      "Epoch:  607  Loss:  50.29269155024533\n",
      "Epoch:  608  Loss:  50.29236645040831\n",
      "Epoch:  609  Loss:  50.29204241892414\n",
      "Epoch:  610  Loss:  50.2917194505445\n",
      "Epoch:  611  Loss:  50.2913975400554\n",
      "Epoch:  612  Loss:  50.29107668227676\n",
      "Epoch:  613  Loss:  50.290756872062275\n",
      "Epoch:  614  Loss:  50.29043810429904\n",
      "Epoch:  615  Loss:  50.29012037390728\n",
      "Epoch:  616  Loss:  50.289803675840204\n",
      "Epoch:  617  Loss:  50.289488005083534\n",
      "Epoch:  618  Loss:  50.28917335665547\n",
      "Epoch:  619  Loss:  50.28885972560618\n",
      "Epoch:  620  Loss:  50.28854710701781\n",
      "Epoch:  621  Loss:  50.28823549600406\n",
      "Epoch:  622  Loss:  50.28792488770992\n",
      "Epoch:  623  Loss:  50.28761527731152\n",
      "Epoch:  624  Loss:  50.28730666001581\n",
      "Epoch:  625  Loss:  50.28699903106032\n",
      "Epoch:  626  Loss:  50.286692385712975\n",
      "Epoch:  627  Loss:  50.28638671927176\n",
      "Epoch:  628  Loss:  50.286082027064566\n",
      "Epoch:  629  Loss:  50.285778304448904\n",
      "Epoch:  630  Loss:  50.2854755468117\n",
      "Epoch:  631  Loss:  50.28517374956904\n",
      "Epoch:  632  Loss:  50.28487290816594\n",
      "Epoch:  633  Loss:  50.28457301807614\n",
      "Epoch:  634  Loss:  50.28427407480183\n",
      "Epoch:  635  Loss:  50.28397607387354\n",
      "Epoch:  636  Loss:  50.28367901084979\n",
      "Epoch:  637  Loss:  50.2833828813169\n",
      "Epoch:  638  Loss:  50.283087680888826\n",
      "Epoch:  639  Loss:  50.28279340520694\n",
      "Epoch:  640  Loss:  50.28250004993973\n",
      "Epoch:  641  Loss:  50.282207610782706\n",
      "Epoch:  642  Loss:  50.28191608345808\n",
      "Epoch:  643  Loss:  50.2816254637147\n",
      "Epoch:  644  Loss:  50.28133574732762\n",
      "Epoch:  645  Loss:  50.281046930098164\n",
      "Epoch:  646  Loss:  50.280759007853575\n",
      "Epoch:  647  Loss:  50.2804719764467\n",
      "Epoch:  648  Loss:  50.28018583175616\n",
      "Epoch:  649  Loss:  50.2799005696857\n",
      "Epoch:  650  Loss:  50.27961618616436\n",
      "Epoch:  651  Loss:  50.27933267714608\n",
      "Epoch:  652  Loss:  50.279050038609554\n",
      "Epoch:  653  Loss:  50.27876826655811\n",
      "Epoch:  654  Loss:  50.27848735701941\n",
      "Epoch:  655  Loss:  50.27820730604538\n",
      "Epoch:  656  Loss:  50.27792810971192\n",
      "Epoch:  657  Loss:  50.27764976411877\n",
      "Epoch:  658  Loss:  50.277372265389374\n",
      "Epoch:  659  Loss:  50.27709560967061\n",
      "Epoch:  660  Loss:  50.27681979313269\n",
      "Epoch:  661  Loss:  50.27654481196899\n",
      "Epoch:  662  Loss:  50.276270662395696\n",
      "Epoch:  663  Loss:  50.27599734065194\n",
      "Epoch:  664  Loss:  50.27572484299941\n",
      "Epoch:  665  Loss:  50.27545316572219\n",
      "Epoch:  666  Loss:  50.27518230512666\n",
      "Epoch:  667  Loss:  50.274912257541345\n",
      "Epoch:  668  Loss:  50.2746430193167\n",
      "Epoch:  669  Loss:  50.274374586824905\n",
      "Epoch:  670  Loss:  50.274106956459825\n",
      "Epoch:  671  Loss:  50.27384012463672\n",
      "Epoch:  672  Loss:  50.27357408779223\n",
      "Epoch:  673  Loss:  50.27330884238403\n",
      "Epoch:  674  Loss:  50.273044384890845\n",
      "Epoch:  675  Loss:  50.27278071181224\n",
      "Epoch:  676  Loss:  50.27251781966841\n",
      "Epoch:  677  Loss:  50.27225570500008\n",
      "Epoch:  678  Loss:  50.27199436436838\n",
      "Epoch:  679  Loss:  50.27173379435461\n",
      "Epoch:  680  Loss:  50.27147399156021\n",
      "Epoch:  681  Loss:  50.27121495260648\n",
      "Epoch:  682  Loss:  50.270956674134545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  683  Loss:  50.27069915280515\n",
      "Epoch:  684  Loss:  50.27044238529855\n",
      "Epoch:  685  Loss:  50.27018636831433\n",
      "Epoch:  686  Loss:  50.26993109857128\n",
      "Epoch:  687  Loss:  50.26967657280729\n",
      "Epoch:  688  Loss:  50.26942278777916\n",
      "Epoch:  689  Loss:  50.269169740262555\n",
      "Epoch:  690  Loss:  50.26891742705165\n",
      "Epoch:  691  Loss:  50.26866584495931\n",
      "Epoch:  692  Loss:  50.268414990816716\n",
      "Epoch:  693  Loss:  50.268164861473274\n",
      "Epoch:  694  Loss:  50.26791545379663\n",
      "Epoch:  695  Loss:  50.267666764672306\n",
      "Epoch:  696  Loss:  50.26741879100377\n",
      "Epoch:  697  Loss:  50.2671715297122\n",
      "Epoch:  698  Loss:  50.266924977736444\n",
      "Epoch:  699  Loss:  50.266679132032756\n",
      "Epoch:  700  Loss:  50.26643398957483\n",
      "Epoch:  701  Loss:  50.26618954735355\n",
      "Epoch:  702  Loss:  50.265945802376955\n",
      "Epoch:  703  Loss:  50.265702751670055\n",
      "Epoch:  704  Loss:  50.26546039227477\n",
      "Epoch:  705  Loss:  50.26521872124974\n",
      "Epoch:  706  Loss:  50.26497773567029\n",
      "Epoch:  707  Loss:  50.26473743262824\n",
      "Epoch:  708  Loss:  50.264497809231806\n",
      "Epoch:  709  Loss:  50.264258862605516\n",
      "Epoch:  710  Loss:  50.26402058989007\n",
      "Epoch:  711  Loss:  50.26378298824223\n",
      "Epoch:  712  Loss:  50.263546054834706\n",
      "Epoch:  713  Loss:  50.263309786856034\n",
      "Epoch:  714  Loss:  50.263074181510554\n",
      "Epoch:  715  Loss:  50.26283923601808\n",
      "Epoch:  716  Loss:  50.2626049476141\n",
      "Epoch:  717  Loss:  50.26237131354939\n",
      "Epoch:  718  Loss:  50.26213833109006\n",
      "Epoch:  719  Loss:  50.26190599751741\n",
      "Epoch:  720  Loss:  50.261674310127816\n",
      "Epoch:  721  Loss:  50.26144326623262\n",
      "Epoch:  722  Loss:  50.26121286315811\n",
      "Epoch:  723  Loss:  50.260983098245255\n",
      "Epoch:  724  Loss:  50.26075396884977\n",
      "Epoch:  725  Loss:  50.260525472341854\n",
      "Epoch:  726  Loss:  50.26029760610628\n",
      "Epoch:  727  Loss:  50.26007036754214\n",
      "Epoch:  728  Loss:  50.25984375406277\n",
      "Epoch:  729  Loss:  50.25961776309574\n",
      "Epoch:  730  Loss:  50.25939239208268\n",
      "Epoch:  731  Loss:  50.25916763847919\n",
      "Epoch:  732  Loss:  50.258943499754764\n",
      "Epoch:  733  Loss:  50.25871997339266\n",
      "Epoch:  734  Loss:  50.258497056889894\n",
      "Epoch:  735  Loss:  50.25827474775706\n",
      "Epoch:  736  Loss:  50.258053043518245\n",
      "Epoch:  737  Loss:  50.25783194171094\n",
      "Epoch:  738  Loss:  50.25761143988607\n",
      "Epoch:  739  Loss:  50.25739153560771\n",
      "Epoch:  740  Loss:  50.257172226453115\n",
      "Epoch:  741  Loss:  50.25695351001261\n",
      "Epoch:  742  Loss:  50.25673538388945\n",
      "Epoch:  743  Loss:  50.25651784569985\n",
      "Epoch:  744  Loss:  50.256300893072776\n",
      "Epoch:  745  Loss:  50.256084523649946\n",
      "Epoch:  746  Loss:  50.25586873508569\n",
      "Epoch:  747  Loss:  50.255653525046874\n",
      "Epoch:  748  Loss:  50.25543889121286\n",
      "Epoch:  749  Loss:  50.255224831275356\n",
      "Epoch:  750  Loss:  50.25501134293837\n",
      "Epoch:  751  Loss:  50.25479842391819\n",
      "Epoch:  752  Loss:  50.25458607194314\n",
      "Epoch:  753  Loss:  50.25437428475369\n",
      "Epoch:  754  Loss:  50.25416306010222\n",
      "Epoch:  755  Loss:  50.253952395753025\n",
      "Epoch:  756  Loss:  50.253742289482254\n",
      "Epoch:  757  Loss:  50.253532739077734\n",
      "Epoch:  758  Loss:  50.25332374233902\n",
      "Epoch:  759  Loss:  50.25311529707718\n",
      "Epoch:  760  Loss:  50.25290740111488\n",
      "Epoch:  761  Loss:  50.252700052286116\n",
      "Epoch:  762  Loss:  50.25249324843633\n",
      "Epoch:  763  Loss:  50.252286987422224\n",
      "Epoch:  764  Loss:  50.25208126711171\n",
      "Epoch:  765  Loss:  50.25187608538381\n",
      "Epoch:  766  Loss:  50.251671440128646\n",
      "Epoch:  767  Loss:  50.251467329247305\n",
      "Epoch:  768  Loss:  50.25126375065187\n",
      "Epoch:  769  Loss:  50.25106070226518\n",
      "Epoch:  770  Loss:  50.250858182020885\n",
      "Epoch:  771  Loss:  50.250656187863356\n",
      "Epoch:  772  Loss:  50.25045471774766\n",
      "Epoch:  773  Loss:  50.25025376963935\n",
      "Epoch:  774  Loss:  50.250053341514494\n",
      "Epoch:  775  Loss:  50.249853431359675\n",
      "Epoch:  776  Loss:  50.24965403717175\n",
      "Epoch:  777  Loss:  50.249455156957936\n",
      "Epoch:  778  Loss:  50.24925678873572\n",
      "Epoch:  779  Loss:  50.24905893053265\n",
      "Epoch:  780  Loss:  50.24886158038646\n",
      "Epoch:  781  Loss:  50.24866473634498\n",
      "Epoch:  782  Loss:  50.24846839646587\n",
      "Epoch:  783  Loss:  50.24827255881683\n",
      "Epoch:  784  Loss:  50.248077221475384\n",
      "Epoch:  785  Loss:  50.24788238252881\n",
      "Epoch:  786  Loss:  50.24768804007411\n",
      "Epoch:  787  Loss:  50.247494192218014\n",
      "Epoch:  788  Loss:  50.2473008370768\n",
      "Epoch:  789  Loss:  50.24710797277631\n",
      "Epoch:  790  Loss:  50.246915597451874\n",
      "Epoch:  791  Loss:  50.24672370924824\n",
      "Epoch:  792  Loss:  50.2465323063195\n",
      "Epoch:  793  Loss:  50.24634138682907\n",
      "Epoch:  794  Loss:  50.2461509489496\n",
      "Epoch:  795  Loss:  50.24596099086297\n",
      "Epoch:  796  Loss:  50.24577151076012\n",
      "Epoch:  797  Loss:  50.24558250684115\n",
      "Epoch:  798  Loss:  50.245393977315075\n",
      "Epoch:  799  Loss:  50.245205920399954\n",
      "Epoch:  800  Loss:  50.24501833432272\n",
      "Epoch:  801  Loss:  50.24483121731915\n",
      "Epoch:  802  Loss:  50.24464456763384\n",
      "Epoch:  803  Loss:  50.244458383520055\n",
      "Epoch:  804  Loss:  50.24427266323983\n",
      "Epoch:  805  Loss:  50.24408740506384\n",
      "Epoch:  806  Loss:  50.2439026072712\n",
      "Epoch:  807  Loss:  50.24371826814971\n",
      "Epoch:  808  Loss:  50.24353438599555\n",
      "Epoch:  809  Loss:  50.243350959113386\n",
      "Epoch:  810  Loss:  50.24316798581618\n",
      "Epoch:  811  Loss:  50.2429854644252\n",
      "Epoch:  812  Loss:  50.242803393270094\n",
      "Epoch:  813  Loss:  50.2426217706886\n",
      "Epoch:  814  Loss:  50.24244059502671\n",
      "Epoch:  815  Loss:  50.242259864638456\n",
      "Epoch:  816  Loss:  50.24207957788599\n",
      "Epoch:  817  Loss:  50.241899733139455\n",
      "Epoch:  818  Loss:  50.24172032877696\n",
      "Epoch:  819  Loss:  50.24154136318454\n",
      "Epoch:  820  Loss:  50.24136283475608\n",
      "Epoch:  821  Loss:  50.24118474189333\n",
      "Epoch:  822  Loss:  50.24100708300575\n",
      "Epoch:  823  Loss:  50.24082985651059\n",
      "Epoch:  824  Loss:  50.24065306083275\n",
      "Epoch:  825  Loss:  50.24047669440474\n",
      "Epoch:  826  Loss:  50.24030075566669\n",
      "Epoch:  827  Loss:  50.24012524306629\n",
      "Epoch:  828  Loss:  50.23995015505868\n",
      "Epoch:  829  Loss:  50.23977549010647\n",
      "Epoch:  830  Loss:  50.23960124667968\n",
      "Epoch:  831  Loss:  50.239427423255705\n",
      "Epoch:  832  Loss:  50.239254018319194\n",
      "Epoch:  833  Loss:  50.239081030362186\n",
      "Epoch:  834  Loss:  50.23890845788385\n",
      "Epoch:  835  Loss:  50.23873629939056\n",
      "Epoch:  836  Loss:  50.23856455339582\n",
      "Epoch:  837  Loss:  50.23839321842034\n",
      "Epoch:  838  Loss:  50.23822229299174\n",
      "Epoch:  839  Loss:  50.238051775644756\n",
      "Epoch:  840  Loss:  50.23788166492109\n",
      "Epoch:  841  Loss:  50.237711959369356\n",
      "Epoch:  842  Loss:  50.23754265754503\n",
      "Epoch:  843  Loss:  50.23737375801047\n",
      "Epoch:  844  Loss:  50.23720525933489\n",
      "Epoch:  845  Loss:  50.23703716009421\n",
      "Epoch:  846  Loss:  50.23686945887111\n",
      "Epoch:  847  Loss:  50.23670215425496\n",
      "Epoch:  848  Loss:  50.236535244841754\n",
      "Epoch:  849  Loss:  50.23636872923415\n",
      "Epoch:  850  Loss:  50.23620260604137\n",
      "Epoch:  851  Loss:  50.2360368738791\n",
      "Epoch:  852  Loss:  50.23587153136962\n",
      "Epoch:  853  Loss:  50.23570657714162\n",
      "Epoch:  854  Loss:  50.235542009830205\n",
      "Epoch:  855  Loss:  50.23537782807687\n",
      "Epoch:  856  Loss:  50.2352140305295\n",
      "Epoch:  857  Loss:  50.23505061584222\n",
      "Epoch:  858  Loss:  50.23488758267547\n",
      "Epoch:  859  Loss:  50.234724929695915\n",
      "Epoch:  860  Loss:  50.23456265557642\n",
      "Epoch:  861  Loss:  50.23440075899604\n",
      "Epoch:  862  Loss:  50.23423923863993\n",
      "Epoch:  863  Loss:  50.23407809319929\n",
      "Epoch:  864  Loss:  50.2339173213715\n",
      "Epoch:  865  Loss:  50.23375692185988\n",
      "Epoch:  866  Loss:  50.2335968933737\n",
      "Epoch:  867  Loss:  50.23343723462827\n",
      "Epoch:  868  Loss:  50.23327794434482\n",
      "Epoch:  869  Loss:  50.233119021250374\n",
      "Epoch:  870  Loss:  50.232960464077884\n",
      "Epoch:  871  Loss:  50.23280227156612\n",
      "Epoch:  872  Loss:  50.23264444245957\n",
      "Epoch:  873  Loss:  50.23248697550857\n",
      "Epoch:  874  Loss:  50.2323298694691\n",
      "Epoch:  875  Loss:  50.23217312310284\n",
      "Epoch:  876  Loss:  50.23201673517715\n",
      "Epoch:  877  Loss:  50.231860704465\n",
      "Epoch:  878  Loss:  50.231705029744894\n",
      "Epoch:  879  Loss:  50.23154970980104\n",
      "Epoch:  880  Loss:  50.23139474342298\n",
      "Epoch:  881  Loss:  50.231240129405904\n",
      "Epoch:  882  Loss:  50.23108586655039\n",
      "Epoch:  883  Loss:  50.23093195366248\n",
      "Epoch:  884  Loss:  50.23077838955358\n",
      "Epoch:  885  Loss:  50.23062517304051\n",
      "Epoch:  886  Loss:  50.23047230294541\n",
      "Epoch:  887  Loss:  50.23031977809577\n",
      "Epoch:  888  Loss:  50.230167597324275\n",
      "Epoch:  889  Loss:  50.23001575946891\n",
      "Epoch:  890  Loss:  50.22986426337294\n",
      "Epoch:  891  Loss:  50.22971310788473\n",
      "Epoch:  892  Loss:  50.22956229185786\n",
      "Epoch:  893  Loss:  50.22941181415103\n",
      "Epoch:  894  Loss:  50.22926167362803\n",
      "Epoch:  895  Loss:  50.22911186915776\n",
      "Epoch:  896  Loss:  50.22896239961416\n",
      "Epoch:  897  Loss:  50.22881326387622\n",
      "Epoch:  898  Loss:  50.22866446082783\n",
      "Epoch:  899  Loss:  50.22851598935794\n",
      "Epoch:  900  Loss:  50.228367848360385\n",
      "Epoch:  901  Loss:  50.22822003673393\n",
      "Epoch:  902  Loss:  50.22807255338224\n",
      "Epoch:  903  Loss:  50.22792539721382\n",
      "Epoch:  904  Loss:  50.227778567141996\n",
      "Epoch:  905  Loss:  50.22763206208489\n",
      "Epoch:  906  Loss:  50.22748588096545\n",
      "Epoch:  907  Loss:  50.22734002271131\n",
      "Epoch:  908  Loss:  50.22719448625487\n",
      "Epoch:  909  Loss:  50.227049270533215\n",
      "Epoch:  910  Loss:  50.226904374488086\n",
      "Epoch:  911  Loss:  50.22675979706592\n",
      "Epoch:  912  Loss:  50.22661553721771\n",
      "Epoch:  913  Loss:  50.226471593899106\n",
      "Epoch:  914  Loss:  50.22632796607029\n",
      "Epoch:  915  Loss:  50.22618465269597\n",
      "Epoch:  916  Loss:  50.226041652745465\n",
      "Epoch:  917  Loss:  50.22589896519248\n",
      "Epoch:  918  Loss:  50.22575658901526\n",
      "Epoch:  919  Loss:  50.225614523196484\n",
      "Epoch:  920  Loss:  50.22547276672323\n",
      "Epoch:  921  Loss:  50.22533131858703\n",
      "Epoch:  922  Loss:  50.22519017778375\n",
      "Epoch:  923  Loss:  50.22504934331358\n",
      "Epoch:  924  Loss:  50.224908814181106\n",
      "Epoch:  925  Loss:  50.2247685893952\n",
      "Epoch:  926  Loss:  50.22462866796898\n",
      "Epoch:  927  Loss:  50.22448904891982\n",
      "Epoch:  928  Loss:  50.22434973126939\n",
      "Epoch:  929  Loss:  50.22421071404355\n",
      "Epoch:  930  Loss:  50.22407199627231\n",
      "Epoch:  931  Loss:  50.22393357698989\n",
      "Epoch:  932  Loss:  50.22379545523462\n",
      "Epoch:  933  Loss:  50.22365763004901\n",
      "Epoch:  934  Loss:  50.22352010047961\n",
      "Epoch:  935  Loss:  50.2233828655771\n",
      "Epoch:  936  Loss:  50.22324592439616\n",
      "Epoch:  937  Loss:  50.22310927599557\n",
      "Epoch:  938  Loss:  50.222972919438064\n",
      "Epoch:  939  Loss:  50.22283685379041\n",
      "Epoch:  940  Loss:  50.222701078123364\n",
      "Epoch:  941  Loss:  50.22256559151156\n",
      "Epoch:  942  Loss:  50.222430393033626\n",
      "Epoch:  943  Loss:  50.2222954817721\n",
      "Epoch:  944  Loss:  50.222160856813375\n",
      "Epoch:  945  Loss:  50.222026517247706\n",
      "Epoch:  946  Loss:  50.22189246216924\n",
      "Epoch:  947  Loss:  50.221758690675884\n",
      "Epoch:  948  Loss:  50.22162520186946\n",
      "Epoch:  949  Loss:  50.221491994855455\n",
      "Epoch:  950  Loss:  50.22135906874321\n",
      "Epoch:  951  Loss:  50.22122642264574\n",
      "Epoch:  952  Loss:  50.221094055679885\n",
      "Epoch:  953  Loss:  50.22096196696609\n",
      "Epoch:  954  Loss:  50.22083015562853\n",
      "Epoch:  955  Loss:  50.22069862079508\n",
      "Epoch:  956  Loss:  50.220567361597226\n",
      "Epoch:  957  Loss:  50.22043637717011\n",
      "Epoch:  958  Loss:  50.22030566665247\n",
      "Epoch:  959  Loss:  50.22017522918661\n",
      "Epoch:  960  Loss:  50.22004506391848\n",
      "Epoch:  961  Loss:  50.219915169997535\n",
      "Epoch:  962  Loss:  50.21978554657674\n",
      "Epoch:  963  Loss:  50.219656192812636\n",
      "Epoch:  964  Loss:  50.21952710786526\n",
      "Epoch:  965  Loss:  50.21939829089811\n",
      "Epoch:  966  Loss:  50.21926974107818\n",
      "Epoch:  967  Loss:  50.21914145757581\n",
      "Epoch:  968  Loss:  50.21901343956489\n",
      "Epoch:  969  Loss:  50.218885686222706\n",
      "Epoch:  970  Loss:  50.21875819672985\n",
      "Epoch:  971  Loss:  50.218630970270404\n",
      "Epoch:  972  Loss:  50.218504006031694\n",
      "Epoch:  973  Loss:  50.21837730320448\n",
      "Epoch:  974  Loss:  50.218250860982806\n",
      "Epoch:  975  Loss:  50.21812467856402\n",
      "Epoch:  976  Loss:  50.21799875514878\n",
      "Epoch:  977  Loss:  50.217873089940994\n",
      "Epoch:  978  Loss:  50.21774768214786\n",
      "Epoch:  979  Loss:  50.21762253097978\n",
      "Epoch:  980  Loss:  50.21749763565039\n",
      "Epoch:  981  Loss:  50.21737299537654\n",
      "Epoch:  982  Loss:  50.217248609378316\n",
      "Epoch:  983  Loss:  50.217124476878865\n",
      "Epoch:  984  Loss:  50.21700059710462\n",
      "Epoch:  985  Loss:  50.21687696928504\n",
      "Epoch:  986  Loss:  50.216753592652815\n",
      "Epoch:  987  Loss:  50.216630466443675\n",
      "Epoch:  988  Loss:  50.21650758989648\n",
      "Epoch:  989  Loss:  50.21638496225314\n",
      "Epoch:  990  Loss:  50.21626258275867\n",
      "Epoch:  991  Loss:  50.21614045066111\n",
      "Epoch:  992  Loss:  50.2160185652115\n",
      "Epoch:  993  Loss:  50.215896925663955\n",
      "Epoch:  994  Loss:  50.215775531275575\n",
      "Epoch:  995  Loss:  50.215654381306436\n",
      "Epoch:  996  Loss:  50.21553347501958\n",
      "Epoch:  997  Loss:  50.21541281168104\n",
      "Epoch:  998  Loss:  50.21529239055975\n",
      "Epoch:  999  Loss:  50.21517221092761\n",
      "['department', 'computer', 'professors', 'science', 'today']\n"
     ]
    }
   ],
   "source": [
    "text_corpus = \"\" \n",
    "text_corpus += \"Welcome to the Department of Computer Science. We have great faculty and professors. We will have a welcome program today.\"\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Data preprocessing  \n",
    "training_data = preprocessing(text_corpus)\n",
    "\n",
    "# Word2Vec\n",
    "w2v = word2vec()\n",
    "\n",
    "# Generate Training data\n",
    "generate_training_data(training_data,w2v)\n",
    "\n",
    "# Train the model\n",
    "w2v.train(epochs)\n",
    "\n",
    "# Predict using model  \n",
    "print(w2v.predict(\"welcome\", 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
