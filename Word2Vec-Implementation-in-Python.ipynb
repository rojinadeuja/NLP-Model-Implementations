{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x. This implementation provides better numerical stability.'''\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def softmax_1(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x'''\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / np.sum(e_x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(self, word):\n",
    "    '''Function to covert a word to one-hot-encoded value'''\n",
    "    word_vec = [0 for i in range(0, self.v_count)]\n",
    "    word_index = self.word_index[word]\n",
    "    word_vec[word_index] = 1\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    '''Implementation of Skip-Gram Word2Vec model'''\n",
    "    def __init__(self):\n",
    "        self.N = 10\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        \n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.001\n",
    "        \n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "    \n",
    "    def initialize(self, V, data):\n",
    "        '''Function to initialze the neural network'''\n",
    "        self.V = V\n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "            \n",
    "    def feed_forward(self, X):\n",
    "        '''Function for feed-forward step'''\n",
    "        self.h = np.dot(self.W.T, X).reshape(self.N, 1) \n",
    "        self.u = np.dot(self.W1.T, self.h)\n",
    "        self.y = softmax(self.u)   \n",
    "        return self.y\n",
    "    \n",
    "    def backpropagate(self, x, t):\n",
    "        '''Function for back propagation using Stochastic Gradient Descent step'''\n",
    "        e = self.y - np.asarray(t).reshape(self.V, 1) #e.shape is V X 1\n",
    "        # Calculate partial derivative of loss function wrt W1\n",
    "        dEdW1 = np.dot(self.h, e.T)\n",
    "        \n",
    "        X = np.array(x).reshape(self.V, 1)\n",
    "        # Calculate partial derivative of loss function wrt W\n",
    "        dEdW = np.dot(X, np.dot(self.W1, e).T)\n",
    "        \n",
    "        # Update the weights\n",
    "        self.W1 = self.W1 - self.alpha * dEdW1\n",
    "        self.W = self.W - self.alpha * dEdW\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        '''Function to train the Word2Vec model'''\n",
    "        # Loop through each epoch\n",
    "        for x in range(1,epochs):\n",
    "            \n",
    "            # Initialize Loss\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Loop through each training sample\n",
    "            for j in range(len(self.X_train)):\n",
    "                # Forward Pass\n",
    "                self.feed_forward(self.X_train[j])\n",
    "                \n",
    "                # Backpropagation\n",
    "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                        \n",
    "                # Calculate Loss        \n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u)))\n",
    "                \n",
    "            print(\"Epoch: \", x, \" Loss: \", self.loss)\n",
    "            self.alpha *= 1/((1+self.alpha*x))\n",
    "            \n",
    "    def predict(self, word, number_of_predictions):\n",
    "        '''Function to predict context words using Word2Vec model'''\n",
    "        # Check if word is contained in the dictionary\n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X)\n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i\n",
    "                \n",
    "            # Sort top context words in the output    \n",
    "            sorted_output = [] \n",
    "            for k in sorted(output, reverse=True): \n",
    "                sorted_output.append(self.words[output[k]]) \n",
    "                if(len(sorted_output)>=number_of_predictions): \n",
    "                    break\n",
    "            return sorted_output \n",
    "        else: \n",
    "            print(\"Error: Word not found in dicitonary\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    '''Function for data preprocessing'''\n",
    "    processed = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    # Split text corpus into sentences\n",
    "    sentences = corpus.split(\".\")\n",
    "    # Loop through each sentence\n",
    "    for i in range(len(sentences)):\n",
    "        # Remove leading and trailing characters\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        # Split sentence into list of words\n",
    "        sentence = sentences[i].split()\n",
    "        # Remove punctuations\n",
    "        x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "        # Convert to lower case\n",
    "        x = [word.lower() for word in x]\n",
    "        processed.append(x) \n",
    "    return processed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sentences, w2v):\n",
    "    '''Function to generate training data for Word2Vec'''\n",
    "    data = {}\n",
    "    \n",
    "    # Loop throuch each sentence\n",
    "    for sentence in sentences:\n",
    "        # Loop through each word\n",
    "        for word in sentence:\n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) # Size of Vocabulary\n",
    "    data = sorted(list(data.keys()))\n",
    "    \n",
    "    vocab = {}\n",
    "    # Store words into vocabulary\n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "    \n",
    "    print('\\nVocabulary:', vocab, '\\n')\n",
    "    \n",
    "    # Loop through each sentence \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)] \n",
    "              \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "                    \n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context)\n",
    "            \n",
    "    w2v.initialize(V, data)\n",
    "    return w2v.X_train, w2v.y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary: {'computer': 0, 'department': 1, 'faculty': 2, 'great': 3, 'professors': 4, 'program': 5, 'science': 6, 'today': 7, 'we': 8, 'welcome': 9} \n",
      "\n",
      "Epoch:  1  Loss:  58.85458113051705\n",
      "Epoch:  2  Loss:  58.78457304208971\n",
      "Epoch:  3  Loss:  58.714811439017396\n",
      "Epoch:  4  Loss:  58.64536360964764\n",
      "Epoch:  5  Loss:  58.57629535656976\n",
      "Epoch:  6  Loss:  58.50767062753499\n",
      "Epoch:  7  Loss:  58.439551170757525\n",
      "Epoch:  8  Loss:  58.37199621906913\n",
      "Epoch:  9  Loss:  58.30506220668431\n",
      "Epoch:  10  Loss:  58.2388025215414\n",
      "Epoch:  11  Loss:  58.17326729535166\n",
      "Epoch:  12  Loss:  58.108503232637844\n",
      "Epoch:  13  Loss:  58.04455347921334\n",
      "Epoch:  14  Loss:  57.98145752976072\n",
      "Epoch:  15  Loss:  57.91925117344453\n",
      "Epoch:  16  Loss:  57.85796647585016\n",
      "Epoch:  17  Loss:  57.79763179499536\n",
      "Epoch:  18  Loss:  57.73827182871855\n",
      "Epoch:  19  Loss:  57.67990769041429\n",
      "Epoch:  20  Loss:  57.62255700985579\n",
      "Epoch:  21  Loss:  57.566234055717516\n",
      "Epoch:  22  Loss:  57.510949876373346\n",
      "Epoch:  23  Loss:  57.45671245559139\n",
      "Epoch:  24  Loss:  57.40352687986157\n",
      "Epoch:  25  Loss:  57.35139551426425\n",
      "Epoch:  26  Loss:  57.30031818400402\n",
      "Epoch:  27  Loss:  57.250292358981845\n",
      "Epoch:  28  Loss:  57.20131333904744\n",
      "Epoch:  29  Loss:  57.153374437854175\n",
      "Epoch:  30  Loss:  57.1064671635203\n",
      "Epoch:  31  Loss:  57.06058139457634\n",
      "Epoch:  32  Loss:  57.01570554994374\n",
      "Epoch:  33  Loss:  56.97182675193915\n",
      "Epoch:  34  Loss:  56.92893098152785\n",
      "Epoch:  35  Loss:  56.88700322525997\n",
      "Epoch:  36  Loss:  56.84602761350844\n",
      "Epoch:  37  Loss:  56.80598754979322\n",
      "Epoch:  38  Loss:  56.76686583111672\n",
      "Epoch:  39  Loss:  56.72864475935769\n",
      "Epoch:  40  Loss:  56.69130624387022\n",
      "Epoch:  41  Loss:  56.654831895517866\n",
      "Epoch:  42  Loss:  56.61920311243777\n",
      "Epoch:  43  Loss:  56.58440115788026\n",
      "Epoch:  44  Loss:  56.55040723050679\n",
      "Epoch:  45  Loss:  56.51720252755505\n",
      "Epoch:  46  Loss:  56.48476830129455\n",
      "Epoch:  47  Loss:  56.453085909204894\n",
      "Epoch:  48  Loss:  56.42213685830856\n",
      "Epoch:  49  Loss:  56.39190284408447\n",
      "Epoch:  50  Loss:  56.36236578437913\n",
      "Epoch:  51  Loss:  56.333507848718064\n",
      "Epoch:  52  Loss:  56.30531148340502\n",
      "Epoch:  53  Loss:  56.27775943277666\n",
      "Epoch:  54  Loss:  56.250834756962426\n",
      "Epoch:  55  Loss:  56.22452084647773\n",
      "Epoch:  56  Loss:  56.19880143395865\n",
      "Epoch:  57  Loss:  56.1736606033252\n",
      "Epoch:  58  Loss:  56.14908279663964\n",
      "Epoch:  59  Loss:  56.12505281890686\n",
      "Epoch:  60  Loss:  56.10155584104397\n",
      "Epoch:  61  Loss:  56.07857740122833\n",
      "Epoch:  62  Loss:  56.05610340481513\n",
      "Epoch:  63  Loss:  56.034120122999504\n",
      "Epoch:  64  Loss:  56.0126141903823\n",
      "Epoch:  65  Loss:  55.99157260158386\n",
      "Epoch:  66  Loss:  55.97098270703693\n",
      "Epoch:  67  Loss:  55.95083220807675\n",
      "Epoch:  68  Loss:  55.93110915143495\n",
      "Epoch:  69  Loss:  55.911801923232886\n",
      "Epoch:  70  Loss:  55.892899242560496\n",
      "Epoch:  71  Loss:  55.874390154716885\n",
      "Epoch:  72  Loss:  55.856264024181556\n",
      "Epoch:  73  Loss:  55.8385105273767\n",
      "Epoch:  74  Loss:  55.821119645274656\n",
      "Epoch:  75  Loss:  55.80408165589801\n",
      "Epoch:  76  Loss:  55.78738712675423\n",
      "Epoch:  77  Loss:  55.771026907241946\n",
      "Epoch:  78  Loss:  55.754992121060305\n",
      "Epoch:  79  Loss:  55.73927415865022\n",
      "Epoch:  80  Loss:  55.723864669690954\n",
      "Epoch:  81  Loss:  55.708755555673136\n",
      "Epoch:  82  Loss:  55.69393896256617\n",
      "Epoch:  83  Loss:  55.679407273594705\n",
      "Epoch:  84  Loss:  55.66515310213704\n",
      "Epoch:  85  Loss:  55.65116928475602\n",
      "Epoch:  86  Loss:  55.637448874370925\n",
      "Epoch:  87  Loss:  55.62398513357696\n",
      "Epoch:  88  Loss:  55.610771528118505\n",
      "Epoch:  89  Loss:  55.597801720519264\n",
      "Epoch:  90  Loss:  55.58506956387292\n",
      "Epoch:  91  Loss:  55.572569095795764\n",
      "Epoch:  92  Loss:  55.56029453254261\n",
      "Epoch:  93  Loss:  55.54824026328585\n",
      "Epoch:  94  Loss:  55.53640084455764\n",
      "Epoch:  95  Loss:  55.52477099485378\n",
      "Epoch:  96  Loss:  55.513345589397815\n",
      "Epoch:  97  Loss:  55.502119655063765\n",
      "Epoch:  98  Loss:  55.491088365454665\n",
      "Epoch:  99  Loss:  55.4802470361345\n",
      "Epoch:  100  Loss:  55.4695911200108\n",
      "Epoch:  101  Loss:  55.459116202864735\n",
      "Epoch:  102  Loss:  55.448817999025096\n",
      "Epoch:  103  Loss:  55.4386923471835\n",
      "Epoch:  104  Loss:  55.428735206346744\n",
      "Epoch:  105  Loss:  55.41894265192311\n",
      "Epoch:  106  Loss:  55.409310871938715\n",
      "Epoch:  107  Loss:  55.39983616338085\n",
      "Epoch:  108  Loss:  55.39051492866395\n",
      "Epoch:  109  Loss:  55.381343672215294\n",
      "Epoch:  110  Loss:  55.37231899717634\n",
      "Epoch:  111  Loss:  55.36343760221648\n",
      "Epoch:  112  Loss:  55.35469627845543\n",
      "Epoch:  113  Loss:  55.34609190649107\n",
      "Epoch:  114  Loss:  55.33762145352913\n",
      "Epoch:  115  Loss:  55.329281970611554\n",
      "Epoch:  116  Loss:  55.321070589940035\n",
      "Epoch:  117  Loss:  55.3129845222919\n",
      "Epoch:  118  Loss:  55.305021054524914\n",
      "Epoch:  119  Loss:  55.297177547168104\n",
      "Epoch:  120  Loss:  55.28945143209567\n",
      "Epoch:  121  Loss:  55.28184021028102\n",
      "Epoch:  122  Loss:  55.274341449628274\n",
      "Epoch:  123  Loss:  55.266952782878434\n",
      "Epoch:  124  Loss:  55.259671905587595\n",
      "Epoch:  125  Loss:  55.25249657417464\n",
      "Epoch:  126  Loss:  55.24542460403606\n",
      "Epoch:  127  Loss:  55.23845386772545\n",
      "Epoch:  128  Loss:  55.23158229319523\n",
      "Epoch:  129  Loss:  55.22480786209875\n",
      "Epoch:  130  Loss:  55.21812860815005\n",
      "Epoch:  131  Loss:  55.211542615539834\n",
      "Epoch:  132  Loss:  55.20504801740499\n",
      "Epoch:  133  Loss:  55.198642994350486\n",
      "Epoch:  134  Loss:  55.19232577302084\n",
      "Epoch:  135  Loss:  55.18609462472023\n",
      "Epoch:  136  Loss:  55.179947864079004\n",
      "Epoch:  137  Loss:  55.17388384776501\n",
      "Epoch:  138  Loss:  55.1679009732381\n",
      "Epoch:  139  Loss:  55.16199767754647\n",
      "Epoch:  140  Loss:  55.156172436163054\n",
      "Epoch:  141  Loss:  55.15042376186058\n",
      "Epoch:  142  Loss:  55.144750203624014\n",
      "Epoch:  143  Loss:  55.13915034559908\n",
      "Epoch:  144  Loss:  55.13362280607529\n",
      "Epoch:  145  Loss:  55.12816623650241\n",
      "Epoch:  146  Loss:  55.12277932053945\n",
      "Epoch:  147  Loss:  55.11746077313426\n",
      "Epoch:  148  Loss:  55.11220933963374\n",
      "Epoch:  149  Loss:  55.10702379492218\n",
      "Epoch:  150  Loss:  55.101902942588225\n",
      "Epoch:  151  Loss:  55.096845614118216\n",
      "Epoch:  152  Loss:  55.09185066811569\n",
      "Epoch:  153  Loss:  55.08691698954584\n",
      "Epoch:  154  Loss:  55.082043489004086\n",
      "Epoch:  155  Loss:  55.07722910200797\n",
      "Epoch:  156  Loss:  55.07247278831146\n",
      "Epoch:  157  Loss:  55.067773531240924\n",
      "Epoch:  158  Loss:  55.06313033705216\n",
      "Epoch:  159  Loss:  55.05854223430728\n",
      "Epoch:  160  Loss:  55.05400827327154\n",
      "Epoch:  161  Loss:  55.04952752532855\n",
      "Epoch:  162  Loss:  55.045099082413785\n",
      "Epoch:  163  Loss:  55.040722056465796\n",
      "Epoch:  164  Loss:  55.03639557889401\n",
      "Epoch:  165  Loss:  55.032118800063174\n",
      "Epoch:  166  Loss:  55.027890888793394\n",
      "Epoch:  167  Loss:  55.02371103187549\n",
      "Epoch:  168  Loss:  55.01957843360104\n",
      "Epoch:  169  Loss:  55.01549231530659\n",
      "Epoch:  170  Loss:  55.01145191493184\n",
      "Epoch:  171  Loss:  55.00745648659086\n",
      "Epoch:  172  Loss:  55.003505300156036\n",
      "Epoch:  173  Loss:  54.999597640854894\n",
      "Epoch:  174  Loss:  54.99573280887845\n",
      "Epoch:  175  Loss:  54.99191011900123\n",
      "Epoch:  176  Loss:  54.98812890021276\n",
      "Epoch:  177  Loss:  54.98438849535947\n",
      "Epoch:  178  Loss:  54.98068826079737\n",
      "Epoch:  179  Loss:  54.97702756605487\n",
      "Epoch:  180  Loss:  54.9734057935051\n",
      "Epoch:  181  Loss:  54.969822338048075\n",
      "Epoch:  182  Loss:  54.96627660680164\n",
      "Epoch:  183  Loss:  54.96276801880166\n",
      "Epoch:  184  Loss:  54.95929600471058\n",
      "Epoch:  185  Loss:  54.955860006534216\n",
      "Epoch:  186  Loss:  54.95245947734677\n",
      "Epoch:  187  Loss:  54.949093881023565\n",
      "Epoch:  188  Loss:  54.9457626919812\n",
      "Epoch:  189  Loss:  54.942465394925165\n",
      "Epoch:  190  Loss:  54.93920148460435\n",
      "Epoch:  191  Loss:  54.9359704655725\n",
      "Epoch:  192  Loss:  54.93277185195629\n",
      "Epoch:  193  Loss:  54.929605167229674\n",
      "Epoch:  194  Loss:  54.92646994399467\n",
      "Epoch:  195  Loss:  54.92336572376795\n",
      "Epoch:  196  Loss:  54.92029205677341\n",
      "Epoch:  197  Loss:  54.917248501740325\n",
      "Epoch:  198  Loss:  54.914234625706854\n",
      "Epoch:  199  Loss:  54.91125000382915\n",
      "Epoch:  200  Loss:  54.9082942191954\n",
      "Epoch:  201  Loss:  54.90536686264478\n",
      "Epoch:  202  Loss:  54.90246753259153\n",
      "Epoch:  203  Loss:  54.89959583485335\n",
      "Epoch:  204  Loss:  54.89675138248476\n",
      "Epoch:  205  Loss:  54.89393379561437\n",
      "Epoch:  206  Loss:  54.89114270128702\n",
      "Epoch:  207  Loss:  54.88837773330943\n",
      "Epoch:  208  Loss:  54.8856385321004\n",
      "Epoch:  209  Loss:  54.88292474454464\n",
      "Epoch:  210  Loss:  54.8802360238504\n",
      "Epoch:  211  Loss:  54.87757202941099\n",
      "Epoch:  212  Loss:  54.87493242666953\n",
      "Epoch:  213  Loss:  54.87231688698738\n",
      "Epoch:  214  Loss:  54.869725087515995\n",
      "Epoch:  215  Loss:  54.86715671107189\n",
      "Epoch:  216  Loss:  54.86461144601478\n",
      "Epoch:  217  Loss:  54.86208898612893\n",
      "Epoch:  218  Loss:  54.859589030507294\n",
      "Epoch:  219  Loss:  54.85711128343888\n",
      "Epoch:  220  Loss:  54.85465545429854\n",
      "Epoch:  221  Loss:  54.852221257439695\n",
      "Epoch:  222  Loss:  54.849808412089885\n",
      "Epoch:  223  Loss:  54.84741664224861\n",
      "Epoch:  224  Loss:  54.84504567658783\n",
      "Epoch:  225  Loss:  54.842695248354886\n",
      "Epoch:  226  Loss:  54.84036509527785\n",
      "Epoch:  227  Loss:  54.83805495947306\n",
      "Epoch:  228  Loss:  54.83576458735503\n",
      "Epoch:  229  Loss:  54.83349372954847\n",
      "Epoch:  230  Loss:  54.83124214080246\n",
      "Epoch:  231  Loss:  54.829009579906554\n",
      "Epoch:  232  Loss:  54.82679580960917\n",
      "Epoch:  233  Loss:  54.824600596537636\n",
      "Epoch:  234  Loss:  54.82242371112032\n",
      "Epoch:  235  Loss:  54.82026492751054\n",
      "Epoch:  236  Loss:  54.818124023512205\n",
      "Epoch:  237  Loss:  54.816000780507416\n",
      "Epoch:  238  Loss:  54.81389498338548\n",
      "Epoch:  239  Loss:  54.81180642047381\n",
      "Epoch:  240  Loss:  54.80973488347028\n",
      "Epoch:  241  Loss:  54.80768016737733\n",
      "Epoch:  242  Loss:  54.80564207043736\n",
      "Epoch:  243  Loss:  54.803620394069725\n",
      "Epoch:  244  Loss:  54.8016149428094\n",
      "Epoch:  245  Loss:  54.799625524246494\n",
      "Epoch:  246  Loss:  54.79765194896777\n",
      "Epoch:  247  Loss:  54.79569403049902\n",
      "Epoch:  248  Loss:  54.793751585249055\n",
      "Epoch:  249  Loss:  54.7918244324547\n",
      "Epoch:  250  Loss:  54.78991239412731\n",
      "Epoch:  251  Loss:  54.78801529500023\n",
      "Epoch:  252  Loss:  54.78613296247748\n",
      "Epoch:  253  Loss:  54.78426522658382\n",
      "Epoch:  254  Loss:  54.78241191991553\n",
      "Epoch:  255  Loss:  54.78057287759259\n",
      "Epoch:  256  Loss:  54.77874793721185\n",
      "Epoch:  257  Loss:  54.776936938801015\n",
      "Epoch:  258  Loss:  54.77513972477399\n",
      "Epoch:  259  Loss:  54.77335613988698\n",
      "Epoch:  260  Loss:  54.771586031195405\n",
      "Epoch:  261  Loss:  54.769829248012165\n",
      "Epoch:  262  Loss:  54.76808564186645\n",
      "Epoch:  263  Loss:  54.76635506646351\n",
      "Epoch:  264  Loss:  54.76463737764544\n",
      "Epoch:  265  Loss:  54.76293243335258\n",
      "Epoch:  266  Loss:  54.761240093586\n",
      "Epoch:  267  Loss:  54.7595602203704\n",
      "Epoch:  268  Loss:  54.75789267771823\n",
      "Epoch:  269  Loss:  54.75623733159439\n",
      "Epoch:  270  Loss:  54.75459404988141\n",
      "Epoch:  271  Loss:  54.75296270234585\n",
      "Epoch:  272  Loss:  54.75134316060503\n",
      "Epoch:  273  Loss:  54.74973529809455\n",
      "Epoch:  274  Loss:  54.748138990036644\n",
      "Epoch:  275  Loss:  54.74655411340888\n",
      "Epoch:  276  Loss:  54.7449805469138\n",
      "Epoch:  277  Loss:  54.74341817094903\n",
      "Epoch:  278  Loss:  54.74186686757802\n",
      "Epoch:  279  Loss:  54.740326520501426\n",
      "Epoch:  280  Loss:  54.73879701502903\n",
      "Epoch:  281  Loss:  54.737278238052234\n",
      "Epoch:  282  Loss:  54.73577007801723\n",
      "Epoch:  283  Loss:  54.734272424898414\n",
      "Epoch:  284  Loss:  54.732785170172704\n",
      "Epoch:  285  Loss:  54.7313082067941\n",
      "Epoch:  286  Loss:  54.72984142916894\n",
      "Epoch:  287  Loss:  54.7283847331314\n",
      "Epoch:  288  Loss:  54.726938015919835\n",
      "Epoch:  289  Loss:  54.725501176153216\n",
      "Epoch:  290  Loss:  54.72407411380838\n",
      "Epoch:  291  Loss:  54.72265673019738\n",
      "Epoch:  292  Loss:  54.72124892794559\n",
      "Epoch:  293  Loss:  54.71985061097011\n",
      "Epoch:  294  Loss:  54.71846168445845\n",
      "Epoch:  295  Loss:  54.71708205484791\n",
      "Epoch:  296  Loss:  54.71571162980517\n",
      "Epoch:  297  Loss:  54.714350318206264\n",
      "Epoch:  298  Loss:  54.71299803011711\n",
      "Epoch:  299  Loss:  54.71165467677422\n",
      "Epoch:  300  Loss:  54.710320170565836\n",
      "Epoch:  301  Loss:  54.70899442501357\n",
      "Epoch:  302  Loss:  54.707677354754125\n",
      "Epoch:  303  Loss:  54.706368875521655\n",
      "Epoch:  304  Loss:  54.705068904130165\n",
      "Epoch:  305  Loss:  54.703777358456556\n",
      "Epoch:  306  Loss:  54.702494157423715\n",
      "Epoch:  307  Loss:  54.701219220984115\n",
      "Epoch:  308  Loss:  54.69995247010348\n",
      "Epoch:  309  Loss:  54.69869382674517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  310  Loss:  54.69744321385437\n",
      "Epoch:  311  Loss:  54.69620055534294\n",
      "Epoch:  312  Loss:  54.6949657760744\n",
      "Epoch:  313  Loss:  54.69373880184906\n",
      "Epoch:  314  Loss:  54.69251955938983\n",
      "Epoch:  315  Loss:  54.69130797632777\n",
      "Epoch:  316  Loss:  54.69010398118826\n",
      "Epoch:  317  Loss:  54.688907503377415\n",
      "Epoch:  318  Loss:  54.68771847316846\n",
      "Epoch:  319  Loss:  54.68653682168871\n",
      "Epoch:  320  Loss:  54.68536248090652\n",
      "Epoch:  321  Loss:  54.68419538361863\n",
      "Epoch:  322  Loss:  54.68303546343767\n",
      "Epoch:  323  Loss:  54.68188265477983\n",
      "Epoch:  324  Loss:  54.68073689285292\n",
      "Epoch:  325  Loss:  54.67959811364447\n",
      "Epoch:  326  Loss:  54.67846625391013\n",
      "Epoch:  327  Loss:  54.677341251162275\n",
      "Epoch:  328  Loss:  54.676223043658744\n",
      "Epoch:  329  Loss:  54.67511157039185\n",
      "Epoch:  330  Loss:  54.67400677107762\n",
      "Epoch:  331  Loss:  54.67290858614504\n",
      "Epoch:  332  Loss:  54.67181695672571\n",
      "Epoch:  333  Loss:  54.67073182464357\n",
      "Epoch:  334  Loss:  54.66965313240473\n",
      "Epoch:  335  Loss:  54.668580823187696\n",
      "Epoch:  336  Loss:  54.667514840833505\n",
      "Epoch:  337  Loss:  54.66645512983618\n",
      "Epoch:  338  Loss:  54.6654016353334\n",
      "Epoch:  339  Loss:  54.66435430309716\n",
      "Epoch:  340  Loss:  54.6633130795248\n",
      "Epoch:  341  Loss:  54.662277911629864\n",
      "Epoch:  342  Loss:  54.66124874703354\n",
      "Epoch:  343  Loss:  54.6602255339559\n",
      "Epoch:  344  Loss:  54.65920822120748\n",
      "Epoch:  345  Loss:  54.65819675818087\n",
      "Epoch:  346  Loss:  54.65719109484253\n",
      "Epoch:  347  Loss:  54.65619118172479\n",
      "Epoch:  348  Loss:  54.65519696991784\n",
      "Epoch:  349  Loss:  54.654208411061965\n",
      "Epoch:  350  Loss:  54.65322545733983\n",
      "Epoch:  351  Loss:  54.65224806146903\n",
      "Epoch:  352  Loss:  54.651276176694616\n",
      "Epoch:  353  Loss:  54.65030975678178\n",
      "Epoch:  354  Loss:  54.64934875600869\n",
      "Epoch:  355  Loss:  54.64839312915948\n",
      "Epoch:  356  Loss:  54.64744283151731\n",
      "Epoch:  357  Loss:  54.646497818857405\n",
      "Epoch:  358  Loss:  54.64555804744051\n",
      "Epoch:  359  Loss:  54.644623474006224\n",
      "Epoch:  360  Loss:  54.64369405576642\n",
      "Epoch:  361  Loss:  54.642769750398955\n",
      "Epoch:  362  Loss:  54.64185051604134\n",
      "Epoch:  363  Loss:  54.64093631128452\n",
      "Epoch:  364  Loss:  54.64002709516689\n",
      "Epoch:  365  Loss:  54.639122827168066\n",
      "Epoch:  366  Loss:  54.638223467203275\n",
      "Epoch:  367  Loss:  54.637328975617365\n",
      "Epoch:  368  Loss:  54.63643931317913\n",
      "Epoch:  369  Loss:  54.63555444107569\n",
      "Epoch:  370  Loss:  54.634674320907024\n",
      "Epoch:  371  Loss:  54.63379891468039\n",
      "Epoch:  372  Loss:  54.63292818480509\n",
      "Epoch:  373  Loss:  54.63206209408708\n",
      "Epoch:  374  Loss:  54.63120060572392\n",
      "Epoch:  375  Loss:  54.63034368329956\n",
      "Epoch:  376  Loss:  54.62949129077926\n",
      "Epoch:  377  Loss:  54.62864339250471\n",
      "Epoch:  378  Loss:  54.62779995318923\n",
      "Epoch:  379  Loss:  54.6269609379128\n",
      "Epoch:  380  Loss:  54.62612631211738\n",
      "Epoch:  381  Loss:  54.62529604160236\n",
      "Epoch:  382  Loss:  54.62447009251982\n",
      "Epoch:  383  Loss:  54.62364843137011\n",
      "Epoch:  384  Loss:  54.62283102499743\n",
      "Epoch:  385  Loss:  54.62201784058531\n",
      "Epoch:  386  Loss:  54.62120884565245\n",
      "Epoch:  387  Loss:  54.620404008048375\n",
      "Epoch:  388  Loss:  54.619603295949354\n",
      "Epoch:  389  Loss:  54.61880667785414\n",
      "Epoch:  390  Loss:  54.61801412258001\n",
      "Epoch:  391  Loss:  54.61722559925874\n",
      "Epoch:  392  Loss:  54.6164410773327\n",
      "Epoch:  393  Loss:  54.61566052655088\n",
      "Epoch:  394  Loss:  54.61488391696522\n",
      "Epoch:  395  Loss:  54.614111218926666\n",
      "Epoch:  396  Loss:  54.61334240308166\n",
      "Epoch:  397  Loss:  54.612577440368284\n",
      "Epoch:  398  Loss:  54.6118163020128\n",
      "Epoch:  399  Loss:  54.611058959526055\n",
      "Epoch:  400  Loss:  54.61030538470002\n",
      "Epoch:  401  Loss:  54.609555549604266\n",
      "Epoch:  402  Loss:  54.60880942658265\n",
      "Epoch:  403  Loss:  54.60806698824992\n",
      "Epoch:  404  Loss:  54.60732820748849\n",
      "Epoch:  405  Loss:  54.60659305744509\n",
      "Epoch:  406  Loss:  54.60586151152767\n",
      "Epoch:  407  Loss:  54.605133543402154\n",
      "Epoch:  408  Loss:  54.604409126989374\n",
      "Epoch:  409  Loss:  54.603688236462006\n",
      "Epoch:  410  Loss:  54.60297084624161\n",
      "Epoch:  411  Loss:  54.60225693099549\n",
      "Epoch:  412  Loss:  54.6015464656339\n",
      "Epoch:  413  Loss:  54.60083942530714\n",
      "Epoch:  414  Loss:  54.60013578540264\n",
      "Epoch:  415  Loss:  54.5994355215422\n",
      "Epoch:  416  Loss:  54.59873860957917\n",
      "Epoch:  417  Loss:  54.59804502559577\n",
      "Epoch:  418  Loss:  54.597354745900375\n",
      "Epoch:  419  Loss:  54.59666774702488\n",
      "Epoch:  420  Loss:  54.59598400572199\n",
      "Epoch:  421  Loss:  54.59530349896277\n",
      "Epoch:  422  Loss:  54.59462620393399\n",
      "Epoch:  423  Loss:  54.593952098035665\n",
      "Epoch:  424  Loss:  54.59328115887866\n",
      "Epoch:  425  Loss:  54.59261336428199\n",
      "Epoch:  426  Loss:  54.591948692270705\n",
      "Epoch:  427  Loss:  54.59128712107333\n",
      "Epoch:  428  Loss:  54.590628629119635\n",
      "Epoch:  429  Loss:  54.589973195038276\n",
      "Epoch:  430  Loss:  54.58932079765442\n",
      "Epoch:  431  Loss:  54.58867141598769\n",
      "Epoch:  432  Loss:  54.58802502924982\n",
      "Epoch:  433  Loss:  54.58738161684244\n",
      "Epoch:  434  Loss:  54.58674115835512\n",
      "Epoch:  435  Loss:  54.5861036335629\n",
      "Epoch:  436  Loss:  54.58546902242456\n",
      "Epoch:  437  Loss:  54.58483730508029\n",
      "Epoch:  438  Loss:  54.58420846184978\n",
      "Epoch:  439  Loss:  54.5835824732301\n",
      "Epoch:  440  Loss:  54.58295931989384\n",
      "Epoch:  441  Loss:  54.58233898268706\n",
      "Epoch:  442  Loss:  54.58172144262733\n",
      "Epoch:  443  Loss:  54.58110668090195\n",
      "Epoch:  444  Loss:  54.5804946788659\n",
      "Epoch:  445  Loss:  54.579885418040085\n",
      "Epoch:  446  Loss:  54.579278880109506\n",
      "Epoch:  447  Loss:  54.57867504692142\n",
      "Epoch:  448  Loss:  54.578073900483474\n",
      "Epoch:  449  Loss:  54.57747542296211\n",
      "Epoch:  450  Loss:  54.57687959668072\n",
      "Epoch:  451  Loss:  54.576286404117894\n",
      "Epoch:  452  Loss:  54.575695827905804\n",
      "Epoch:  453  Loss:  54.57510785082851\n",
      "Epoch:  454  Loss:  54.57452245582033\n",
      "Epoch:  455  Loss:  54.57393962596406\n",
      "Epoch:  456  Loss:  54.57335934448965\n",
      "Epoch:  457  Loss:  54.572781594772344\n",
      "Epoch:  458  Loss:  54.572206360331236\n",
      "Epoch:  459  Loss:  54.57163362482773\n",
      "Epoch:  460  Loss:  54.57106337206401\n",
      "Epoch:  461  Loss:  54.57049558598141\n",
      "Epoch:  462  Loss:  54.56993025065916\n",
      "Epoch:  463  Loss:  54.56936735031271\n",
      "Epoch:  464  Loss:  54.56880686929234\n",
      "Epoch:  465  Loss:  54.56824879208182\n",
      "Epoch:  466  Loss:  54.5676931032968\n",
      "Epoch:  467  Loss:  54.56713978768362\n",
      "Epoch:  468  Loss:  54.56658883011786\n",
      "Epoch:  469  Loss:  54.56604021560283\n",
      "Epoch:  470  Loss:  54.5654939292685\n",
      "Epoch:  471  Loss:  54.56494995636997\n",
      "Epoch:  472  Loss:  54.5644082822862\n",
      "Epoch:  473  Loss:  54.56386889251877\n",
      "Epoch:  474  Loss:  54.563331772690496\n",
      "Epoch:  475  Loss:  54.56279690854437\n",
      "Epoch:  476  Loss:  54.56226428594203\n",
      "Epoch:  477  Loss:  54.56173389086279\n",
      "Epoch:  478  Loss:  54.56120570940225\n",
      "Epoch:  479  Loss:  54.56067972777121\n",
      "Epoch:  480  Loss:  54.56015593229436\n",
      "Epoch:  481  Loss:  54.559634309409255\n",
      "Epoch:  482  Loss:  54.55911484566505\n",
      "Epoch:  483  Loss:  54.55859752772138\n",
      "Epoch:  484  Loss:  54.55808234234727\n",
      "Epoch:  485  Loss:  54.55756927641997\n",
      "Epoch:  486  Loss:  54.55705831692389\n",
      "Epoch:  487  Loss:  54.55654945094949\n",
      "Epoch:  488  Loss:  54.556042665692225\n",
      "Epoch:  489  Loss:  54.555537948451445\n",
      "Epoch:  490  Loss:  54.55503528662936\n",
      "Epoch:  491  Loss:  54.55453466773008\n",
      "Epoch:  492  Loss:  54.55403607935848\n",
      "Epoch:  493  Loss:  54.553539509219206\n",
      "Epoch:  494  Loss:  54.553044945115765\n",
      "Epoch:  495  Loss:  54.55255237494941\n",
      "Epoch:  496  Loss:  54.552061786718255\n",
      "Epoch:  497  Loss:  54.55157316851638\n",
      "Epoch:  498  Loss:  54.5510865085326\n",
      "Epoch:  499  Loss:  54.55060179504986\n",
      "Epoch:  500  Loss:  54.550119016444036\n",
      "Epoch:  501  Loss:  54.54963816118324\n",
      "Epoch:  502  Loss:  54.54915921782676\n",
      "Epoch:  503  Loss:  54.54868217502415\n",
      "Epoch:  504  Loss:  54.54820702151447\n",
      "Epoch:  505  Loss:  54.547733746125324\n",
      "Epoch:  506  Loss:  54.547262337771976\n",
      "Epoch:  507  Loss:  54.546792785456546\n",
      "Epoch:  508  Loss:  54.54632507826712\n",
      "Epoch:  509  Loss:  54.54585920537697\n",
      "Epoch:  510  Loss:  54.545395156043675\n",
      "Epoch:  511  Loss:  54.544932919608314\n",
      "Epoch:  512  Loss:  54.54447248549466\n",
      "Epoch:  513  Loss:  54.54401384320845\n",
      "Epoch:  514  Loss:  54.54355698233637\n",
      "Epoch:  515  Loss:  54.54310189254564\n",
      "Epoch:  516  Loss:  54.542648563582915\n",
      "Epoch:  517  Loss:  54.5421969852737\n",
      "Epoch:  518  Loss:  54.54174714752148\n",
      "Epoch:  519  Loss:  54.541299040307095\n",
      "Epoch:  520  Loss:  54.54085265368795\n",
      "Epoch:  521  Loss:  54.54040797779727\n",
      "Epoch:  522  Loss:  54.539965002843374\n",
      "Epoch:  523  Loss:  54.53952371910901\n",
      "Epoch:  524  Loss:  54.539084116950605\n",
      "Epoch:  525  Loss:  54.53864618679764\n",
      "Epoch:  526  Loss:  54.53820991915182\n",
      "Epoch:  527  Loss:  54.53777530458657\n",
      "Epoch:  528  Loss:  54.53734233374623\n",
      "Epoch:  529  Loss:  54.53691099734548\n",
      "Epoch:  530  Loss:  54.53648128616861\n",
      "Epoch:  531  Loss:  54.53605319106886\n",
      "Epoch:  532  Loss:  54.53562670296789\n",
      "Epoch:  533  Loss:  54.53520181285509\n",
      "Epoch:  534  Loss:  54.53477851178682\n",
      "Epoch:  535  Loss:  54.534356790886015\n",
      "Epoch:  536  Loss:  54.533936641341356\n",
      "Epoch:  537  Loss:  54.53351805440688\n",
      "Epoch:  538  Loss:  54.5331010214012\n",
      "Epoch:  539  Loss:  54.53268553370691\n",
      "Epoch:  540  Loss:  54.53227158277016\n",
      "Epoch:  541  Loss:  54.531859160099934\n",
      "Epoch:  542  Loss:  54.53144825726753\n",
      "Epoch:  543  Loss:  54.53103886590591\n",
      "Epoch:  544  Loss:  54.53063097770928\n",
      "Epoch:  545  Loss:  54.530224584432425\n",
      "Epoch:  546  Loss:  54.52981967789018\n",
      "Epoch:  547  Loss:  54.52941624995691\n",
      "Epoch:  548  Loss:  54.52901429256595\n",
      "Epoch:  549  Loss:  54.528613797709056\n",
      "Epoch:  550  Loss:  54.52821475743593\n",
      "Epoch:  551  Loss:  54.52781716385369\n",
      "Epoch:  552  Loss:  54.527421009126236\n",
      "Epoch:  553  Loss:  54.52702628547392\n",
      "Epoch:  554  Loss:  54.526632985172995\n",
      "Epoch:  555  Loss:  54.52624110055493\n",
      "Epoch:  556  Loss:  54.5258506240062\n",
      "Epoch:  557  Loss:  54.525461547967595\n",
      "Epoch:  558  Loss:  54.52507386493384\n",
      "Epoch:  559  Loss:  54.52468756745301\n",
      "Epoch:  560  Loss:  54.52430264812622\n",
      "Epoch:  561  Loss:  54.52391909960701\n",
      "Epoch:  562  Loss:  54.5235369146009\n",
      "Epoch:  563  Loss:  54.523156085865004\n",
      "Epoch:  564  Loss:  54.522776606207586\n",
      "Epoch:  565  Loss:  54.52239846848747\n",
      "Epoch:  566  Loss:  54.522021665613735\n",
      "Epoch:  567  Loss:  54.521646190545184\n",
      "Epoch:  568  Loss:  54.521272036290064\n",
      "Epoch:  569  Loss:  54.5208991959054\n",
      "Epoch:  570  Loss:  54.52052766249677\n",
      "Epoch:  571  Loss:  54.520157429217754\n",
      "Epoch:  572  Loss:  54.519788489269615\n",
      "Epoch:  573  Loss:  54.51942083590083\n",
      "Epoch:  574  Loss:  54.51905446240671\n",
      "Epoch:  575  Loss:  54.51868936212891\n",
      "Epoch:  576  Loss:  54.518325528455165\n",
      "Epoch:  577  Loss:  54.517962954818806\n",
      "Epoch:  578  Loss:  54.51760163469839\n",
      "Epoch:  579  Loss:  54.517241561617304\n",
      "Epoch:  580  Loss:  54.516882729143404\n",
      "Epoch:  581  Loss:  54.51652513088862\n",
      "Epoch:  582  Loss:  54.51616876050855\n",
      "Epoch:  583  Loss:  54.5158136117021\n",
      "Epoch:  584  Loss:  54.515459678211236\n",
      "Epoch:  585  Loss:  54.51510695382042\n",
      "Epoch:  586  Loss:  54.514755432356324\n",
      "Epoch:  587  Loss:  54.51440510768761\n",
      "Epoch:  588  Loss:  54.514055973724346\n",
      "Epoch:  589  Loss:  54.513708024417824\n",
      "Epoch:  590  Loss:  54.51336125376013\n",
      "Epoch:  591  Loss:  54.51301565578387\n",
      "Epoch:  592  Loss:  54.51267122456175\n",
      "Epoch:  593  Loss:  54.51232795420628\n",
      "Epoch:  594  Loss:  54.51198583886949\n",
      "Epoch:  595  Loss:  54.51164487274249\n",
      "Epoch:  596  Loss:  54.51130505005524\n",
      "Epoch:  597  Loss:  54.51096636507615\n",
      "Epoch:  598  Loss:  54.510628812111904\n",
      "Epoch:  599  Loss:  54.510292385506894\n",
      "Epoch:  600  Loss:  54.509957079643186\n",
      "Epoch:  601  Loss:  54.50962288893996\n",
      "Epoch:  602  Loss:  54.50928980785344\n",
      "Epoch:  603  Loss:  54.50895783087638\n",
      "Epoch:  604  Loss:  54.50862695253787\n",
      "Epoch:  605  Loss:  54.50829716740307\n",
      "Epoch:  606  Loss:  54.50796847007279\n",
      "Epoch:  607  Loss:  54.507640855183325\n",
      "Epoch:  608  Loss:  54.50731431740607\n",
      "Epoch:  609  Loss:  54.50698885144733\n",
      "Epoch:  610  Loss:  54.50666445204796\n",
      "Epoch:  611  Loss:  54.506341113983055\n",
      "Epoch:  612  Loss:  54.50601883206176\n",
      "Epoch:  613  Loss:  54.50569760112701\n",
      "Epoch:  614  Loss:  54.50537741605512\n",
      "Epoch:  615  Loss:  54.50505827175562\n",
      "Epoch:  616  Loss:  54.50474016317098\n",
      "Epoch:  617  Loss:  54.50442308527634\n",
      "Epoch:  618  Loss:  54.504107033079165\n",
      "Epoch:  619  Loss:  54.503792001619175\n",
      "Epoch:  620  Loss:  54.50347798596786\n",
      "Epoch:  621  Loss:  54.503164981228366\n",
      "Epoch:  622  Loss:  54.50285298253527\n",
      "Epoch:  623  Loss:  54.50254198505417\n",
      "Epoch:  624  Loss:  54.502231983981595\n",
      "Epoch:  625  Loss:  54.501922974544684\n",
      "Epoch:  626  Loss:  54.50161495200096\n",
      "Epoch:  627  Loss:  54.501307911638115\n",
      "Epoch:  628  Loss:  54.50100184877369\n",
      "Epoch:  629  Loss:  54.50069675875495\n",
      "Epoch:  630  Loss:  54.50039263695854\n",
      "Epoch:  631  Loss:  54.50008947879033\n",
      "Epoch:  632  Loss:  54.4997872796852\n",
      "Epoch:  633  Loss:  54.49948603510673\n",
      "Epoch:  634  Loss:  54.499185740546956\n",
      "Epoch:  635  Loss:  54.49888639152636\n",
      "Epoch:  636  Loss:  54.49858798359336\n",
      "Epoch:  637  Loss:  54.49829051232432\n",
      "Epoch:  638  Loss:  54.497993973323155\n",
      "Epoch:  639  Loss:  54.4976983622213\n",
      "Epoch:  640  Loss:  54.49740367467733\n",
      "Epoch:  641  Loss:  54.49710990637681\n",
      "Epoch:  642  Loss:  54.49681705303214\n",
      "Epoch:  643  Loss:  54.49652511038227\n",
      "Epoch:  644  Loss:  54.49623407419255\n",
      "Epoch:  645  Loss:  54.495943940254456\n",
      "Epoch:  646  Loss:  54.4956547043855\n",
      "Epoch:  647  Loss:  54.49536636242891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  648  Loss:  54.49507891025352\n",
      "Epoch:  649  Loss:  54.49479234375352\n",
      "Epoch:  650  Loss:  54.49450665884827\n",
      "Epoch:  651  Loss:  54.49422185148218\n",
      "Epoch:  652  Loss:  54.49393791762439\n",
      "Epoch:  653  Loss:  54.4936548532687\n",
      "Epoch:  654  Loss:  54.493372654433294\n",
      "Epoch:  655  Loss:  54.49309131716067\n",
      "Epoch:  656  Loss:  54.49281083751722\n",
      "Epoch:  657  Loss:  54.49253121159341\n",
      "Epoch:  658  Loss:  54.49225243550325\n",
      "Epoch:  659  Loss:  54.49197450538431\n",
      "Epoch:  660  Loss:  54.491697417397525\n",
      "Epoch:  661  Loss:  54.49142116772696\n",
      "Epoch:  662  Loss:  54.491145752579634\n",
      "Epoch:  663  Loss:  54.490871168185414\n",
      "Epoch:  664  Loss:  54.49059741079682\n",
      "Epoch:  665  Loss:  54.49032447668885\n",
      "Epoch:  666  Loss:  54.49005236215874\n",
      "Epoch:  667  Loss:  54.48978106352593\n",
      "Epoch:  668  Loss:  54.48951057713183\n",
      "Epoch:  669  Loss:  54.48924089933963\n",
      "Epoch:  670  Loss:  54.48897202653416\n",
      "Epoch:  671  Loss:  54.48870395512176\n",
      "Epoch:  672  Loss:  54.48843668153012\n",
      "Epoch:  673  Loss:  54.48817020220806\n",
      "Epoch:  674  Loss:  54.4879045136254\n",
      "Epoch:  675  Loss:  54.48763961227287\n",
      "Epoch:  676  Loss:  54.487375494661876\n",
      "Epoch:  677  Loss:  54.4871121573244\n",
      "Epoch:  678  Loss:  54.48684959681276\n",
      "Epoch:  679  Loss:  54.486587809699614\n",
      "Epoch:  680  Loss:  54.486326792577685\n",
      "Epoch:  681  Loss:  54.4860665420596\n",
      "Epoch:  682  Loss:  54.48580705477787\n",
      "Epoch:  683  Loss:  54.4855483273847\n",
      "Epoch:  684  Loss:  54.48529035655171\n",
      "Epoch:  685  Loss:  54.48503313897\n",
      "Epoch:  686  Loss:  54.48477667134982\n",
      "Epoch:  687  Loss:  54.48452095042064\n",
      "Epoch:  688  Loss:  54.48426597293077\n",
      "Epoch:  689  Loss:  54.48401173564742\n",
      "Epoch:  690  Loss:  54.483758235356476\n",
      "Epoch:  691  Loss:  54.48350546886239\n",
      "Epoch:  692  Loss:  54.483253432988015\n",
      "Epoch:  693  Loss:  54.48300212457449\n",
      "Epoch:  694  Loss:  54.48275154048113\n",
      "Epoch:  695  Loss:  54.48250167758529\n",
      "Epoch:  696  Loss:  54.48225253278218\n",
      "Epoch:  697  Loss:  54.482004102984824\n",
      "Epoch:  698  Loss:  54.48175638512388\n",
      "Epoch:  699  Loss:  54.48150937614752\n",
      "Epoch:  700  Loss:  54.481263073021296\n",
      "Epoch:  701  Loss:  54.48101747272807\n",
      "Epoch:  702  Loss:  54.480772572267796\n",
      "Epoch:  703  Loss:  54.48052836865753\n",
      "Epoch:  704  Loss:  54.480284858931135\n",
      "Epoch:  705  Loss:  54.48004204013938\n",
      "Epoch:  706  Loss:  54.479799909349616\n",
      "Epoch:  707  Loss:  54.47955846364577\n",
      "Epoch:  708  Loss:  54.47931770012818\n",
      "Epoch:  709  Loss:  54.479077615913546\n",
      "Epoch:  710  Loss:  54.47883820813476\n",
      "Epoch:  711  Loss:  54.47859947394077\n",
      "Epoch:  712  Loss:  54.478361410496554\n",
      "Epoch:  713  Loss:  54.47812401498294\n",
      "Epoch:  714  Loss:  54.477887284596484\n",
      "Epoch:  715  Loss:  54.477651216549404\n",
      "Epoch:  716  Loss:  54.47741580806948\n",
      "Epoch:  717  Loss:  54.47718105639992\n",
      "Epoch:  718  Loss:  54.47694695879919\n",
      "Epoch:  719  Loss:  54.47671351254104\n",
      "Epoch:  720  Loss:  54.47648071491433\n",
      "Epoch:  721  Loss:  54.47624856322288\n",
      "Epoch:  722  Loss:  54.47601705478546\n",
      "Epoch:  723  Loss:  54.4757861869356\n",
      "Epoch:  724  Loss:  54.475555957021584\n",
      "Epoch:  725  Loss:  54.47532636240621\n",
      "Epoch:  726  Loss:  54.47509740046687\n",
      "Epoch:  727  Loss:  54.474869068595275\n",
      "Epoch:  728  Loss:  54.474641364197495\n",
      "Epoch:  729  Loss:  54.47441428469371\n",
      "Epoch:  730  Loss:  54.47418782751836\n",
      "Epoch:  731  Loss:  54.473961990119726\n",
      "Epoch:  732  Loss:  54.473736769960105\n",
      "Epoch:  733  Loss:  54.473512164515625\n",
      "Epoch:  734  Loss:  54.473288171276025\n",
      "Epoch:  735  Loss:  54.47306478774478\n",
      "Epoch:  736  Loss:  54.472842011438914\n",
      "Epoch:  737  Loss:  54.47261983988882\n",
      "Epoch:  738  Loss:  54.47239827063828\n",
      "Epoch:  739  Loss:  54.47217730124438\n",
      "Epoch:  740  Loss:  54.471956929277326\n",
      "Epoch:  741  Loss:  54.47173715232049\n",
      "Epoch:  742  Loss:  54.47151796797013\n",
      "Epoch:  743  Loss:  54.47129937383552\n",
      "Epoch:  744  Loss:  54.47108136753874\n",
      "Epoch:  745  Loss:  54.4708639467146\n",
      "Epoch:  746  Loss:  54.470647109010514\n",
      "Epoch:  747  Loss:  54.47043085208655\n",
      "Epoch:  748  Loss:  54.47021517361527\n",
      "Epoch:  749  Loss:  54.47000007128154\n",
      "Epoch:  750  Loss:  54.469785542782674\n",
      "Epoch:  751  Loss:  54.46957158582812\n",
      "Epoch:  752  Loss:  54.46935819813955\n",
      "Epoch:  753  Loss:  54.46914537745067\n",
      "Epoch:  754  Loss:  54.4689331215073\n",
      "Epoch:  755  Loss:  54.46872142806697\n",
      "Epoch:  756  Loss:  54.46851029489926\n",
      "Epoch:  757  Loss:  54.4682997197854\n",
      "Epoch:  758  Loss:  54.46808970051834\n",
      "Epoch:  759  Loss:  54.46788023490263\n",
      "Epoch:  760  Loss:  54.46767132075436\n",
      "Epoch:  761  Loss:  54.4674629559011\n",
      "Epoch:  762  Loss:  54.467255138181756\n",
      "Epoch:  763  Loss:  54.46704786544652\n",
      "Epoch:  764  Loss:  54.46684113555695\n",
      "Epoch:  765  Loss:  54.46663494638562\n",
      "Epoch:  766  Loss:  54.466429295816276\n",
      "Epoch:  767  Loss:  54.46622418174366\n",
      "Epoch:  768  Loss:  54.46601960207346\n",
      "Epoch:  769  Loss:  54.465815554722205\n",
      "Epoch:  770  Loss:  54.4656120376173\n",
      "Epoch:  771  Loss:  54.4654090486968\n",
      "Epoch:  772  Loss:  54.4652065859095\n",
      "Epoch:  773  Loss:  54.465004647214734\n",
      "Epoch:  774  Loss:  54.46480323058243\n",
      "Epoch:  775  Loss:  54.46460233399287\n",
      "Epoch:  776  Loss:  54.46440195543684\n",
      "Epoch:  777  Loss:  54.464202092915386\n",
      "Epoch:  778  Loss:  54.46400274443984\n",
      "Epoch:  779  Loss:  54.46380390803174\n",
      "Epoch:  780  Loss:  54.463605581722696\n",
      "Epoch:  781  Loss:  54.46340776355442\n",
      "Epoch:  782  Loss:  54.46321045157867\n",
      "Epoch:  783  Loss:  54.46301364385707\n",
      "Epoch:  784  Loss:  54.46281733846113\n",
      "Epoch:  785  Loss:  54.46262153347217\n",
      "Epoch:  786  Loss:  54.46242622698133\n",
      "Epoch:  787  Loss:  54.46223141708932\n",
      "Epoch:  788  Loss:  54.46203710190658\n",
      "Epoch:  789  Loss:  54.46184327955303\n",
      "Epoch:  790  Loss:  54.461649948158154\n",
      "Epoch:  791  Loss:  54.46145710586086\n",
      "Epoch:  792  Loss:  54.46126475080943\n",
      "Epoch:  793  Loss:  54.46107288116147\n",
      "Epoch:  794  Loss:  54.4608814950839\n",
      "Epoch:  795  Loss:  54.46069059075278\n",
      "Epoch:  796  Loss:  54.46050016635339\n",
      "Epoch:  797  Loss:  54.460310220080004\n",
      "Epoch:  798  Loss:  54.46012075013606\n",
      "Epoch:  799  Loss:  54.4599317547339\n",
      "Epoch:  800  Loss:  54.459743232094795\n",
      "Epoch:  801  Loss:  54.4595551804489\n",
      "Epoch:  802  Loss:  54.45936759803517\n",
      "Epoch:  803  Loss:  54.45918048310135\n",
      "Epoch:  804  Loss:  54.45899383390388\n",
      "Epoch:  805  Loss:  54.45880764870779\n",
      "Epoch:  806  Loss:  54.45862192578681\n",
      "Epoch:  807  Loss:  54.458436663423115\n",
      "Epoch:  808  Loss:  54.45825185990749\n",
      "Epoch:  809  Loss:  54.45806751353899\n",
      "Epoch:  810  Loss:  54.457883622625246\n",
      "Epoch:  811  Loss:  54.45770018548209\n",
      "Epoch:  812  Loss:  54.45751720043369\n",
      "Epoch:  813  Loss:  54.45733466581246\n",
      "Epoch:  814  Loss:  54.45715257995895\n",
      "Epoch:  815  Loss:  54.45697094122188\n",
      "Epoch:  816  Loss:  54.45678974795806\n",
      "Epoch:  817  Loss:  54.45660899853228\n",
      "Epoch:  818  Loss:  54.45642869131742\n",
      "Epoch:  819  Loss:  54.45624882469417\n",
      "Epoch:  820  Loss:  54.45606939705121\n",
      "Epoch:  821  Loss:  54.45589040678496\n",
      "Epoch:  822  Loss:  54.45571185229977\n",
      "Epoch:  823  Loss:  54.45553373200761\n",
      "Epoch:  824  Loss:  54.4553560443282\n",
      "Epoch:  825  Loss:  54.45517878768893\n",
      "Epoch:  826  Loss:  54.45500196052475\n",
      "Epoch:  827  Loss:  54.45482556127819\n",
      "Epoch:  828  Loss:  54.45464958839935\n",
      "Epoch:  829  Loss:  54.45447404034566\n",
      "Epoch:  830  Loss:  54.45429891558213\n",
      "Epoch:  831  Loss:  54.45412421258106\n",
      "Epoch:  832  Loss:  54.45394992982212\n",
      "Epoch:  833  Loss:  54.45377606579225\n",
      "Epoch:  834  Loss:  54.453602618985634\n",
      "Epoch:  835  Loss:  54.45342958790368\n",
      "Epoch:  836  Loss:  54.45325697105496\n",
      "Epoch:  837  Loss:  54.453084766955136\n",
      "Epoch:  838  Loss:  54.452912974127\n",
      "Epoch:  839  Loss:  54.452741591100306\n",
      "Epoch:  840  Loss:  54.452570616411855\n",
      "Epoch:  841  Loss:  54.452400048605405\n",
      "Epoch:  842  Loss:  54.452229886231535\n",
      "Epoch:  843  Loss:  54.45206012784783\n",
      "Epoch:  844  Loss:  54.451890772018615\n",
      "Epoch:  845  Loss:  54.45172181731505\n",
      "Epoch:  846  Loss:  54.45155326231494\n",
      "Epoch:  847  Loss:  54.45138510560291\n",
      "Epoch:  848  Loss:  54.451217345770246\n",
      "Epoch:  849  Loss:  54.4510499814148\n",
      "Epoch:  850  Loss:  54.45088301114106\n",
      "Epoch:  851  Loss:  54.45071643356007\n",
      "Epoch:  852  Loss:  54.45055024728934\n",
      "Epoch:  853  Loss:  54.45038445095293\n",
      "Epoch:  854  Loss:  54.45021904318127\n",
      "Epoch:  855  Loss:  54.450054022611226\n",
      "Epoch:  856  Loss:  54.44988938788603\n",
      "Epoch:  857  Loss:  54.44972513765521\n",
      "Epoch:  858  Loss:  54.449561270574584\n",
      "Epoch:  859  Loss:  54.44939778530628\n",
      "Epoch:  860  Loss:  54.44923468051859\n",
      "Epoch:  861  Loss:  54.44907195488599\n",
      "Epoch:  862  Loss:  54.4489096070891\n",
      "Epoch:  863  Loss:  54.448747635814705\n",
      "Epoch:  864  Loss:  54.448586039755554\n",
      "Epoch:  865  Loss:  54.44842481761052\n",
      "Epoch:  866  Loss:  54.4482639680845\n",
      "Epoch:  867  Loss:  54.448103489888226\n",
      "Epoch:  868  Loss:  54.44794338173856\n",
      "Epoch:  869  Loss:  54.447783642358075\n",
      "Epoch:  870  Loss:  54.447624270475345\n",
      "Epoch:  871  Loss:  54.44746526482474\n",
      "Epoch:  872  Loss:  54.44730662414636\n",
      "Epoch:  873  Loss:  54.4471483471862\n",
      "Epoch:  874  Loss:  54.44699043269586\n",
      "Epoch:  875  Loss:  54.44683287943276\n",
      "Epoch:  876  Loss:  54.44667568615986\n",
      "Epoch:  877  Loss:  54.446518851645905\n",
      "Epoch:  878  Loss:  54.44636237466509\n",
      "Epoch:  879  Loss:  54.44620625399734\n",
      "Epoch:  880  Loss:  54.44605048842797\n",
      "Epoch:  881  Loss:  54.445895076747895\n",
      "Epoch:  882  Loss:  54.44574001775349\n",
      "Epoch:  883  Loss:  54.4455853102466\n",
      "Epoch:  884  Loss:  54.445430953034425\n",
      "Epoch:  885  Loss:  54.44527694492962\n",
      "Epoch:  886  Loss:  54.445123284750096\n",
      "Epoch:  887  Loss:  54.444969971319274\n",
      "Epoch:  888  Loss:  54.44481700346565\n",
      "Epoch:  889  Loss:  54.444664380023156\n",
      "Epoch:  890  Loss:  54.44451209983083\n",
      "Epoch:  891  Loss:  54.44436016173306\n",
      "Epoch:  892  Loss:  54.44420856457926\n",
      "Epoch:  893  Loss:  54.444057307224114\n",
      "Epoch:  894  Loss:  54.44390638852735\n",
      "Epoch:  895  Loss:  54.44375580735384\n",
      "Epoch:  896  Loss:  54.4436055625735\n",
      "Epoch:  897  Loss:  54.443455653061264\n",
      "Epoch:  898  Loss:  54.44330607769708\n",
      "Epoch:  899  Loss:  54.4431568353659\n",
      "Epoch:  900  Loss:  54.44300792495759\n",
      "Epoch:  901  Loss:  54.44285934536698\n",
      "Epoch:  902  Loss:  54.44271109549379\n",
      "Epoch:  903  Loss:  54.44256317424255\n",
      "Epoch:  904  Loss:  54.442415580522706\n",
      "Epoch:  905  Loss:  54.442268313248505\n",
      "Epoch:  906  Loss:  54.44212137133898\n",
      "Epoch:  907  Loss:  54.44197475371786\n",
      "Epoch:  908  Loss:  54.441828459313776\n",
      "Epoch:  909  Loss:  54.441682487059886\n",
      "Epoch:  910  Loss:  54.44153683589414\n",
      "Epoch:  911  Loss:  54.44139150475912\n",
      "Epoch:  912  Loss:  54.441246492602055\n",
      "Epoch:  913  Loss:  54.44110179837475\n",
      "Epoch:  914  Loss:  54.44095742103366\n",
      "Epoch:  915  Loss:  54.44081335953968\n",
      "Epoch:  916  Loss:  54.4406696128584\n",
      "Epoch:  917  Loss:  54.440526179959825\n",
      "Epoch:  918  Loss:  54.44038305981835\n",
      "Epoch:  919  Loss:  54.44024025141304\n",
      "Epoch:  920  Loss:  54.44009775372727\n",
      "Epoch:  921  Loss:  54.4399555657488\n",
      "Epoch:  922  Loss:  54.439813686469876\n",
      "Epoch:  923  Loss:  54.43967211488703\n",
      "Epoch:  924  Loss:  54.439530850001184\n",
      "Epoch:  925  Loss:  54.43938989081753\n",
      "Epoch:  926  Loss:  54.4392492363456\n",
      "Epoch:  927  Loss:  54.439108885599175\n",
      "Epoch:  928  Loss:  54.43896883759628\n",
      "Epoch:  929  Loss:  54.43882909135916\n",
      "Epoch:  930  Loss:  54.43868964591429\n",
      "Epoch:  931  Loss:  54.4385505002923\n",
      "Epoch:  932  Loss:  54.438411653528014\n",
      "Epoch:  933  Loss:  54.43827310466034\n",
      "Epoch:  934  Loss:  54.4381348527323\n",
      "Epoch:  935  Loss:  54.43799689679107\n",
      "Epoch:  936  Loss:  54.43785923588784\n",
      "Epoch:  937  Loss:  54.43772186907788\n",
      "Epoch:  938  Loss:  54.437584795420435\n",
      "Epoch:  939  Loss:  54.437448013978816\n",
      "Epoch:  940  Loss:  54.437311523820284\n",
      "Epoch:  941  Loss:  54.437175324016074\n",
      "Epoch:  942  Loss:  54.437039413641386\n",
      "Epoch:  943  Loss:  54.43690379177522\n",
      "Epoch:  944  Loss:  54.43676845750068\n",
      "Epoch:  945  Loss:  54.43663340990457\n",
      "Epoch:  946  Loss:  54.43649864807765\n",
      "Epoch:  947  Loss:  54.436364171114484\n",
      "Epoch:  948  Loss:  54.43622997811344\n",
      "Epoch:  949  Loss:  54.436096068176724\n",
      "Epoch:  950  Loss:  54.43596244041031\n",
      "Epoch:  951  Loss:  54.43582909392389\n",
      "Epoch:  952  Loss:  54.435696027830915\n",
      "Epoch:  953  Loss:  54.43556324124862\n",
      "Epoch:  954  Loss:  54.43543073329783\n",
      "Epoch:  955  Loss:  54.435298503103155\n",
      "Epoch:  956  Loss:  54.435166549792754\n",
      "Epoch:  957  Loss:  54.43503487249852\n",
      "Epoch:  958  Loss:  54.43490347035593\n",
      "Epoch:  959  Loss:  54.43477234250411\n",
      "Epoch:  960  Loss:  54.43464148808567\n",
      "Epoch:  961  Loss:  54.434510906246906\n",
      "Epoch:  962  Loss:  54.43438059613758\n",
      "Epoch:  963  Loss:  54.43425055691101\n",
      "Epoch:  964  Loss:  54.43412078772404\n",
      "Epoch:  965  Loss:  54.43399128773699\n",
      "Epoch:  966  Loss:  54.43386205611362\n",
      "Epoch:  967  Loss:  54.43373309202123\n",
      "Epoch:  968  Loss:  54.43360439463049\n",
      "Epoch:  969  Loss:  54.433475963115555\n",
      "Epoch:  970  Loss:  54.433347796653884\n",
      "Epoch:  971  Loss:  54.43321989442646\n",
      "Epoch:  972  Loss:  54.43309225561748\n",
      "Epoch:  973  Loss:  54.43296487941464\n",
      "Epoch:  974  Loss:  54.432837765008856\n",
      "Epoch:  975  Loss:  54.43271091159441\n",
      "Epoch:  976  Loss:  54.43258431836892\n",
      "Epoch:  977  Loss:  54.432457984533215\n",
      "Epoch:  978  Loss:  54.432331909291435\n",
      "Epoch:  979  Loss:  54.43220609185098\n",
      "Epoch:  980  Loss:  54.43208053142243\n",
      "Epoch:  981  Loss:  54.43195522721967\n",
      "Epoch:  982  Loss:  54.43183017845968\n",
      "Epoch:  983  Loss:  54.431705384362694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  984  Loss:  54.43158084415211\n",
      "Epoch:  985  Loss:  54.43145655705449\n",
      "Epoch:  986  Loss:  54.43133252229946\n",
      "Epoch:  987  Loss:  54.43120873911986\n",
      "Epoch:  988  Loss:  54.43108520675154\n",
      "Epoch:  989  Loss:  54.43096192443356\n",
      "Epoch:  990  Loss:  54.43083889140794\n",
      "Epoch:  991  Loss:  54.430716106919775\n",
      "Epoch:  992  Loss:  54.43059357021728\n",
      "Epoch:  993  Loss:  54.43047128055166\n",
      "Epoch:  994  Loss:  54.43034923717704\n",
      "Epoch:  995  Loss:  54.43022743935071\n",
      "Epoch:  996  Loss:  54.430105886332804\n",
      "Epoch:  997  Loss:  54.429984577386456\n",
      "Epoch:  998  Loss:  54.42986351177781\n",
      "Epoch:  999  Loss:  54.4297426887759\n",
      "['professors', 'we', 'department', 'today', 'program']\n"
     ]
    }
   ],
   "source": [
    "text_corpus = \"Welcome to the Department of Computer Science. We have great faculty and professors. We will have a welcome program today.\"\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Data preprocessing  \n",
    "training_data = preprocessing(text_corpus)\n",
    "\n",
    "# Word2Vec\n",
    "w2v = word2vec()\n",
    "\n",
    "# Generate Training data\n",
    "generate_training_data(training_data,w2v)\n",
    "\n",
    "# Train the model\n",
    "w2v.train(epochs)\n",
    "\n",
    "# Predict using model  \n",
    "print(w2v.predict(\"welcome\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
