{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Drive for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "xBOmEp_zYY06",
    "outputId": "cd0f25b5-6e86-4971-8b1d-9d1f5813f61b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPzDO_rZYrlH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvN2yv6lyrBX"
   },
   "outputs": [],
   "source": [
    "def load_vectors(file):\n",
    "    '''Function to load the the word embedding matrix values'''\n",
    "    with open(file, 'r', encoding=\"utf-8\") as file:\n",
    "        # unique words\n",
    "        words = set()\n",
    "        word_to_vec = {}\n",
    "        # each line starts with a word then the values for the different features\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            # take the word \n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            # rest of the features for the word\n",
    "            word_to_vec[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxfqCw99-f-f"
   },
   "outputs": [],
   "source": [
    "# Get the words and their vectors from the Wikipedia 2014 dataset (6 Billions) and emedding size = 100\n",
    "# words, word_to_vec = load_vectors('/content/drive/My Drive/Colab Notebooks/glove.6B.100d.txt')\n",
    "words, word_to_vec = load_vectors('Datasets/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity (Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZPxwi84_0K6"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    '''Function to find the cosine similarity between two word vectors'''\n",
    "    distance = 0.0\n",
    "    # The cosine similarity measures the angle between two vectors, and has the property that it only considers the direction of the vectors, not their the magnitudes\n",
    "    # find the dot product between u and v \n",
    "    dot = np.dot(u,v)\n",
    "    # find the L2 norm of u \n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    # Compute the cosine similarity\n",
    "    cosine_sim = dot/(norm_u)/norm_v\n",
    "    \n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "ZFNDjjvC_4Ui",
    "outputId": "d4b203fb-f893-4b65-f46f-2136a1de30db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The cosine similarity between Nepal and Kathmandu is 0.7068282980813972\n",
      "\n",
      "The cosine similarity between Nepal and Paris is 0.09641994220643965\n"
     ]
    }
   ],
   "source": [
    "# Get some sample word vectors and find their cosine similarity\n",
    "nepal = word_to_vec[\"nepal\"]\n",
    "kathmandu = word_to_vec[\"kathmandu\"]\n",
    "print(\"\\nThe cosine similarity between Nepal and Kathmandu is\", cosine_similarity(nepal, kathmandu))\n",
    "\n",
    "nepal = word_to_vec[\"nepal\"]\n",
    "sea = word_to_vec[\"paris\"]\n",
    "print(\"\\nThe cosine similarity between Nepal and Paris is\", cosine_similarity(nepal, sea))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mikolov et al. Word Analogy Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONiCh5MGBOhE"
   },
   "outputs": [],
   "source": [
    "def word_analogy(a, b, c, word_to_vec):\n",
    "    '''Function to find the word analogy based on question: a is to b as c is to __?'''\n",
    "    # convert words to lower case\n",
    "    a = a.lower()\n",
    "    b = b.lower()\n",
    "    c = c.lower()\n",
    "    \n",
    "    # find the word embeddings for a, b, c\n",
    "    e_a, e_b, e_c = word_to_vec[a], word_to_vec[b], word_to_vec[c]\n",
    "    \n",
    "    words = word_to_vec.keys()\n",
    "    max_cosine_sim = -999              \n",
    "    d = None                  \n",
    "\n",
    "    # search for word_d in the whole word vector set\n",
    "    for w in words:        \n",
    "        # ignore input words\n",
    "        if w in [a, b, c] :\n",
    "            continue\n",
    "\n",
    "        # Compute cosine similarity between the vectors u and v\n",
    "        #u:(e_b - e_a) \n",
    "        #v:((w's vector representation) - e_c)\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec[w] - e_c)\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            # update word_d\n",
    "            d = w\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "wYU2CfvwBeez",
    "outputId": "b48b8f1a-ba3f-44ca-ffb1-34e9ddfddf05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> athens is to greece as berlin is to germany\n"
     ]
    }
   ],
   "source": [
    "# Get three sample word vectors that are analogous and find the fourth word\n",
    "samples = [('athens','greece','berlin')]\n",
    "for sample in samples:\n",
    "    print ('\\n> {} is to {} as {} is to {}'.format( *sample, word_analogy(*sample, word_to_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "0s63Wkp-DyjM",
    "outputId": "008d34f2-9357-493d-a371-f614d17b9377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> dance is to dancing as fly is to donkeys\n"
     ]
    }
   ],
   "source": [
    "# Get three sample word vectors that are analogous and find the fourth word\n",
    "samples = [('dance','dancing','fly')]\n",
    "for sample in samples:\n",
    "    print ('\\n> {} is to {} as {} is to {}'.format( *sample, word_analogy(*sample, word_to_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy Task with User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGo2CGdmMQfz"
   },
   "outputs": [],
   "source": [
    "def word_analogy_manual():\n",
    "    '''Function that takes a,b,c words from user and prints d'''\n",
    "    \n",
    "    print('\\nWord Analogy Task (aka a is to b as c is to d)')\n",
    "\n",
    "    print('\\n> Enter words for a, b, c separated by commas')\n",
    "    words = input().split(',')\n",
    "    d = word_analogy(*words, word_to_vec)\n",
    "    print ('\\n> {} is to {} as {} is to {}'.format( *words, d))\n",
    "    print('\\n> Analogous word is: ' + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "xsMOms2LOIlt",
    "outputId": "a08f1a50-146f-4a0d-d52c-68edb1f11f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Analogy Task (aka a is to b as c is to d)\n",
      "\n",
      "> Enter words for a, b, c separated by commas\n",
      "waiter,waitress,actor\n",
      "\n",
      "> waiter is to waitress as actor is to actress\n",
      "\n",
      "> Analogous word is: actress\n"
     ]
    }
   ],
   "source": [
    "# Get input words form user for word analogy task\n",
    "word_analogy_manual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Word Analogy (using TorchText)\n",
    "To load pre-trained GloVe embeddings, we'll use a package called torchtext. The documentation for torchtext GloVe vectors are available at: https://torchtext.readthedocs.io/en/latest/vocab.html#glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For import torch not working: !pip3 install torch==1.5.0+cpu torchvision==0.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "# The first time you run this will download a ~823MB file\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
    "                              dim=50)   # embedding size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6014,  0.2852, -0.0320, -0.4303,  0.7481,  0.2622, -0.9736,  0.0786,\n",
       "        -0.5759, -1.1880, -1.8507, -0.2489,  0.0555,  0.0086,  0.0680,  0.4055,\n",
       "        -0.0740, -0.2132,  0.3717, -0.7179,  1.2234,  0.3555, -0.4154, -0.2193,\n",
       "        -0.3966, -1.7831, -0.4151,  0.2953, -0.4125,  0.0201,  2.7425, -0.9926,\n",
       "        -0.7103, -0.4681,  0.2826, -0.0776,  0.3041, -0.0664,  0.3951, -0.7075,\n",
       "        -0.3889,  0.2316, -0.4951,  0.1461, -0.0231,  0.5639, -0.8619, -1.0278,\n",
       "         0.0399,  0.2002])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print embeddings of a sample word\n",
    "glove['house']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity (Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities of various words with the word: cat\n",
      "\n",
      ">  dog 0.9218005537986755\n",
      "\n",
      ">  bike 0.44144073128700256\n",
      "\n",
      ">  kitten 0.6386305689811707\n",
      "\n",
      ">  puppy 0.7625599503517151\n",
      "\n",
      ">  kite 0.4891083538532257\n",
      "\n",
      ">  computer 0.3525111973285675\n",
      "\n",
      ">  neuron 0.21150361001491547\n"
     ]
    }
   ],
   "source": [
    "word = 'cat'\n",
    "other = ['dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
    "print(\"Similarities of various words with the word: cat\")\n",
    "for w in other:\n",
    "    dist = torch.cosine_similarity(glove[word].unsqueeze(0) , glove[w].unsqueeze(0))\n",
    "    print(\"\\n> \", w, float(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities of various words with:  cat\n",
      "\n",
      ">  dog 1.8846031\n",
      "\n",
      ">  rabbit 2.4572797\n",
      "\n",
      ">  monkey 2.8102052\n",
      "\n",
      ">  cats 2.8972247\n",
      "\n",
      ">  rat 2.9455352\n",
      "\n",
      ">  beast 2.9878407\n",
      "\n",
      ">  monster 3.0022194\n",
      "\n",
      ">  pet 3.0396757\n",
      "\n",
      ">  snake 3.0617998\n",
      "\n",
      ">  puppy 3.0644655\n"
     ]
    }
   ],
   "source": [
    "def most_similar(word, word_vec, n=5):\n",
    "    '''Function that returns the n most similar words to a given input word and its word vector'''\n",
    "    print(\"Similarities of various words with: \", word)\n",
    "    dists = torch.norm(glove.vectors - word_vec, dim=1)     # compute distances to all words\n",
    "    \n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
    "    \n",
    "    for idx, difference in lst[1:n+1]:    # take the top n\n",
    "        print(\"\\n> \",glove.itos[idx], difference)\n",
    "\n",
    "most_similar(\"cat\", glove[\"cat\"], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some vector operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities of various words with:  (happy + sad) /2\n",
      "\n",
      ">  happy 1.9199749\n",
      "\n",
      ">  feels 2.3604643\n",
      "\n",
      ">  sorry 2.4984782\n",
      "\n",
      ">  hardly 2.52593\n",
      "\n",
      ">  imagine 2.5652788\n"
     ]
    }
   ],
   "source": [
    "most_similar(\"(happy + sad) /2\", (glove['happy'] + glove['sad']) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities of various words with:  (king - man + woman)\n",
      "\n",
      ">  queen 2.8391209\n",
      "\n",
      ">  prince 3.6610038\n",
      "\n",
      ">  elizabeth 3.7152522\n",
      "\n",
      ">  daughter 3.8317878\n",
      "\n",
      ">  widow 3.8493774\n"
     ]
    }
   ],
   "source": [
    "most_similar(\"(king - man + woman)\", (glove['king'] - glove['man'] + glove['woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities of various words with:  (queen - woman + man)\n",
      "\n",
      ">  king 2.8391209\n",
      "\n",
      ">  prince 3.2508988\n",
      "\n",
      ">  crown 3.4485192\n",
      "\n",
      ">  knight 3.5587437\n",
      "\n",
      ">  coronation 3.6198905\n"
     ]
    }
   ],
   "source": [
    "most_similar(\"(queen - woman + man)\", (glove['queen'] - glove['woman'] + glove['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities of various words with:  (programmer + good - bad)\n",
      "\n",
      ">  versatile 4.381561\n",
      "\n",
      ">  creative 4.5690007\n",
      "\n",
      ">  entrepreneur 4.6343737\n",
      "\n",
      ">  enables 4.7177725\n",
      "\n",
      ">  intelligent 4.7349973\n"
     ]
    }
   ],
   "source": [
    "most_similar(\"(programmer + good - bad)\", (glove['programmer'] - glove['bad'] + glove['good']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities of various words with:  (programmer - good + bad)\n",
      "\n",
      ">  hacker 3.8383653\n",
      "\n",
      ">  glitch 4.003873\n",
      "\n",
      ">  originator 4.041952\n",
      "\n",
      ">  hack 4.047719\n",
      "\n",
      ">  serial 4.2250676\n"
     ]
    }
   ],
   "source": [
    "most_similar(\"(programmer - good + bad)\", (glove['programmer'] - glove['good'] + glove['bad']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GloVe-Word-Analogy-Task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
