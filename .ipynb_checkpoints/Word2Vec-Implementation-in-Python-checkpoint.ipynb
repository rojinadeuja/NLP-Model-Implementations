{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x. This implementation provides better numerical stability.'''\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def softmax_1(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x'''\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / np.sum(e_x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(self, word):\n",
    "    '''Function to covert a word to one-hot-encoded value'''\n",
    "    word_vec = [0 for i in range(0, self.v_count)]\n",
    "    word_index = self.word_index[word]\n",
    "    word_vec[word_index] = 1\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    '''Implementation of Skip-Gram Word2Vec model'''\n",
    "    def __init__(self):\n",
    "        self.N = 10\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        \n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.001\n",
    "        \n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "    \n",
    "    def initialize(self, V, data):\n",
    "        '''Function to initialze the neural network'''\n",
    "        self.V = V\n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "            \n",
    "    def feed_forward(self, X):\n",
    "        '''Function for feed-forward step'''\n",
    "        self.h = np.dot(self.W.T, X).reshape(self.N, 1) \n",
    "        self.u = np.dot(self.W1.T, self.h)\n",
    "        self.y = softmax(self.u)   \n",
    "        return self.y\n",
    "    \n",
    "    def backpropagate(self, x, t):\n",
    "        '''Function for back propagation using Stochastic Gradient Descent step'''\n",
    "        e = self.y - np.asarray(t).reshape(self.V, 1) #e.shape is V X 1\n",
    "        # Calculate partial derivative of loss function wrt W1\n",
    "        dEdW1 = np.dot(self.h, e.T)\n",
    "        \n",
    "        X = np.array(x).reshape(self.V, 1)\n",
    "        # Calculate partial derivative of loss function wrt W\n",
    "        dEdW = np.dot(X, np.dot(self.W1, e).T)\n",
    "        \n",
    "        # Update the weights\n",
    "        self.W1 = self.W1 - self.alpha * dEdW1\n",
    "        self.W = self.W - self.alpha * dEdW\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        '''Function to train the Word2Vec model'''\n",
    "        # Loop through each epoch\n",
    "        for x in range(1,epochs):\n",
    "            \n",
    "            # Initialize Loss\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Loop through each training sample\n",
    "            for j in range(len(self.X_train)):\n",
    "                # Forward Pass\n",
    "                self.feed_forward(self.X_train[j])\n",
    "                \n",
    "                # Backpropagation\n",
    "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                        \n",
    "                # Calculate Loss        \n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u)))\n",
    "                \n",
    "            print(\"Epoch: \", x, \" Loss: \", self.loss)\n",
    "            self.alpha *= 1/((1+self.alpha*x))\n",
    "            \n",
    "    def predict(self, word, number_of_predictions):       \n",
    "        # Check if word is contained in the dictionary\n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X)\n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i\n",
    "                \n",
    "            # Sort top context words in the output    \n",
    "            sorted_output = [] \n",
    "            for k in sorted(output, reverse=True): \n",
    "                sorted_output.append(self.words[output[k]]) \n",
    "                if(len(sorted_output)>=number_of_predictions): \n",
    "                    break\n",
    "            return sorted_output \n",
    "        else: \n",
    "            print(\"Error: Word not found in dicitonary\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    processed = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    # Split text corpus into sentences\n",
    "    sentences = corpus.split(\".\")\n",
    "    # Loop through each sentence\n",
    "    for i in range(len(sentences)):\n",
    "        # Remove leading and trailing characters\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        # Split sentence into list of words\n",
    "        sentence = sentences[i].split()\n",
    "        # Remove punctuations\n",
    "        x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "        # Convert to lowe\n",
    "        x = [word.lower() for word in x] \n",
    "        processed.append(x) \n",
    "    return processed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sentences, w2v):\n",
    "    data = {}\n",
    "    # Loop throuch each sentence\n",
    "    for sentence in sentences:\n",
    "        # Loop through each word\n",
    "        for word in sentence: \n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) # Size of Vocabulary\n",
    "    data = sorted(list(data.keys()))\n",
    "    \n",
    "    vocab = {}\n",
    "    # Store words into vocabulary\n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "       \n",
    "    # Loop through each sentence \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)] \n",
    "              \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context) \n",
    "    w2v.initialize(V,data) \n",
    "    return w2v.X_train,w2v.y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  61.802825656965155\n",
      "Epoch:  2  Loss:  61.70624913849453\n",
      "Epoch:  3  Loss:  61.6100843314518\n",
      "Epoch:  4  Loss:  61.51442327288594\n",
      "Epoch:  5  Loss:  61.41935566267462\n",
      "Epoch:  6  Loss:  61.32496836682249\n",
      "Epoch:  7  Loss:  61.23134495739262\n",
      "Epoch:  8  Loss:  61.13856529489851\n",
      "Epoch:  9  Loss:  61.04670515794645\n",
      "Epoch:  10  Loss:  60.955835923791305\n",
      "Epoch:  11  Loss:  60.86602430229532\n",
      "Epoch:  12  Loss:  60.77733212460163\n",
      "Epoch:  13  Loss:  60.689816186693214\n",
      "Epoch:  14  Loss:  60.603528146935865\n",
      "Epoch:  15  Loss:  60.518514475734484\n",
      "Epoch:  16  Loss:  60.434816454583995\n",
      "Epoch:  17  Loss:  60.35247022108936\n",
      "Epoch:  18  Loss:  60.271506855968795\n",
      "Epoch:  19  Loss:  60.19195250764474\n",
      "Epoch:  20  Loss:  60.113828549763035\n",
      "Epoch:  21  Loss:  60.03715176685352\n",
      "Epoch:  22  Loss:  59.96193456334297\n",
      "Epoch:  23  Loss:  59.88818519123573\n",
      "Epoch:  24  Loss:  59.815907991975784\n",
      "Epoch:  25  Loss:  59.74510364827202\n",
      "Epoch:  26  Loss:  59.67576944199605\n",
      "Epoch:  27  Loss:  59.60789951462478\n",
      "Epoch:  28  Loss:  59.54148512708881\n",
      "Epoch:  29  Loss:  59.476514916284486\n",
      "Epoch:  30  Loss:  59.41297514590312\n",
      "Epoch:  31  Loss:  59.35084994961566\n",
      "Epoch:  32  Loss:  59.29012156501547\n",
      "Epoch:  33  Loss:  59.23077055706298\n",
      "Epoch:  34  Loss:  59.1727760300881\n",
      "Epoch:  35  Loss:  59.11611582768723\n",
      "Epoch:  36  Loss:  59.06076672010155\n",
      "Epoch:  37  Loss:  59.00670457888072\n",
      "Epoch:  38  Loss:  58.95390453882255\n",
      "Epoch:  39  Loss:  58.90234114733655\n",
      "Epoch:  40  Loss:  58.851988501508835\n",
      "Epoch:  41  Loss:  58.80282037325059\n",
      "Epoch:  42  Loss:  58.75481032299377\n",
      "Epoch:  43  Loss:  58.707931802459264\n",
      "Epoch:  44  Loss:  58.662158247066955\n",
      "Epoch:  45  Loss:  58.61746315858537\n",
      "Epoch:  46  Loss:  58.57382017863398\n",
      "Epoch:  47  Loss:  58.53120315365632\n",
      "Epoch:  48  Loss:  58.489586191976876\n",
      "Epoch:  49  Loss:  58.44894371354373\n",
      "Epoch:  50  Loss:  58.409250492941055\n",
      "Epoch:  51  Loss:  58.370481696233306\n",
      "Epoch:  52  Loss:  58.332612912178774\n",
      "Epoch:  53  Loss:  58.29562017832129\n",
      "Epoch:  54  Loss:  58.25948000244111\n",
      "Epoch:  55  Loss:  58.22416937981569\n",
      "Epoch:  56  Loss:  58.189665806711226\n",
      "Epoch:  57  Loss:  58.15594729049607\n",
      "Epoch:  58  Loss:  58.12299235673874\n",
      "Epoch:  59  Loss:  58.09078005362388\n",
      "Epoch:  60  Loss:  58.059289953993996\n",
      "Epoch:  61  Loss:  58.02850215529762\n",
      "Epoch:  62  Loss:  57.99839727770127\n",
      "Epoch:  63  Loss:  57.96895646059898\n",
      "Epoch:  64  Loss:  57.94016135773163\n",
      "Epoch:  65  Loss:  57.91199413110865\n",
      "Epoch:  66  Loss:  57.88443744390573\n",
      "Epoch:  67  Loss:  57.857474452494735\n",
      "Epoch:  68  Loss:  57.83108879774647\n",
      "Epoch:  69  Loss:  57.80526459573213\n",
      "Epoch:  70  Loss:  57.77998642793599\n",
      "Epoch:  71  Loss:  57.75523933107891\n",
      "Epoch:  72  Loss:  57.73100878664242\n",
      "Epoch:  73  Loss:  57.70728071017141\n",
      "Epoch:  74  Loss:  57.68404144042544\n",
      "Epoch:  75  Loss:  57.661277728439515\n",
      "Epoch:  76  Loss:  57.638976726547945\n",
      "Epoch:  77  Loss:  57.617125977418056\n",
      "Epoch:  78  Loss:  57.59571340313436\n",
      "Epoch:  79  Loss:  57.57472729436799\n",
      "Epoch:  80  Loss:  57.554156299661805\n",
      "Epoch:  81  Loss:  57.533989414856606\n",
      "Epoch:  82  Loss:  57.514215972679935\n",
      "Epoch:  83  Loss:  57.49482563251633\n",
      "Epoch:  84  Loss:  57.47580837037301\n",
      "Epoch:  85  Loss:  57.4571544690541\n",
      "Epoch:  86  Loss:  57.438854508552495\n",
      "Epoch:  87  Loss:  57.42089935666738\n",
      "Epoch:  88  Loss:  57.403280159852635\n",
      "Epoch:  89  Loss:  57.385988334300485\n",
      "Epoch:  90  Loss:  57.36901555726244\n",
      "Epoch:  91  Loss:  57.35235375860905\n",
      "Epoch:  92  Loss:  57.33599511262819\n",
      "Epoch:  93  Loss:  57.31993203006141\n",
      "Epoch:  94  Loss:  57.304157150376106\n",
      "Epoch:  95  Loss:  57.28866333427145\n",
      "Epoch:  96  Loss:  57.27344365641487\n",
      "Epoch:  97  Loss:  57.25849139840526\n",
      "Epoch:  98  Loss:  57.24380004195936\n",
      "Epoch:  99  Loss:  57.22936326231675\n",
      "Epoch:  100  Loss:  57.21517492185868\n",
      "Epoch:  101  Loss:  57.20122906393593\n",
      "Epoch:  102  Loss:  57.18751990690105\n",
      "Epoch:  103  Loss:  57.17404183833903\n",
      "Epoch:  104  Loss:  57.16078940949195\n",
      "Epoch:  105  Loss:  57.14775732987172\n",
      "Epoch:  106  Loss:  57.13494046205595\n",
      "Epoch:  107  Loss:  57.12233381666107\n",
      "Epoch:  108  Loss:  57.10993254748828\n",
      "Epoch:  109  Loss:  57.09773194683597\n",
      "Epoch:  110  Loss:  57.08572744097425\n",
      "Epoch:  111  Loss:  57.07391458577592\n",
      "Epoch:  112  Loss:  57.06228906249916\n",
      "Epoch:  113  Loss:  57.05084667371669\n",
      "Epoch:  114  Loss:  57.03958333938656\n",
      "Epoch:  115  Loss:  57.028495093060116\n",
      "Epoch:  116  Loss:  57.01757807822195\n",
      "Epoch:  117  Loss:  57.006828544757795\n",
      "Epoch:  118  Loss:  56.99624284554568\n",
      "Epoch:  119  Loss:  56.98581743316606\n",
      "Epoch:  120  Loss:  56.97554885672677\n",
      "Epoch:  121  Loss:  56.96543375879894\n",
      "Epoch:  122  Loss:  56.95546887245943\n",
      "Epoch:  123  Loss:  56.94565101843664\n",
      "Epoch:  124  Loss:  56.935977102355366\n",
      "Epoch:  125  Loss:  56.926444112077704\n",
      "Epoch:  126  Loss:  56.917049115136\n",
      "Epoch:  127  Loss:  56.907789256255\n",
      "Epoch:  128  Loss:  56.89866175495956\n",
      "Epoch:  129  Loss:  56.88966390326519\n",
      "Epoch:  130  Loss:  56.88079306344803\n",
      "Epoch:  131  Loss:  56.87204666589164\n",
      "Epoch:  132  Loss:  56.863422207007716\n",
      "Epoch:  133  Loss:  56.85491724722806\n",
      "Epoch:  134  Loss:  56.84652940906505\n",
      "Epoch:  135  Loss:  56.83825637523833\n",
      "Epoch:  136  Loss:  56.83009588686526\n",
      "Epoch:  137  Loss:  56.82204574171255\n",
      "Epoch:  138  Loss:  56.814103792507126\n",
      "Epoch:  139  Loss:  56.806267945304064\n",
      "Epoch:  140  Loss:  56.798536157909226\n",
      "Epoch:  141  Loss:  56.790906438354924\n",
      "Epoch:  142  Loss:  56.78337684342651\n",
      "Epoch:  143  Loss:  56.77594547723809\n",
      "Epoch:  144  Loss:  56.76861048985556\n",
      "Epoch:  145  Loss:  56.76137007596521\n",
      "Epoch:  146  Loss:  56.75422247358639\n",
      "Epoch:  147  Loss:  56.74716596282646\n",
      "Epoch:  148  Loss:  56.74019886467666\n",
      "Epoch:  149  Loss:  56.73331953984726\n",
      "Epoch:  150  Loss:  56.726526387640774\n",
      "Epoch:  151  Loss:  56.71981784486169\n",
      "Epoch:  152  Loss:  56.71319238476146\n",
      "Epoch:  153  Loss:  56.70664851601769\n",
      "Epoch:  154  Loss:  56.70018478174591\n",
      "Epoch:  155  Loss:  56.69379975854322\n",
      "Epoch:  156  Loss:  56.687492055562196\n",
      "Epoch:  157  Loss:  56.681260313614516\n",
      "Epoch:  158  Loss:  56.67510320430288\n",
      "Epoch:  159  Loss:  56.66901942918024\n",
      "Epoch:  160  Loss:  56.66300771893561\n",
      "Epoch:  161  Loss:  56.65706683260538\n",
      "Epoch:  162  Loss:  56.6511955568092\n",
      "Epoch:  163  Loss:  56.64539270500965\n",
      "Epoch:  164  Loss:  56.63965711679494\n",
      "Epoch:  165  Loss:  56.633987657183646\n",
      "Epoch:  166  Loss:  56.6283832159509\n",
      "Epoch:  167  Loss:  56.62284270697525\n",
      "Epoch:  168  Loss:  56.61736506760542\n",
      "Epoch:  169  Loss:  56.611949258046366\n",
      "Epoch:  170  Loss:  56.60659426076395\n",
      "Epoch:  171  Loss:  56.601299079907484\n",
      "Epoch:  172  Loss:  56.59606274074982\n",
      "Epoch:  173  Loss:  56.59088428914412\n",
      "Epoch:  174  Loss:  56.585762790996775\n",
      "Epoch:  175  Loss:  56.580697331756134\n",
      "Epoch:  176  Loss:  56.57568701591629\n",
      "Epoch:  177  Loss:  56.570730966535535\n",
      "Epoch:  178  Loss:  56.56582832476901\n",
      "Epoch:  179  Loss:  56.560978249414944\n",
      "Epoch:  180  Loss:  56.55617991647419\n",
      "Epoch:  181  Loss:  56.55143251872261\n",
      "Epoch:  182  Loss:  56.546735265295844\n",
      "Epoch:  183  Loss:  56.542087381285775\n",
      "Epoch:  184  Loss:  56.53748810734911\n",
      "Epoch:  185  Loss:  56.53293669932667\n",
      "Epoch:  186  Loss:  56.52843242787376\n",
      "Epoch:  187  Loss:  56.52397457810111\n",
      "Epoch:  188  Loss:  56.51956244922576\n",
      "Epoch:  189  Loss:  56.515195354231956\n",
      "Epoch:  190  Loss:  56.51087261954146\n",
      "Epoch:  191  Loss:  56.50659358469323\n",
      "Epoch:  192  Loss:  56.50235760203181\n",
      "Epoch:  193  Loss:  56.498164036404546\n",
      "Epoch:  194  Loss:  56.49401226486712\n",
      "Epoch:  195  Loss:  56.489901676397174\n",
      "Epoch:  196  Loss:  56.48583167161576\n",
      "Epoch:  197  Loss:  56.48180166251655\n",
      "Epoch:  198  Loss:  56.477811072202165\n",
      "Epoch:  199  Loss:  56.473859334628045\n",
      "Epoch:  200  Loss:  56.46994589435273\n",
      "Epoch:  201  Loss:  56.466070206295356\n",
      "Epoch:  202  Loss:  56.46223173549938\n",
      "Epoch:  203  Loss:  56.458429956902634\n",
      "Epoch:  204  Loss:  56.454664355113565\n",
      "Epoch:  205  Loss:  56.45093442419341\n",
      "Epoch:  206  Loss:  56.44723966744415\n",
      "Epoch:  207  Loss:  56.443579597201875\n",
      "Epoch:  208  Loss:  56.43995373463578\n",
      "Epoch:  209  Loss:  56.436361609552385\n",
      "Epoch:  210  Loss:  56.43280276020458\n",
      "Epoch:  211  Loss:  56.42927673310609\n",
      "Epoch:  212  Loss:  56.425783082850295\n",
      "Epoch:  213  Loss:  56.42232137193395\n",
      "Epoch:  214  Loss:  56.41889117058537\n",
      "Epoch:  215  Loss:  56.41549205659702\n",
      "Epoch:  216  Loss:  56.41212361516228\n",
      "Epoch:  217  Loss:  56.40878543871651\n",
      "Epoch:  218  Loss:  56.40547712678197\n",
      "Epoch:  219  Loss:  56.40219828581667\n",
      "Epoch:  220  Loss:  56.39894852906725\n",
      "Epoch:  221  Loss:  56.395727476425066\n",
      "Epoch:  222  Loss:  56.39253475428633\n",
      "Epoch:  223  Loss:  56.38936999541548\n",
      "Epoch:  224  Loss:  56.38623283881185\n",
      "Epoch:  225  Loss:  56.38312292957992\n",
      "Epoch:  226  Loss:  56.380039918802495\n",
      "Epoch:  227  Loss:  56.376983463417\n",
      "Epoch:  228  Loss:  56.37395322609486\n",
      "Epoch:  229  Loss:  56.37094887512393\n",
      "Epoch:  230  Loss:  56.367970084293546\n",
      "Epoch:  231  Loss:  56.36501653278232\n",
      "Epoch:  232  Loss:  56.36208790504899\n",
      "Epoch:  233  Loss:  56.35918389072541\n",
      "Epoch:  234  Loss:  56.356304184512446\n",
      "Epoch:  235  Loss:  56.35344848607817\n",
      "Epoch:  236  Loss:  56.35061649995855\n",
      "Epoch:  237  Loss:  56.347807935460395\n",
      "Epoch:  238  Loss:  56.345022506566664\n",
      "Epoch:  239  Loss:  56.34225993184396\n",
      "Epoch:  240  Loss:  56.33951993435215\n",
      "Epoch:  241  Loss:  56.336802241556164\n",
      "Epoch:  242  Loss:  56.33410658523976\n",
      "Epoch:  243  Loss:  56.33143270142138\n",
      "Epoch:  244  Loss:  56.328780330271734\n",
      "Epoch:  245  Loss:  56.32614921603365\n",
      "Epoch:  246  Loss:  56.32353910694329\n",
      "Epoch:  247  Loss:  56.320949755153514\n",
      "Epoch:  248  Loss:  56.318380916658896\n",
      "Epoch:  249  Loss:  56.31583235122235\n",
      "Epoch:  250  Loss:  56.31330382230347\n",
      "Epoch:  251  Loss:  56.310795096988464\n",
      "Epoch:  252  Loss:  56.308305945921745\n",
      "Epoch:  253  Loss:  56.305836143238956\n",
      "Epoch:  254  Loss:  56.30338546650134\n",
      "Epoch:  255  Loss:  56.300953696632014\n",
      "Epoch:  256  Loss:  56.29854061785309\n",
      "Epoch:  257  Loss:  56.29614601762473\n",
      "Epoch:  258  Loss:  56.29376968658495\n",
      "Epoch:  259  Loss:  56.291411418491286\n",
      "Epoch:  260  Loss:  56.28907101016348\n",
      "Epoch:  261  Loss:  56.286748261427334\n",
      "Epoch:  262  Loss:  56.28444297505993\n",
      "Epoch:  263  Loss:  56.28215495673599\n",
      "Epoch:  264  Loss:  56.27988401497533\n",
      "Epoch:  265  Loss:  56.27762996109162\n",
      "Epoch:  266  Loss:  56.27539260914193\n",
      "Epoch:  267  Loss:  56.27317177587759\n",
      "Epoch:  268  Loss:  56.27096728069622\n",
      "Epoch:  269  Loss:  56.2687789455942\n",
      "Epoch:  270  Loss:  56.26660659512092\n",
      "Epoch:  271  Loss:  56.26445005633338\n",
      "Epoch:  272  Loss:  56.26230915875209\n",
      "Epoch:  273  Loss:  56.260183734317565\n",
      "Epoch:  274  Loss:  56.258073617348174\n",
      "Epoch:  275  Loss:  56.25597864449842\n",
      "Epoch:  276  Loss:  56.253898654718455\n",
      "Epoch:  277  Loss:  56.25183348921405\n",
      "Epoch:  278  Loss:  56.24978299140775\n",
      "Epoch:  279  Loss:  56.247747006900596\n",
      "Epoch:  280  Loss:  56.24572538343477\n",
      "Epoch:  281  Loss:  56.243717970856764\n",
      "Epoch:  282  Loss:  56.24172462108168\n",
      "Epoch:  283  Loss:  56.239745188057874\n",
      "Epoch:  284  Loss:  56.23777952773252\n",
      "Epoch:  285  Loss:  56.23582749801781\n",
      "Epoch:  286  Loss:  56.233888958757944\n",
      "Epoch:  287  Loss:  56.2319637716965\n",
      "Epoch:  288  Loss:  56.23005180044474\n",
      "Epoch:  289  Loss:  56.2281529104504\n",
      "Epoch:  290  Loss:  56.22626696896723\n",
      "Epoch:  291  Loss:  56.22439384502482\n",
      "Epoch:  292  Loss:  56.22253340939949\n",
      "Epoch:  293  Loss:  56.22068553458535\n",
      "Epoch:  294  Loss:  56.218850094766076\n",
      "Epoch:  295  Loss:  56.2170269657873\n",
      "Epoch:  296  Loss:  56.21521602512944\n",
      "Epoch:  297  Loss:  56.213417151881096\n",
      "Epoch:  298  Loss:  56.211630226712906\n",
      "Epoch:  299  Loss:  56.20985513185205\n",
      "Epoch:  300  Loss:  56.208091751057\n",
      "Epoch:  301  Loss:  56.20633996959306\n",
      "Epoch:  302  Loss:  56.20459967420811\n",
      "Epoch:  303  Loss:  56.202870753108925\n",
      "Epoch:  304  Loss:  56.20115309593799\n",
      "Epoch:  305  Loss:  56.199446593750565\n",
      "Epoch:  306  Loss:  56.197751138992444\n",
      "Epoch:  307  Loss:  56.19606662547801\n",
      "Epoch:  308  Loss:  56.19439294836853\n",
      "Epoch:  309  Loss:  56.19273000415114\n",
      "Epoch:  310  Loss:  56.19107769061818\n",
      "Epoch:  311  Loss:  56.18943590684658\n",
      "Epoch:  312  Loss:  56.187804553178175\n",
      "Epoch:  313  Loss:  56.18618353119986\n",
      "Epoch:  314  Loss:  56.18457274372443\n",
      "Epoch:  315  Loss:  56.18297209477169\n",
      "Epoch:  316  Loss:  56.181381489549935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  317  Loss:  56.179800834437586\n",
      "Epoch:  318  Loss:  56.17823003696549\n",
      "Epoch:  319  Loss:  56.1766690057993\n",
      "Epoch:  320  Loss:  56.175117650722115\n",
      "Epoch:  321  Loss:  56.1735758826178\n",
      "Epoch:  322  Loss:  56.172043613454065\n",
      "Epoch:  323  Loss:  56.170520756266455\n",
      "Epoch:  324  Loss:  56.169007225141975\n",
      "Epoch:  325  Loss:  56.167502935203686\n",
      "Epoch:  326  Loss:  56.16600780259497\n",
      "Epoch:  327  Loss:  56.164521744464494\n",
      "Epoch:  328  Loss:  56.1630446789512\n",
      "Epoch:  329  Loss:  56.161576525169686\n",
      "Epoch:  330  Loss:  56.160117203195846\n",
      "Epoch:  331  Loss:  56.158666634052615\n",
      "Epoch:  332  Loss:  56.157224739696126\n",
      "Epoch:  333  Loss:  56.155791443002116\n",
      "Epoch:  334  Loss:  56.154366667752335\n",
      "Epoch:  335  Loss:  56.152950338621565\n",
      "Epoch:  336  Loss:  56.15154238116452\n",
      "Epoch:  337  Loss:  56.15014272180307\n",
      "Epoch:  338  Loss:  56.14875128781391\n",
      "Epoch:  339  Loss:  56.14736800731609\n",
      "Epoch:  340  Loss:  56.14599280925903\n",
      "Epoch:  341  Loss:  56.1446256234105\n",
      "Epoch:  342  Loss:  56.14326638034511\n",
      "Epoch:  343  Loss:  56.141915011432644\n",
      "Epoch:  344  Loss:  56.140571448827\n",
      "Epoch:  345  Loss:  56.13923562545484\n",
      "Epoch:  346  Loss:  56.137907475004795\n",
      "Epoch:  347  Loss:  56.13658693191682\n",
      "Epoch:  348  Loss:  56.1352739313715\n",
      "Epoch:  349  Loss:  56.13396840927971\n",
      "Epoch:  350  Loss:  56.132670302272466\n",
      "Epoch:  351  Loss:  56.131379547690926\n",
      "Epoch:  352  Loss:  56.13009608357633\n",
      "Epoch:  353  Loss:  56.128819848660626\n",
      "Epoch:  354  Loss:  56.127550782356565\n",
      "Epoch:  355  Loss:  56.12628882474857\n",
      "Epoch:  356  Loss:  56.12503391658342\n",
      "Epoch:  357  Loss:  56.12378599926118\n",
      "Epoch:  358  Loss:  56.12254501482627\n",
      "Epoch:  359  Loss:  56.12131090595867\n",
      "Epoch:  360  Loss:  56.12008361596525\n",
      "Epoch:  361  Loss:  56.118863088771334\n",
      "Epoch:  362  Loss:  56.11764926891233\n",
      "Epoch:  363  Loss:  56.11644210152538\n",
      "Epoch:  364  Loss:  56.11524153234139\n",
      "Epoch:  365  Loss:  56.114047507677085\n",
      "Epoch:  366  Loss:  56.112859974426996\n",
      "Epoch:  367  Loss:  56.111678880055884\n",
      "Epoch:  368  Loss:  56.110504172591156\n",
      "Epoch:  369  Loss:  56.10933580061528\n",
      "Epoch:  370  Loss:  56.1081737132585\n",
      "Epoch:  371  Loss:  56.10701786019163\n",
      "Epoch:  372  Loss:  56.10586819161886\n",
      "Epoch:  373  Loss:  56.1047246582708\n",
      "Epoch:  374  Loss:  56.10358721139748\n",
      "Epoch:  375  Loss:  56.102455802761675\n",
      "Epoch:  376  Loss:  56.101330384632185\n",
      "Epoch:  377  Loss:  56.10021090977714\n",
      "Epoch:  378  Loss:  56.09909733145766\n",
      "Epoch:  379  Loss:  56.097989603421425\n",
      "Epoch:  380  Loss:  56.09688767989635\n",
      "Epoch:  381  Loss:  56.09579151558437\n",
      "Epoch:  382  Loss:  56.09470106565548\n",
      "Epoch:  383  Loss:  56.09361628574163\n",
      "Epoch:  384  Loss:  56.09253713193084\n",
      "Epoch:  385  Loss:  56.091463560761355\n",
      "Epoch:  386  Loss:  56.09039552921596\n",
      "Epoch:  387  Loss:  56.08933299471635\n",
      "Epoch:  388  Loss:  56.08827591511745\n",
      "Epoch:  389  Loss:  56.08722424870217\n",
      "Epoch:  390  Loss:  56.086177954175724\n",
      "Epoch:  391  Loss:  56.08513699066056\n",
      "Epoch:  392  Loss:  56.08410131769105\n",
      "Epoch:  393  Loss:  56.083070895208245\n",
      "Epoch:  394  Loss:  56.08204568355495\n",
      "Epoch:  395  Loss:  56.081025643470596\n",
      "Epoch:  396  Loss:  56.080010736086464\n",
      "Epoch:  397  Loss:  56.07900092292066\n",
      "Epoch:  398  Loss:  56.077996165873394\n",
      "Epoch:  399  Loss:  56.07699642722237\n",
      "Epoch:  400  Loss:  56.07600166961799\n",
      "Epoch:  401  Loss:  56.07501185607884\n",
      "Epoch:  402  Loss:  56.074026949987285\n",
      "Epoch:  403  Loss:  56.073046915084845\n",
      "Epoch:  404  Loss:  56.07207171546795\n",
      "Epoch:  405  Loss:  56.071101315583505\n",
      "Epoch:  406  Loss:  56.070135680224894\n",
      "Epoch:  407  Loss:  56.06917477452742\n",
      "Epoch:  408  Loss:  56.068218563964514\n",
      "Epoch:  409  Loss:  56.067267014343436\n",
      "Epoch:  410  Loss:  56.0663200918014\n",
      "Epoch:  411  Loss:  56.06537776280157\n",
      "Epoch:  412  Loss:  56.064439994129145\n",
      "Epoch:  413  Loss:  56.063506752887534\n",
      "Epoch:  414  Loss:  56.062578006494626\n",
      "Epoch:  415  Loss:  56.06165372267891\n",
      "Epoch:  416  Loss:  56.06073386947602\n",
      "Epoch:  417  Loss:  56.059818415224804\n",
      "Epoch:  418  Loss:  56.05890732856408\n",
      "Epoch:  419  Loss:  56.05800057842884\n",
      "Epoch:  420  Loss:  56.057098134046925\n",
      "Epoch:  421  Loss:  56.056199964935594\n",
      "Epoch:  422  Loss:  56.05530604089802\n",
      "Epoch:  423  Loss:  56.05441633202003\n",
      "Epoch:  424  Loss:  56.053530808666906\n",
      "Epoch:  425  Loss:  56.05264944148005\n",
      "Epoch:  426  Loss:  56.05177220137368\n",
      "Epoch:  427  Loss:  56.050899059531964\n",
      "Epoch:  428  Loss:  56.05002998740562\n",
      "Epoch:  429  Loss:  56.04916495670902\n",
      "Epoch:  430  Loss:  56.04830393941713\n",
      "Epoch:  431  Loss:  56.04744690776248\n",
      "Epoch:  432  Loss:  56.04659383423228\n",
      "Epoch:  433  Loss:  56.04574469156546\n",
      "Epoch:  434  Loss:  56.04489945274992\n",
      "Epoch:  435  Loss:  56.04405809101953\n",
      "Epoch:  436  Loss:  56.043220579851514\n",
      "Epoch:  437  Loss:  56.042386892963556\n",
      "Epoch:  438  Loss:  56.041557004311265\n",
      "Epoch:  439  Loss:  56.04073088808531\n",
      "Epoch:  440  Loss:  56.0399085187089\n",
      "Epoch:  441  Loss:  56.03908987083517\n",
      "Epoch:  442  Loss:  56.038274919344516\n",
      "Epoch:  443  Loss:  56.03746363934225\n",
      "Epoch:  444  Loss:  56.03665600615589\n",
      "Epoch:  445  Loss:  56.03585199533283\n",
      "Epoch:  446  Loss:  56.03505158263787\n",
      "Epoch:  447  Loss:  56.03425474405078\n",
      "Epoch:  448  Loss:  56.03346145576396\n",
      "Epoch:  449  Loss:  56.03267169418013\n",
      "Epoch:  450  Loss:  56.031885435909956\n",
      "Epoch:  451  Loss:  56.03110265776983\n",
      "Epoch:  452  Loss:  56.03032333677953\n",
      "Epoch:  453  Loss:  56.02954745016013\n",
      "Epoch:  454  Loss:  56.028774975331714\n",
      "Epoch:  455  Loss:  56.028005889911256\n",
      "Epoch:  456  Loss:  56.02724017171047\n",
      "Epoch:  457  Loss:  56.02647779873369\n",
      "Epoch:  458  Loss:  56.0257187491758\n",
      "Epoch:  459  Loss:  56.02496300142027\n",
      "Epoch:  460  Loss:  56.0242105340369\n",
      "Epoch:  461  Loss:  56.02346132578012\n",
      "Epoch:  462  Loss:  56.02271535558675\n",
      "Epoch:  463  Loss:  56.02197260257425\n",
      "Epoch:  464  Loss:  56.021233046038645\n",
      "Epoch:  465  Loss:  56.020496665452704\n",
      "Epoch:  466  Loss:  56.019763440464075\n",
      "Epoch:  467  Loss:  56.019033350893366\n",
      "Epoch:  468  Loss:  56.01830637673241\n",
      "Epoch:  469  Loss:  56.01758249814233\n",
      "Epoch:  470  Loss:  56.016861695451944\n",
      "Epoch:  471  Loss:  56.0161439491558\n",
      "Epoch:  472  Loss:  56.01542923991252\n",
      "Epoch:  473  Loss:  56.01471754854322\n",
      "Epoch:  474  Loss:  56.01400885602952\n",
      "Epoch:  475  Loss:  56.013303143512154\n",
      "Epoch:  476  Loss:  56.01260039228919\n",
      "Epoch:  477  Loss:  56.01190058381438\n",
      "Epoch:  478  Loss:  56.01120369969556\n",
      "Epoch:  479  Loss:  56.01050972169312\n",
      "Epoch:  480  Loss:  56.009818631718346\n",
      "Epoch:  481  Loss:  56.009130411831954\n",
      "Epoch:  482  Loss:  56.00844504424246\n",
      "Epoch:  483  Loss:  56.00776251130473\n",
      "Epoch:  484  Loss:  56.00708279551845\n",
      "Epoch:  485  Loss:  56.00640587952668\n",
      "Epoch:  486  Loss:  56.005731746114385\n",
      "Epoch:  487  Loss:  56.00506037820694\n",
      "Epoch:  488  Loss:  56.00439175886877\n",
      "Epoch:  489  Loss:  56.00372587130195\n",
      "Epoch:  490  Loss:  56.003062698844744\n",
      "Epoch:  491  Loss:  56.002402224970226\n",
      "Epoch:  492  Loss:  56.00174443328507\n",
      "Epoch:  493  Loss:  56.00108930752803\n",
      "Epoch:  494  Loss:  56.0004368315687\n",
      "Epoch:  495  Loss:  55.9997869894062\n",
      "Epoch:  496  Loss:  55.999139765167875\n",
      "Epoch:  497  Loss:  55.99849514310801\n",
      "Epoch:  498  Loss:  55.997853107606566\n",
      "Epoch:  499  Loss:  55.99721364316798\n",
      "Epoch:  500  Loss:  55.99657673441982\n",
      "Epoch:  501  Loss:  55.99594236611173\n",
      "Epoch:  502  Loss:  55.995310523114036\n",
      "Epoch:  503  Loss:  55.9946811904167\n",
      "Epoch:  504  Loss:  55.99405435312809\n",
      "Epoch:  505  Loss:  55.99342999647384\n",
      "Epoch:  506  Loss:  55.992808105795675\n",
      "Epoch:  507  Loss:  55.99218866655026\n",
      "Epoch:  508  Loss:  55.99157166430812\n",
      "Epoch:  509  Loss:  55.99095708475252\n",
      "Epoch:  510  Loss:  55.99034491367842\n",
      "Epoch:  511  Loss:  55.9897351369912\n",
      "Epoch:  512  Loss:  55.989127740705854\n",
      "Epoch:  513  Loss:  55.98852271094571\n",
      "Epoch:  514  Loss:  55.98792003394157\n",
      "Epoch:  515  Loss:  55.98731969603049\n",
      "Epoch:  516  Loss:  55.986721683654906\n",
      "Epoch:  517  Loss:  55.98612598336154\n",
      "Epoch:  518  Loss:  55.98553258180046\n",
      "Epoch:  519  Loss:  55.98494146572403\n",
      "Epoch:  520  Loss:  55.98435262198597\n",
      "Epoch:  521  Loss:  55.98376603754043\n",
      "Epoch:  522  Loss:  55.98318169944091\n",
      "Epoch:  523  Loss:  55.98259959483949\n",
      "Epoch:  524  Loss:  55.98201971098573\n",
      "Epoch:  525  Loss:  55.98144203522586\n",
      "Epoch:  526  Loss:  55.980866555001846\n",
      "Epoch:  527  Loss:  55.980293257850434\n",
      "Epoch:  528  Loss:  55.97972213140234\n",
      "Epoch:  529  Loss:  55.979153163381326\n",
      "Epoch:  530  Loss:  55.97858634160328\n",
      "Epoch:  531  Loss:  55.97802165397553\n",
      "Epoch:  532  Loss:  55.977459088495706\n",
      "Epoch:  533  Loss:  55.97689863325116\n",
      "Epoch:  534  Loss:  55.97634027641804\n",
      "Epoch:  535  Loss:  55.975784006260405\n",
      "Epoch:  536  Loss:  55.97522981112954\n",
      "Epoch:  537  Loss:  55.97467767946296\n",
      "Epoch:  538  Loss:  55.97412759978385\n",
      "Epoch:  539  Loss:  55.97357956070008\n",
      "Epoch:  540  Loss:  55.97303355090353\n",
      "Epoch:  541  Loss:  55.97248955916928\n",
      "Epoch:  542  Loss:  55.9719475743549\n",
      "Epoch:  543  Loss:  55.97140758539957\n",
      "Epoch:  544  Loss:  55.970869581323505\n",
      "Epoch:  545  Loss:  55.97033355122707\n",
      "Epoch:  546  Loss:  55.96979948429014\n",
      "Epoch:  547  Loss:  55.96926736977134\n",
      "Epoch:  548  Loss:  55.968737197007364\n",
      "Epoch:  549  Loss:  55.96820895541221\n",
      "Epoch:  550  Loss:  55.967682634476574\n",
      "Epoch:  551  Loss:  55.96715822376702\n",
      "Epoch:  552  Loss:  55.96663571292548\n",
      "Epoch:  553  Loss:  55.966115091668385\n",
      "Epoch:  554  Loss:  55.965596349786125\n",
      "Epoch:  555  Loss:  55.9650794771424\n",
      "Epoch:  556  Loss:  55.96456446367341\n",
      "Epoch:  557  Loss:  55.96405129938745\n",
      "Epoch:  558  Loss:  55.963539974363975\n",
      "Epoch:  559  Loss:  55.9630304787533\n",
      "Epoch:  560  Loss:  55.96252280277565\n",
      "Epoch:  561  Loss:  55.96201693672077\n",
      "Epoch:  562  Loss:  55.961512870947196\n",
      "Epoch:  563  Loss:  55.9610105958817\n",
      "Epoch:  564  Loss:  55.960510102018645\n",
      "Epoch:  565  Loss:  55.96001137991943\n",
      "Epoch:  566  Loss:  55.959514420211875\n",
      "Epoch:  567  Loss:  55.95901921358966\n",
      "Epoch:  568  Loss:  55.958525750811745\n",
      "Epoch:  569  Loss:  55.95803402270178\n",
      "Epoch:  570  Loss:  55.95754402014756\n",
      "Epoch:  571  Loss:  55.95705573410046\n",
      "Epoch:  572  Loss:  55.95656915557486\n",
      "Epoch:  573  Loss:  55.9560842756477\n",
      "Epoch:  574  Loss:  55.95560108545778\n",
      "Epoch:  575  Loss:  55.9551195762053\n",
      "Epoch:  576  Loss:  55.954639739151396\n",
      "Epoch:  577  Loss:  55.95416156561754\n",
      "Epoch:  578  Loss:  55.95368504698501\n",
      "Epoch:  579  Loss:  55.953210174694405\n",
      "Epoch:  580  Loss:  55.95273694024515\n",
      "Epoch:  581  Loss:  55.952265335194944\n",
      "Epoch:  582  Loss:  55.951795351159376\n",
      "Epoch:  583  Loss:  55.951326979811284\n",
      "Epoch:  584  Loss:  55.950860212880286\n",
      "Epoch:  585  Loss:  55.95039504215248\n",
      "Epoch:  586  Loss:  55.949931459469695\n",
      "Epoch:  587  Loss:  55.949469456729226\n",
      "Epoch:  588  Loss:  55.94900902588332\n",
      "Epoch:  589  Loss:  55.948550158938595\n",
      "Epoch:  590  Loss:  55.948092847955735\n",
      "Epoch:  591  Loss:  55.94763708504894\n",
      "Epoch:  592  Loss:  55.94718286238556\n",
      "Epoch:  593  Loss:  55.94673017218555\n",
      "Epoch:  594  Loss:  55.94627900672106\n",
      "Epoch:  595  Loss:  55.9458293583161\n",
      "Epoch:  596  Loss:  55.94538121934587\n",
      "Epoch:  597  Loss:  55.94493458223669\n",
      "Epoch:  598  Loss:  55.94448943946518\n",
      "Epoch:  599  Loss:  55.94404578355818\n",
      "Epoch:  600  Loss:  55.94360360709213\n",
      "Epoch:  601  Loss:  55.9431629026926\n",
      "Epoch:  602  Loss:  55.942723663034215\n",
      "Epoch:  603  Loss:  55.94228588083989\n",
      "Epoch:  604  Loss:  55.94184954888063\n",
      "Epoch:  605  Loss:  55.94141465997512\n",
      "Epoch:  606  Loss:  55.94098120698914\n",
      "Epoch:  607  Loss:  55.940549182835525\n",
      "Epoch:  608  Loss:  55.94011858047349\n",
      "Epoch:  609  Loss:  55.93968939290835\n",
      "Epoch:  610  Loss:  55.939261613191185\n",
      "Epoch:  611  Loss:  55.938835234418384\n",
      "Epoch:  612  Loss:  55.93841024973136\n",
      "Epoch:  613  Loss:  55.93798665231613\n",
      "Epoch:  614  Loss:  55.93756443540291\n",
      "Epoch:  615  Loss:  55.93714359226596\n",
      "Epoch:  616  Loss:  55.93672411622295\n",
      "Epoch:  617  Loss:  55.93630600063477\n",
      "Epoch:  618  Loss:  55.935889238905176\n",
      "Epoch:  619  Loss:  55.93547382448047\n",
      "Epoch:  620  Loss:  55.935059750848986\n",
      "Epoch:  621  Loss:  55.93464701154101\n",
      "Epoch:  622  Loss:  55.934235600128254\n",
      "Epoch:  623  Loss:  55.9338255102236\n",
      "Epoch:  624  Loss:  55.93341673548075\n",
      "Epoch:  625  Loss:  55.93300926959391\n",
      "Epoch:  626  Loss:  55.93260310629745\n",
      "Epoch:  627  Loss:  55.932198239365626\n",
      "Epoch:  628  Loss:  55.93179466261227\n",
      "Epoch:  629  Loss:  55.931392369890354\n",
      "Epoch:  630  Loss:  55.93099135509184\n",
      "Epoch:  631  Loss:  55.930591612147325\n",
      "Epoch:  632  Loss:  55.93019313502564\n",
      "Epoch:  633  Loss:  55.9297959177337\n",
      "Epoch:  634  Loss:  55.92939995431606\n",
      "Epoch:  635  Loss:  55.92900523885478\n",
      "Epoch:  636  Loss:  55.92861176546896\n",
      "Epoch:  637  Loss:  55.92821952831458\n",
      "Epoch:  638  Loss:  55.92782852158419\n",
      "Epoch:  639  Loss:  55.927438739506464\n",
      "Epoch:  640  Loss:  55.927050176346235\n",
      "Epoch:  641  Loss:  55.926662826403934\n",
      "Epoch:  642  Loss:  55.926276684015434\n",
      "Epoch:  643  Loss:  55.92589174355172\n",
      "Epoch:  644  Loss:  55.92550799941872\n",
      "Epoch:  645  Loss:  55.92512544605692\n",
      "Epoch:  646  Loss:  55.924744077941185\n",
      "Epoch:  647  Loss:  55.9243638895804\n",
      "Epoch:  648  Loss:  55.923984875517306\n",
      "Epoch:  649  Loss:  55.9236070303282\n",
      "Epoch:  650  Loss:  55.923230348622646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  651  Loss:  55.92285482504328\n",
      "Epoch:  652  Loss:  55.922480454265504\n",
      "Epoch:  653  Loss:  55.92210723099727\n",
      "Epoch:  654  Loss:  55.92173514997886\n",
      "Epoch:  655  Loss:  55.92136420598249\n",
      "Epoch:  656  Loss:  55.920994393812336\n",
      "Epoch:  657  Loss:  55.920625708303994\n",
      "Epoch:  658  Loss:  55.9202581443245\n",
      "Epoch:  659  Loss:  55.91989169677187\n",
      "Epoch:  660  Loss:  55.91952636057505\n",
      "Epoch:  661  Loss:  55.919162130693586\n",
      "Epoch:  662  Loss:  55.91879900211736\n",
      "Epoch:  663  Loss:  55.91843696986652\n",
      "Epoch:  664  Loss:  55.918076028991024\n",
      "Epoch:  665  Loss:  55.91771617457064\n",
      "Epoch:  666  Loss:  55.91735740171462\n",
      "Epoch:  667  Loss:  55.91699970556139\n",
      "Epoch:  668  Loss:  55.91664308127852\n",
      "Epoch:  669  Loss:  55.91628752406236\n",
      "Epoch:  670  Loss:  55.91593302913792\n",
      "Epoch:  671  Loss:  55.91557959175855\n",
      "Epoch:  672  Loss:  55.91522720720589\n",
      "Epoch:  673  Loss:  55.9148758707895\n",
      "Epoch:  674  Loss:  55.914525577846725\n",
      "Epoch:  675  Loss:  55.91417632374251\n",
      "Epoch:  676  Loss:  55.91382810386913\n",
      "Epoch:  677  Loss:  55.913480913646076\n",
      "Epoch:  678  Loss:  55.913134748519795\n",
      "Epoch:  679  Loss:  55.9127896039635\n",
      "Epoch:  680  Loss:  55.912445475476964\n",
      "Epoch:  681  Loss:  55.91210235858639\n",
      "Epoch:  682  Loss:  55.91176024884408\n",
      "Epoch:  683  Loss:  55.9114191418285\n",
      "Epoch:  684  Loss:  55.911079033143714\n",
      "Epoch:  685  Loss:  55.910739918419544\n",
      "Epoch:  686  Loss:  55.910401793311244\n",
      "Epoch:  687  Loss:  55.910064653499234\n",
      "Epoch:  688  Loss:  55.90972849468912\n",
      "Epoch:  689  Loss:  55.909393312611286\n",
      "Epoch:  690  Loss:  55.909059103020844\n",
      "Epoch:  691  Loss:  55.90872586169749\n",
      "Epoch:  692  Loss:  55.90839358444524\n",
      "Epoch:  693  Loss:  55.90806226709226\n",
      "Epoch:  694  Loss:  55.907731905490756\n",
      "Epoch:  695  Loss:  55.90740249551674\n",
      "Epoch:  696  Loss:  55.9070740330699\n",
      "Epoch:  697  Loss:  55.906746514073404\n",
      "Epoch:  698  Loss:  55.906419934473746\n",
      "Epoch:  699  Loss:  55.906094290240546\n",
      "Epoch:  700  Loss:  55.905769577366506\n",
      "Epoch:  701  Loss:  55.905445791867066\n",
      "Epoch:  702  Loss:  55.90512292978036\n",
      "Epoch:  703  Loss:  55.90480098716703\n",
      "Epoch:  704  Loss:  55.904479960110024\n",
      "Epoch:  705  Loss:  55.90415984471461\n",
      "Epoch:  706  Loss:  55.90384063710791\n",
      "Epoch:  707  Loss:  55.903522333439035\n",
      "Epoch:  708  Loss:  55.9032049298788\n",
      "Epoch:  709  Loss:  55.90288842261957\n",
      "Epoch:  710  Loss:  55.902572807875124\n",
      "Epoch:  711  Loss:  55.90225808188052\n",
      "Epoch:  712  Loss:  55.90194424089193\n",
      "Epoch:  713  Loss:  55.901631281186546\n",
      "Epoch:  714  Loss:  55.9013191990623\n",
      "Epoch:  715  Loss:  55.90100799083785\n",
      "Epoch:  716  Loss:  55.900697652852365\n",
      "Epoch:  717  Loss:  55.90038818146545\n",
      "Epoch:  718  Loss:  55.900079573056914\n",
      "Epoch:  719  Loss:  55.8997718240267\n",
      "Epoch:  720  Loss:  55.899464930794764\n",
      "Epoch:  721  Loss:  55.89915888980076\n",
      "Epoch:  722  Loss:  55.898853697504165\n",
      "Epoch:  723  Loss:  55.89854935038403\n",
      "Epoch:  724  Loss:  55.89824584493873\n",
      "Epoch:  725  Loss:  55.89794317768599\n",
      "Epoch:  726  Loss:  55.89764134516266\n",
      "Epoch:  727  Loss:  55.8973403439247\n",
      "Epoch:  728  Loss:  55.89704017054685\n",
      "Epoch:  729  Loss:  55.89674082162273\n",
      "Epoch:  730  Loss:  55.89644229376453\n",
      "Epoch:  731  Loss:  55.89614458360293\n",
      "Epoch:  732  Loss:  55.89584768778714\n",
      "Epoch:  733  Loss:  55.895551602984426\n",
      "Epoch:  734  Loss:  55.89525632588034\n",
      "Epoch:  735  Loss:  55.894961853178415\n",
      "Epoch:  736  Loss:  55.894668181600075\n",
      "Epoch:  737  Loss:  55.89437530788445\n",
      "Epoch:  738  Loss:  55.89408322878843\n",
      "Epoch:  739  Loss:  55.89379194108639\n",
      "Epoch:  740  Loss:  55.89350144157006\n",
      "Epoch:  741  Loss:  55.893211727048566\n",
      "Epoch:  742  Loss:  55.89292279434815\n",
      "Epoch:  743  Loss:  55.89263464031215\n",
      "Epoch:  744  Loss:  55.892347261800815\n",
      "Epoch:  745  Loss:  55.89206065569131\n",
      "Epoch:  746  Loss:  55.89177481887741\n",
      "Epoch:  747  Loss:  55.891489748269585\n",
      "Epoch:  748  Loss:  55.891205440794856\n",
      "Epoch:  749  Loss:  55.89092189339649\n",
      "Epoch:  750  Loss:  55.89063910303415\n",
      "Epoch:  751  Loss:  55.890357066683606\n",
      "Epoch:  752  Loss:  55.89007578133677\n",
      "Epoch:  753  Loss:  55.889795244001455\n",
      "Epoch:  754  Loss:  55.889515451701335\n",
      "Epoch:  755  Loss:  55.889236401475905\n",
      "Epoch:  756  Loss:  55.8889580903802\n",
      "Epoch:  757  Loss:  55.88868051548487\n",
      "Epoch:  758  Loss:  55.888403673875956\n",
      "Epoch:  759  Loss:  55.88812756265494\n",
      "Epoch:  760  Loss:  55.88785217893843\n",
      "Epoch:  761  Loss:  55.88757751985821\n",
      "Epoch:  762  Loss:  55.88730358256111\n",
      "Epoch:  763  Loss:  55.88703036420894\n",
      "Epoch:  764  Loss:  55.88675786197829\n",
      "Epoch:  765  Loss:  55.88648607306051\n",
      "Epoch:  766  Loss:  55.88621499466165\n",
      "Epoch:  767  Loss:  55.885944624002285\n",
      "Epoch:  768  Loss:  55.88567495831744\n",
      "Epoch:  769  Loss:  55.885405994856505\n",
      "Epoch:  770  Loss:  55.88513773088317\n",
      "Epoch:  771  Loss:  55.884870163675316\n",
      "Epoch:  772  Loss:  55.884603290524836\n",
      "Epoch:  773  Loss:  55.884337108737746\n",
      "Epoch:  774  Loss:  55.88407161563388\n",
      "Epoch:  775  Loss:  55.88380680854696\n",
      "Epoch:  776  Loss:  55.88354268482429\n",
      "Epoch:  777  Loss:  55.88327924182702\n",
      "Epoch:  778  Loss:  55.88301647692974\n",
      "Epoch:  779  Loss:  55.882754387520535\n",
      "Epoch:  780  Loss:  55.882492971000836\n",
      "Epoch:  781  Loss:  55.882232224785454\n",
      "Epoch:  782  Loss:  55.88197214630229\n",
      "Epoch:  783  Loss:  55.881712732992476\n",
      "Epoch:  784  Loss:  55.88145398231016\n",
      "Epoch:  785  Loss:  55.88119589172247\n",
      "Epoch:  786  Loss:  55.880938458709366\n",
      "Epoch:  787  Loss:  55.88068168076361\n",
      "Epoch:  788  Loss:  55.880425555390715\n",
      "Epoch:  789  Loss:  55.88017008010881\n",
      "Epoch:  790  Loss:  55.87991525244858\n",
      "Epoch:  791  Loss:  55.87966106995322\n",
      "Epoch:  792  Loss:  55.87940753017827\n",
      "Epoch:  793  Loss:  55.87915463069161\n",
      "Epoch:  794  Loss:  55.87890236907338\n",
      "Epoch:  795  Loss:  55.87865074291588\n",
      "Epoch:  796  Loss:  55.87839974982349\n",
      "Epoch:  797  Loss:  55.87814938741256\n",
      "Epoch:  798  Loss:  55.8778996533115\n",
      "Epoch:  799  Loss:  55.87765054516049\n",
      "Epoch:  800  Loss:  55.87740206061149\n",
      "Epoch:  801  Loss:  55.87715419732822\n",
      "Epoch:  802  Loss:  55.87690695298603\n",
      "Epoch:  803  Loss:  55.87666032527185\n",
      "Epoch:  804  Loss:  55.876414311884126\n",
      "Epoch:  805  Loss:  55.876168910532684\n",
      "Epoch:  806  Loss:  55.875924118938705\n",
      "Epoch:  807  Loss:  55.87567993483473\n",
      "Epoch:  808  Loss:  55.87543635596443\n",
      "Epoch:  809  Loss:  55.87519338008271\n",
      "Epoch:  810  Loss:  55.87495100495551\n",
      "Epoch:  811  Loss:  55.874709228359734\n",
      "Epoch:  812  Loss:  55.87446804808333\n",
      "Epoch:  813  Loss:  55.87422746192504\n",
      "Epoch:  814  Loss:  55.87398746769445\n",
      "Epoch:  815  Loss:  55.87374806321189\n",
      "Epoch:  816  Loss:  55.87350924630839\n",
      "Epoch:  817  Loss:  55.873271014825534\n",
      "Epoch:  818  Loss:  55.873033366615516\n",
      "Epoch:  819  Loss:  55.87279629954096\n",
      "Epoch:  820  Loss:  55.872559811474936\n",
      "Epoch:  821  Loss:  55.87232390030087\n",
      "Epoch:  822  Loss:  55.87208856391251\n",
      "Epoch:  823  Loss:  55.87185380021378\n",
      "Epoch:  824  Loss:  55.871619607118795\n",
      "Epoch:  825  Loss:  55.87138598255179\n",
      "Epoch:  826  Loss:  55.87115292444703\n",
      "Epoch:  827  Loss:  55.87092043074882\n",
      "Epoch:  828  Loss:  55.87068849941126\n",
      "Epoch:  829  Loss:  55.870457128398485\n",
      "Epoch:  830  Loss:  55.870226315684306\n",
      "Epoch:  831  Loss:  55.86999605925232\n",
      "Epoch:  832  Loss:  55.86976635709587\n",
      "Epoch:  833  Loss:  55.86953720721784\n",
      "Epoch:  834  Loss:  55.86930860763073\n",
      "Epoch:  835  Loss:  55.86908055635657\n",
      "Epoch:  836  Loss:  55.86885305142685\n",
      "Epoch:  837  Loss:  55.868626090882415\n",
      "Epoch:  838  Loss:  55.86839967277349\n",
      "Epoch:  839  Loss:  55.868173795159606\n",
      "Epoch:  840  Loss:  55.8679484561095\n",
      "Epoch:  841  Loss:  55.86772365370113\n",
      "Epoch:  842  Loss:  55.86749938602155\n",
      "Epoch:  843  Loss:  55.86727565116687\n",
      "Epoch:  844  Loss:  55.867052447242244\n",
      "Epoch:  845  Loss:  55.86682977236182\n",
      "Epoch:  846  Loss:  55.8666076246486\n",
      "Epoch:  847  Loss:  55.866386002234485\n",
      "Epoch:  848  Loss:  55.86616490326015\n",
      "Epoch:  849  Loss:  55.86594432587508\n",
      "Epoch:  850  Loss:  55.86572426823742\n",
      "Epoch:  851  Loss:  55.865504728513976\n",
      "Epoch:  852  Loss:  55.865285704880186\n",
      "Epoch:  853  Loss:  55.86506719552\n",
      "Epoch:  854  Loss:  55.86484919862589\n",
      "Epoch:  855  Loss:  55.86463171239883\n",
      "Epoch:  856  Loss:  55.86441473504805\n",
      "Epoch:  857  Loss:  55.864198264791334\n",
      "Epoch:  858  Loss:  55.863982299854584\n",
      "Epoch:  859  Loss:  55.8637668384721\n",
      "Epoch:  860  Loss:  55.863551878886355\n",
      "Epoch:  861  Loss:  55.86333741934793\n",
      "Epoch:  862  Loss:  55.86312345811557\n",
      "Epoch:  863  Loss:  55.862909993456086\n",
      "Epoch:  864  Loss:  55.862697023644266\n",
      "Epoch:  865  Loss:  55.86248454696297\n",
      "Epoch:  866  Loss:  55.86227256170287\n",
      "Epoch:  867  Loss:  55.8620610661626\n",
      "Epoch:  868  Loss:  55.86185005864862\n",
      "Epoch:  869  Loss:  55.8616395374752\n",
      "Epoch:  870  Loss:  55.86142950096425\n",
      "Epoch:  871  Loss:  55.86121994744552\n",
      "Epoch:  872  Loss:  55.86101087525634\n",
      "Epoch:  873  Loss:  55.86080228274171\n",
      "Epoch:  874  Loss:  55.86059416825419\n",
      "Epoch:  875  Loss:  55.86038653015375\n",
      "Epoch:  876  Loss:  55.86017936680806\n",
      "Epoch:  877  Loss:  55.859972676592065\n",
      "Epoch:  878  Loss:  55.85976645788817\n",
      "Epoch:  879  Loss:  55.85956070908618\n",
      "Epoch:  880  Loss:  55.85935542858313\n",
      "Epoch:  881  Loss:  55.8591506147834\n",
      "Epoch:  882  Loss:  55.85894626609856\n",
      "Epoch:  883  Loss:  55.85874238094742\n",
      "Epoch:  884  Loss:  55.85853895775588\n",
      "Epoch:  885  Loss:  55.858335994957024\n",
      "Epoch:  886  Loss:  55.858133490990966\n",
      "Epoch:  887  Loss:  55.85793144430487\n",
      "Epoch:  888  Loss:  55.85772985335286\n",
      "Epoch:  889  Loss:  55.857528716596065\n",
      "Epoch:  890  Loss:  55.8573280325025\n",
      "Epoch:  891  Loss:  55.85712779954706\n",
      "Epoch:  892  Loss:  55.85692801621148\n",
      "Epoch:  893  Loss:  55.85672868098427\n",
      "Epoch:  894  Loss:  55.856529792360746\n",
      "Epoch:  895  Loss:  55.8563313488429\n",
      "Epoch:  896  Loss:  55.856133348939466\n",
      "Epoch:  897  Loss:  55.855935791165805\n",
      "Epoch:  898  Loss:  55.855738674043835\n",
      "Epoch:  899  Loss:  55.855541996102126\n",
      "Epoch:  900  Loss:  55.855345755875774\n",
      "Epoch:  901  Loss:  55.85514995190634\n",
      "Epoch:  902  Loss:  55.85495458274189\n",
      "Epoch:  903  Loss:  55.85475964693685\n",
      "Epoch:  904  Loss:  55.85456514305216\n",
      "Epoch:  905  Loss:  55.85437106965503\n",
      "Epoch:  906  Loss:  55.854177425319\n",
      "Epoch:  907  Loss:  55.853984208623956\n",
      "Epoch:  908  Loss:  55.853791418155964\n",
      "Epoch:  909  Loss:  55.85359905250738\n",
      "Epoch:  910  Loss:  55.85340711027672\n",
      "Epoch:  911  Loss:  55.853215590068615\n",
      "Epoch:  912  Loss:  55.85302449049388\n",
      "Epoch:  913  Loss:  55.85283381016937\n",
      "Epoch:  914  Loss:  55.852643547718046\n",
      "Epoch:  915  Loss:  55.8524537017688\n",
      "Epoch:  916  Loss:  55.85226427095661\n",
      "Epoch:  917  Loss:  55.8520752539223\n",
      "Epoch:  918  Loss:  55.851886649312746\n",
      "Epoch:  919  Loss:  55.85169845578059\n",
      "Epoch:  920  Loss:  55.8515106719844\n",
      "Epoch:  921  Loss:  55.85132329658859\n",
      "Epoch:  922  Loss:  55.85113632826328\n",
      "Epoch:  923  Loss:  55.85094976568443\n",
      "Epoch:  924  Loss:  55.85076360753372\n",
      "Epoch:  925  Loss:  55.85057785249848\n",
      "Epoch:  926  Loss:  55.850392499271784\n",
      "Epoch:  927  Loss:  55.85020754655232\n",
      "Epoch:  928  Loss:  55.85002299304431\n",
      "Epoch:  929  Loss:  55.84983883745769\n",
      "Epoch:  930  Loss:  55.84965507850781\n",
      "Epoch:  931  Loss:  55.84947171491566\n",
      "Epoch:  932  Loss:  55.84928874540765\n",
      "Epoch:  933  Loss:  55.849106168715636\n",
      "Epoch:  934  Loss:  55.84892398357696\n",
      "Epoch:  935  Loss:  55.84874218873433\n",
      "Epoch:  936  Loss:  55.84856078293584\n",
      "Epoch:  937  Loss:  55.84837976493488\n",
      "Epoch:  938  Loss:  55.84819913349026\n",
      "Epoch:  939  Loss:  55.848018887365996\n",
      "Epoch:  940  Loss:  55.847839025331375\n",
      "Epoch:  941  Loss:  55.84765954616092\n",
      "Epoch:  942  Loss:  55.847480448634386\n",
      "Epoch:  943  Loss:  55.84730173153665\n",
      "Epoch:  944  Loss:  55.847123393657796\n",
      "Epoch:  945  Loss:  55.846945433793\n",
      "Epoch:  946  Loss:  55.84676785074253\n",
      "Epoch:  947  Loss:  55.84659064331173\n",
      "Epoch:  948  Loss:  55.84641381031096\n",
      "Epoch:  949  Loss:  55.84623735055564\n",
      "Epoch:  950  Loss:  55.84606126286617\n",
      "Epoch:  951  Loss:  55.8458855460679\n",
      "Epoch:  952  Loss:  55.84571019899101\n",
      "Epoch:  953  Loss:  55.845535220470836\n",
      "Epoch:  954  Loss:  55.84536060934734\n",
      "Epoch:  955  Loss:  55.84518636446556\n",
      "Epoch:  956  Loss:  55.845012484675216\n",
      "Epoch:  957  Loss:  55.844838968830835\n",
      "Epoch:  958  Loss:  55.84466581579186\n",
      "Epoch:  959  Loss:  55.84449302442237\n",
      "Epoch:  960  Loss:  55.84432059359121\n",
      "Epoch:  961  Loss:  55.84414852217196\n",
      "Epoch:  962  Loss:  55.84397680904288\n",
      "Epoch:  963  Loss:  55.84380545308683\n",
      "Epoch:  964  Loss:  55.843634453191356\n",
      "Epoch:  965  Loss:  55.84346380824864\n",
      "Epoch:  966  Loss:  55.84329351715541\n",
      "Epoch:  967  Loss:  55.84312357881295\n",
      "Epoch:  968  Loss:  55.84295399212715\n",
      "Epoch:  969  Loss:  55.842784756008314\n",
      "Epoch:  970  Loss:  55.84261586937137\n",
      "Epoch:  971  Loss:  55.84244733113555\n",
      "Epoch:  972  Loss:  55.84227914022471\n",
      "Epoch:  973  Loss:  55.84211129556703\n",
      "Epoch:  974  Loss:  55.841943796095094\n",
      "Epoch:  975  Loss:  55.84177664074589\n",
      "Epoch:  976  Loss:  55.841609828460754\n",
      "Epoch:  977  Loss:  55.84144335818538\n",
      "Epoch:  978  Loss:  55.84127722886975\n",
      "Epoch:  979  Loss:  55.84111143946812\n",
      "Epoch:  980  Loss:  55.840945988939076\n",
      "Epoch:  981  Loss:  55.840780876245375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  982  Loss:  55.840616100354026\n",
      "Epoch:  983  Loss:  55.84045166023629\n",
      "Epoch:  984  Loss:  55.84028755486756\n",
      "Epoch:  985  Loss:  55.84012378322741\n",
      "Epoch:  986  Loss:  55.839960344299556\n",
      "Epoch:  987  Loss:  55.8397972370718\n",
      "Epoch:  988  Loss:  55.839634460536075\n",
      "Epoch:  989  Loss:  55.83947201368841\n",
      "Epoch:  990  Loss:  55.83930989552886\n",
      "Epoch:  991  Loss:  55.83914810506147\n",
      "Epoch:  992  Loss:  55.83898664129444\n",
      "Epoch:  993  Loss:  55.838825503239846\n",
      "Epoch:  994  Loss:  55.838664689913756\n",
      "Epoch:  995  Loss:  55.83850420033625\n",
      "Epoch:  996  Loss:  55.838344033531286\n",
      "Epoch:  997  Loss:  55.83818418852679\n",
      "Epoch:  998  Loss:  55.8380246643545\n",
      "Epoch:  999  Loss:  55.837865460050104\n",
      "['department', 'we', 'program', 'computer', 'professors']\n"
     ]
    }
   ],
   "source": [
    "text_corpus = \"\" \n",
    "text_corpus += \"Welcome to the Department of Computer Science. We have great faculty and professors. We will have a welcome program today.\"\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Data preprocessing  \n",
    "training_data = preprocessing(text_corpus)\n",
    "\n",
    "# Word2Vec\n",
    "w2v = word2vec()\n",
    "\n",
    "# Generate Training data\n",
    "generate_training_data(training_data,w2v)\n",
    "\n",
    "# Train the model\n",
    "w2v.train(epochs)\n",
    "\n",
    "# Predict using model  \n",
    "print(w2v.predict(\"welcome\", 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
