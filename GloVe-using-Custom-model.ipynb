{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe-using-Custom-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIWwFIz4H6tkmijzYgaiTF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rojinadeuja/NLP-Model-Implementations/blob/master/GloVe-using-Custom-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXT3cSbX4kvG"
      },
      "source": [
        "## How to train GloVe on a custom corpus\n",
        "If the web datasets above don't match the semantics of your end use case, you can train word vectors on your own corpus.\n",
        "\n",
        "1. Clone the repository from \n",
        "> `$ git clone http://github.com/stanfordnlp/glove`\n",
        "\n",
        "2. To run the model\n",
        "> `$ cd glove && make`\n",
        "\n",
        "3. To train it on your own corpus, you make to make changes to the *demo.sh* file \n",
        "> The demo.sh script downloads a small corpus *text8*, consisting of the first 100M characters of Wikipedia. It collects unigram counts, constructs and shuffles cooccurrence data, and trains a simple version of the GloVe model. It also runs a word analogy evaluation script in python to verify word vector quality. \n",
        "\n",
        "4. Remove the script from *if* to *fi* after '*make*'\n",
        "5. Replace the CORPUS name with your corpus file name 'filename.txt'\n",
        "6. At the end of the file, there is another *if* loop. Replace text8 with your corpus file name\n",
        "> `if [ \"$CORPUS\" = 'text8' ]; then`\n",
        "\n",
        "7. Run *demo.sh* once the changes are made\n",
        "> `$ ./demo.sh`\n",
        "\n",
        "8. Note: You can also change other model parameters inside the *demo.sh* file. For eg. *vector size*, *window size*, *minimum count* and so on\n",
        "\n",
        "## How to create and use the corpus file\n",
        "- To train your own GloVe vectors, first you'll need to prepare your corpus as a single text file with all words separated by one or more spaces or tabs. \n",
        "\n",
        "- If your corpus has multiple documents, the documents (only) should be separated by new line characters. Co-occurrence contexts for words do not extend past newline characters.\n",
        "\n",
        "To create your corpus file in the correct format, follow the instructions on my notebook **[CSV-to-Corpus](https://github.com/rojinadeuja/Data-Processing-Utilities/blob/main/CSV-to-Corpus.ipynb)**\n",
        "\n",
        "- Once you get the file, move it to the root folder inside your *glove* directory.\n",
        "\n",
        "## How to get GloVe embeddings\n",
        "- After you run the *demo.sh* script, a *vector.txt* file will be generated that will contain all the word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZVyic0oJ3y2"
      },
      "source": [
        "# Use your custom word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AjHa30KYVfl",
        "outputId": "18fa7d40-bd52-4f9b-9a6d-acbe0be2b07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdkNqJuvcHmL"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1C_DTIcXjTY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7aODSUpcKsg"
      },
      "source": [
        "## Load Dataset\n",
        "The IMDb Dataset can also be downloaded using Keras file utility.\n",
        " \n",
        "```\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)\n",
        "```\n",
        "The *train/* directory has *pos* and *neg* folders with movie reviews labelled as positive and negative respectively. You can reviews from *pos* and *neg* folders to train a binary classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwVcCxTiX-S-",
        "outputId": "36c1bdc0-5265-4dea-c47d-820b79f52520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/IMDB.csv')\n",
        "df.replace(['positive', 'negative'], [1, 0], inplace=True)\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRgCBkWDcNu8"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLNO6qbrzxAH"
      },
      "source": [
        "def preprocess(s):\n",
        "    '''Function for data pre-processing'''\n",
        "    # Removing html tags\n",
        "    TAG_RE = re.compile(r'<[^>]+>')\n",
        "    sentence = TAG_RE.sub('', s)\n",
        "    \n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFa5J1DOcTjk"
      },
      "source": [
        "## Create X and y matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGisaXSFZ56Z"
      },
      "source": [
        "# Create feature matrix\n",
        "X = []\n",
        "sentences = list(df['review'])\n",
        "for sentence in sentences:\n",
        "    X.append(preprocess(sentence))\n",
        "\n",
        "# Create target vector\n",
        "y = df['sentiment']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFx01zpRcY39"
      },
      "source": [
        "## Split dataset into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXpSJf88ajsA"
      },
      "source": [
        "# Create train-test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ9t90oWbPVq"
      },
      "source": [
        "# Convert into numpy arrays for processing with tensorflo\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsAUDL_pcni6"
      },
      "source": [
        "## Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCXTQ6mZbkpm"
      },
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAMMczL3b7FX"
      },
      "source": [
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vAPGmP-crdo"
      },
      "source": [
        "## Create Embeddings\n",
        "Keras makes it easy to use word embeddings. We will use our custom embeddings rather than randomly initializing the embedding layer.\n",
        "\n",
        "The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer. \n",
        "\n",
        "For this experiment, we have set the dimensionality to 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YnoSFF0b-bw"
      },
      "source": [
        "# Create embeddings\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/drive/My Drive/Colab Notebooks/glove.imdb.50k.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpSkMYudcAca"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySnrYi7-cu-m"
      },
      "source": [
        "## Classification using a simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm1t4XoxfdDC"
      },
      "source": [
        "# Text Classification with a Simple Neural Network\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufFg6IJ9cHB8",
        "outputId": "649d9317-1915-4c0c-9acc-1d4d7ca29af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          9254700   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 10001     \n",
            "=================================================================\n",
            "Total params: 9,264,701\n",
            "Trainable params: 10,001\n",
            "Non-trainable params: 9,254,700\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSU8-VsJdIdB"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lbGC_5ngWE5",
        "outputId": "2480d894-81aa-4ee8-f707-4d445d90d507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.5145 - acc: 0.7442 - val_loss: 0.4489 - val_acc: 0.7874\n",
            "Epoch 2/6\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3941 - acc: 0.8269 - val_loss: 0.4129 - val_acc: 0.8116\n",
            "Epoch 3/6\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3572 - acc: 0.8463 - val_loss: 0.4283 - val_acc: 0.8074\n",
            "Epoch 4/6\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3360 - acc: 0.8571 - val_loss: 0.4155 - val_acc: 0.8094\n",
            "Epoch 5/6\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3184 - acc: 0.8659 - val_loss: 0.4241 - val_acc: 0.8058\n",
            "Epoch 6/6\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3083 - acc: 0.8695 - val_loss: 0.4307 - val_acc: 0.8058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEwXG8R-dCu2"
      },
      "source": [
        "## Evaluate model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXJPPX9pghOo",
        "outputId": "a8a98aaa-957d-4e5c-c593-da57e56168f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"\\nTest Accuracy:\", score[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.4275 - acc: 0.8028\n",
            "\n",
            "Test Accuracy: 0.8027999997138977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVUJIBSEKmb1"
      },
      "source": [
        "## Results\n",
        "The test accuracy of our model trained on custom word embeddings was found to be 80.5%"
      ]
    }
  ]
}