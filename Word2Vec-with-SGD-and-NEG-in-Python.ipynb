{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with SGD (Stochastic Gradient Descent) and NEG (Negative Sampling) \n",
    "### Implementation Steps:\n",
    "1. Assign the target word and neighboring context words as **Positive** examples.\n",
    "2. Assign randomly sampled words in the lexicon based on a unigram distrubution (built using word frequency) as **Negative** examples.\n",
    "3. Train the model using a Logistic Classifier by optimizing the loss function.\n",
    "4. Use the regression weights as the embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''Function to compute the value of x after applying the Sigmoid function'''\n",
    "    return 1.0 /(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    '''Function for data preprocessing'''\n",
    "    processed = []\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Split text corpus into sentences\n",
    "    sentences = corpus.split(\".\")\n",
    "    \n",
    "    # Loop through each sentence\n",
    "    for i in range(len(sentences)):\n",
    "        \n",
    "        # Remove leading and trailing characters\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        \n",
    "        # Split sentence into list of words\n",
    "        sentence = sentences[i].split()\n",
    "        \n",
    "        # Remove punctuations\n",
    "        x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "        \n",
    "        # Convert to lower case\n",
    "        x = [word.lower() for word in x]\n",
    "        \n",
    "        processed.append(x) \n",
    "        \n",
    "    print('\\nProcessed sentence is:',  processed)\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec_with_NEG:\n",
    "    '''Implmentation of Skip-Gram Word2Vec model with negative sampling'''\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.N = 5 # dimension of word embeddings\n",
    "        self.learning_rate = 0.01 # learning rate\n",
    "        self.epochs = 5000 # number of training epochs\n",
    "        self.window = 2 # window size\n",
    "        self.negative_rate = 5 #ratio of negative samples over positive samples\n",
    "        self.min_count = 5 # minimum count of words to be considered\n",
    "        self.word2idx = None\n",
    "        self.unigram = None\n",
    "        pass\n",
    "    \n",
    "    def generate_training_data(unigram_power=0.75):\n",
    "        '''Function to generate the word counts and mapping from word to index and vice versa\n",
    "        Input: List of tokenized sentences\n",
    "        Output: \n",
    "        v: Vocabulary size\n",
    "        word_list: list of words in vocabulary sorted in alphabetical order\n",
    "        word2idx: dict with word as key and index as value\n",
    "        word_freq: dict with word as key and frequency as value'''\n",
    "        \n",
    "         # Initialize a dictionary of word frequency\n",
    "        word_freq = {}\n",
    "        \n",
    "        # Iterate over each sentence in the list of sentences\n",
    "        for sent in self.sentences:\n",
    "            # Iterate over each word in sentence\n",
    "            for word in sent:\n",
    "                # Create the frequency dictionary to count each word\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        # Remove words that have frequency < minCount\n",
    "        if self.min_count > 1:\n",
    "            word_freq = {word:freq for word, freq in word_freq.items() if freq >= self.min_count}\n",
    "\n",
    "        # Create word2idx and idx2word dictionaries from word_list\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(word_freq.keys())}\n",
    "\n",
    "        # Compute unigram\n",
    "        \n",
    "        # Initialize an array of unigram\n",
    "        unigram = np.zeros(len(self.word2idx))\n",
    "        \n",
    "        # Iterate over list of words and calculate the probability for each word\n",
    "        for word, frequency in word_freq.items():\n",
    "            # Raise each word frequency to the power chosen\n",
    "            f = frequency ** unigram_power\n",
    "            # Update unigram array\n",
    "            unigram[self.word2idx[word]] = f\n",
    "        \n",
    "        # Normalization\n",
    "        self.unigram = unigram / np.sum(unigram)\n",
    "    \n",
    "    def generate_positive_words():\n",
    "        '''Function to generate positive training words'''\n",
    "        \n",
    "        P = [] # Initialize list of positive words\n",
    "        V = len(self.word2id) # Size of vocabulary\n",
    "        \n",
    "        N_sentences = len(self.sentences)\n",
    "        \n",
    "        # If the word does not exist in the dictionary (due to min_count) then set its index to -1\n",
    "        sentences_index_form = [None]* N_Sentences\n",
    "        for idx, sent in enumerate(self.sentences):\n",
    "            sentences_index_form[idx] = [self.word2idx.get(w, -1) for w in sent]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed sentence is: [['welcome', 'students', 'department', 'computer', 'science'], ['we', 'great', 'faculty', 'professors'], ['we', 'welcome', 'program', 'today'], []]\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0) \n",
    "\n",
    "# Get text data\n",
    "text = \"Welcome students to the Department of Computer Science. We have great faculty and professors. We will have a welcome program today.\"\n",
    "\n",
    "# Pre-process the data\n",
    "corpus = preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welcome', 'students', 'department', 'computer', 'science'],\n",
       " ['we', 'great', 'faculty', 'professors'],\n",
       " ['we', 'welcome', 'program', 'today'],\n",
       " []]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
