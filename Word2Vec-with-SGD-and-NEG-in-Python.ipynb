{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with SGD (Stochastic Gradient Descent) and NEG (Negative Sampling) \n",
    "### Implementation Steps:\n",
    "1. Assign the target word and neighboring context words as **Positive** examples.\n",
    "2. Assign randomly sampled words in the lexicon based on a unigram distrubution (built using word frequency) as **Negative** examples.\n",
    "3. Train the model using a Logistic Classifier by optimizing the loss function.\n",
    "4. Use the regression weights as the embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''Function to compute the value of x after applying the Sigmoid function'''\n",
    "    return 1.0 /(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    '''Function for data preprocessing'''\n",
    "    processed = []\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Split text corpus into sentences\n",
    "    sentences = corpus.split(\".\")\n",
    "    \n",
    "    # Loop through each sentence\n",
    "    for i in range(len(sentences)):\n",
    "        \n",
    "        # Remove leading and trailing characters\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        \n",
    "        # Split sentence into list of words\n",
    "        sentence = sentences[i].split()\n",
    "        \n",
    "        # Remove punctuations\n",
    "        x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "        \n",
    "        # Convert to lower case\n",
    "        x = [word.lower() for word in x]\n",
    "        \n",
    "        processed.append(x) \n",
    "        \n",
    "    print('\\nProcessed sentence is:',  processed)\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec_with_NEG:\n",
    "    '''Implmentation of Skip-Gram Word2Vec model with negative sampling'''\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.N = 5 # dimension of word embeddings\n",
    "        self.learning_rate = 0.01 # learning rate\n",
    "        self.epochs = 5000 # number of training epochs\n",
    "        self.window = 2 # window size\n",
    "        self.negative_rate = 5 #ratio of negative samples over positive samples\n",
    "        self.min_count = 5 # minimum count of words to be considered\n",
    "        self.word2idx = None\n",
    "        self.unigram = None\n",
    "        pass\n",
    "    \n",
    "    def generate_training_data(unigram_power=0.75):\n",
    "        '''Function to generate the word counts and mapping from word to index and vice versa\n",
    "        Input: List of tokenized sentences\n",
    "        Output: \n",
    "        v: Vocabulary size\n",
    "        word_list: list of words in vocabulary sorted in alphabetical order\n",
    "        word2idx: dict with word as key and index as value\n",
    "        word_freq: dict with word as key and frequency as value'''\n",
    "        \n",
    "         # Initialize a dictionary of word frequency\n",
    "        word_freq = {}\n",
    "        \n",
    "        # Iterate over each sentence in the list of sentences\n",
    "        for sent in self.sentences:\n",
    "            # Iterate over each word in sentence\n",
    "            for word in sent:\n",
    "                # Create the frequency dictionary to count each word\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        # Remove words that have frequency < minCount\n",
    "        if self.min_count > 1:\n",
    "            word_freq = {word:freq for word, freq in word_freq.items() if freq >= self.min_count}\n",
    "\n",
    "        # Create word2idx and idx2word dictionaries from word_list\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(word_freq.keys())}\n",
    "\n",
    "        # Compute unigram\n",
    "        \n",
    "        # Initialize an array of unigram\n",
    "        unigram = np.zeros(len(self.word2idx))\n",
    "        \n",
    "        # Iterate over list of words and calculate the probability for each word\n",
    "        for word, frequency in word_freq.items():\n",
    "            # Raise each word frequency to the power chosen\n",
    "            f = frequency ** unigram_power\n",
    "            # Update unigram array\n",
    "            unigram[self.word2idx[word]] = f\n",
    "        \n",
    "        # Normalization\n",
    "        self.unigram = unigram / np.sum(unigram)\n",
    "    \n",
    "    def generate_positive_words():\n",
    "        '''Function to generate positive training words'''\n",
    "        \n",
    "        P = [] # Initialize list of positive words\n",
    "        V = len(self.word2id) # Size of vocabulary\n",
    "        \n",
    "        N_sentences = len(self.sentences)\n",
    "        \n",
    "        # If the word does not exist in the dictionary (due to min_count) then set its index to -1\n",
    "        sentences_index_form = [None]* N_Sentences\n",
    "        for idx, sent in enumerate(self.sentences):\n",
    "            sentences_index_form[idx] = [self.word2idx.get(w, -1) for w in sent]\n",
    "        \n",
    "        # For efficiency, pre-compute the number of positives for each word\n",
    "        \n",
    "        N_P = np.zeros(V, dtype=int) # Number of positive words for each word\n",
    "        \n",
    "        for idx, sent_word_indices in enumerate(sentences_index_form):\n",
    "            for i, word_idx in enumerate(sent_word_indices):\n",
    "                if word_idx < 0:\n",
    "                    continue\n",
    "                first = max(0, i-self.winSize)\n",
    "                last = min(i+self.winSize+1, len(sent_word_indices))\n",
    "                N_P[word_idx] += (last - first - 1)\n",
    "                \n",
    "                \n",
    "        # Allocate the memory for P in advance\n",
    "        P = [None]*V\n",
    "        for word_idx in range(V):\n",
    "            P[word_idx] = np.zeros(N_P[word_idx], dtype=int)\n",
    "        P_next_position = [0]*V\n",
    "        \n",
    "        # Loop over each sentence in the corpus to extract the target word and context words\n",
    "        for idx, sent_word_indices in enumerate(sentences_index_form):\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print('Processing sentence', idx + 1, '/', N_Sentences)\n",
    "                \n",
    "            # Iterate over each word in sentence and add to its Positive list\n",
    "            for i, word_idx in enumerate(sent_word_indices):\n",
    "                if word_idx < 0:\n",
    "                    continue\n",
    "                first = max(0, i-self.N)\n",
    "                last = min(i+self.N+1, len(sent_word_indices))\n",
    "                number_of_words = (last - first - 1)\n",
    "                position = P_next_position[word_idx]\n",
    "                P[word_idx][position:position+number_of_words] = np.asarray(sent_word_indices[first:i] + sent_word_indices[i+1:last])\n",
    "                P_next_position[word_idx] += number_of_words\n",
    "                \n",
    "        if self.minCount > 1:\n",
    "            for word_idx in range(V):\n",
    "                P[word_idx] = np.delete(P[word_idx], np.where(P[word_idx] < 0))\n",
    "   \n",
    "        # Remove duplicates\n",
    "        for word_idx in range(V):\n",
    "            P[word_idx] = np.unique(P[word_idx])\n",
    "        return P\n",
    "    \n",
    "    def negative_sampling(t, P):\n",
    "        '''Function to draw negative samples from unigram distribution\n",
    "        Input: \n",
    "        t: Target word\n",
    "        P: List of positive words\n",
    "        Output:\n",
    "        List of indexes of negative samples\n",
    "        '''\n",
    "        # Remove indices of t and P as they cannot be negative\n",
    "        invalid_indices = P.tolist() + [t]\n",
    "        \n",
    "        probabilities = np.copy(self.unigram)\n",
    "        \n",
    "         # To avoid mistakenly obtaining postive samples or t itself,set the probabilities of these indices to 0\n",
    "        probabilities[invalid_indices] = 0\n",
    "        \n",
    "        probabilities /= np.sum(probabilities)\n",
    "        negative_samples = np.random.choice(len(self.unigram), size=self.negativeRate, p=probabilities)\n",
    "\n",
    "        return negative_samples\n",
    "    \n",
    "    def train(self, learning_rate, epochs):\n",
    "        '''Function to train the model\n",
    "        Output: Trained embeddings'''\n",
    "        \n",
    "        print('Generate training data')\n",
    "        self.generate_training_data()\n",
    "\n",
    "        print('\\nGenerate positive words')\n",
    "        P = self.generate_positive_words()\n",
    "\n",
    "        V = len(self.word2idx)\n",
    "        print('\\nTotal number of words in Vocabulary =', V)\n",
    "\n",
    "        # Initialization\n",
    "        W = np.random.rand(self.N, V)\n",
    "        C = np.random.rand(V, self.N)\n",
    "\n",
    "        losses = []\n",
    "        \n",
    "        # Loop through each epoch\n",
    "        for Pass in range(epochs):\n",
    "            print('\\nEpoch: ', Pass + 1)\n",
    "            \n",
    "            #Initialize loss\n",
    "            loss = 0\n",
    "            \n",
    "            # For each target word in V\n",
    "            for t in range(V):\n",
    "                # Get the current embedding vectors\n",
    "                wt = W[:, t]\n",
    "                positive_samples = P[t]\n",
    "                \n",
    "                for p in positive_samples:\n",
    "                    # Get the negative samples\n",
    "                    negative_samples = self.negative_sampling(t, positive_samples)\n",
    "                    \n",
    "                    # Get the context vector\n",
    "                    cp = C[p, :]\n",
    "                    cn = C[negative_samples, :]\n",
    "                    \n",
    "                    # Get intermedite values\n",
    "                    sp = sigmoid(-np.dot(cp, wt))\n",
    "                    sn = sigmoid(np.dot(cn, wt))\n",
    "                    \n",
    "                    # Calculate the partial derivatives\n",
    "                    dwt = - sp*cp + np.dot(sn, cn)\n",
    "                    dcp = - sp*wt\n",
    "                    dcn = np.outer(sn, wt)\n",
    "                    \n",
    "                    # Update the gradient descent\n",
    "                    wt -= learning_rate*dwt\n",
    "                    cp -= learning_rate*dcp\n",
    "                    cn -= learning_rate*dcn\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss += -np.log(sigmoid(np.dot(cp, wt))) + np.sum(-np.log(sigmoid(-np.dot(C_neg, wt))))\n",
    "                    \n",
    "                # Output progress\n",
    "                if (t+1)%100 == 0:\n",
    "                    print('\\t step ' + str(t + 1) + '/' + str(V) + 'loss: %.2f'%(loss), end='\\r')\n",
    "            \n",
    "            losses.append(loss)\n",
    "            \n",
    "            print('\\tLoss: %.2f' %loss, 'Elapsed time:', time.time() - start, '(s)--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed sentence is: [['welcome', 'students', 'department', 'computer', 'science'], ['we', 'great', 'faculty', 'professors'], ['we', 'welcome', 'program', 'today'], []]\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0) \n",
    "\n",
    "# Get text data\n",
    "text = \"Welcome students to the Department of Computer Science. We have great faculty and professors. We will have a welcome program today.\"\n",
    "\n",
    "# Pre-process the data\n",
    "corpus = preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
