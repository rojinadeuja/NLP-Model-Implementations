{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x. This implementation provides better numerical stability.'''\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    '''Function for data preprocessing'''\n",
    "    processed = []\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Split text corpus into sentences\n",
    "    sentences = corpus.split(\".\")\n",
    "    \n",
    "    # Loop through each sentence\n",
    "    for i in range(len(sentences)):\n",
    "        \n",
    "        # Remove leading and trailing characters\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        \n",
    "        # Split sentence into list of words\n",
    "        sentence = sentences[i].split()\n",
    "        \n",
    "        # Remove punctuations\n",
    "        x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "        \n",
    "        # Convert to lower case\n",
    "        x = [word.lower() for word in x]\n",
    "        \n",
    "        processed.append(x) \n",
    "        \n",
    "    print('\\nProcessed sentence is:',  processed)\n",
    "        \n",
    "    return processed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    '''Implementation of Skip-Gram Word2Vec model'''\n",
    "    def __init__(self):\n",
    "        self.N = 5 # dimension of word embeddings\n",
    "        self.learning_rate = 0.01 # learning rate\n",
    "        self.epochs = 5000 # number of training epochs\n",
    "        self.window = 2 # window size\n",
    "        pass\n",
    "    \n",
    "    def generate_training_data(self, corpus):\n",
    "        '''Function to generate training data for Word2Vec'''\n",
    "    \n",
    "        print('\\n------GENERATE TRANING DATA------')\n",
    "        \n",
    "        # Generate word counts\n",
    "        word_counts = defaultdict(int)\n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "                \n",
    "        # Get vocabulary size\n",
    "        self.V = len(word_counts.keys())\n",
    "        \n",
    "        # Get lookup dict\n",
    "        self.words_list = sorted(list(word_counts.keys()),reverse=False)\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        # Loop through each sentence in the corpus\n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "\n",
    "            # Loop through each word in the sentence\n",
    "            for i, word in enumerate(sentence):\n",
    "        \n",
    "                w_target = self.onehotencoding(sentence[i])\n",
    "\n",
    "                # Loop through context window\n",
    "                w_context = []\n",
    "                for j in range( i-self.window, i+self.window+1 ):\n",
    "                    if j!=i and j <= sent_len-1 and j >= 0:\n",
    "                        w_context.append( self.onehotencoding(sentence[j]) )\n",
    "                        \n",
    "                training_data.append([w_target, w_context])  \n",
    "                \n",
    "        return np.array(training_data)\n",
    "        \n",
    "    def onehotencoding(self, word):\n",
    "        '''Function to covert a word to one-hot-encoded value'''\n",
    "        word_vec = [0 for i in range(0, self.V)]\n",
    "        word_index = self.word_index[word]\n",
    "        word_vec[word_index] = 1\n",
    "        return word_vec\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        '''Function for feed-forward step'''\n",
    "        h = np.dot(self.W.T, x)\n",
    "        u = np.dot(self.W1.T, h)\n",
    "        y = softmax(u)\n",
    "        \n",
    "#         print('\\n----FEEDFORWARD STEP------')\n",
    "#         print('\\n Size of y is: ', y.shape)\n",
    "        return y, h, u\n",
    "    \n",
    "    def backpropagate(self, e, h, x):\n",
    "        '''Function for back propagation using Stochastic Gradient Descent step'''\n",
    "    \n",
    "        # Calculate partial derivative of loss function wrt W1\n",
    "        dEdW1 = np.outer(h, e)\n",
    "        \n",
    "        # Calculate partial derivative of loss function wrt W\n",
    "        dEdW = np.outer(x, np.dot(self.W1, e.T))\n",
    "        \n",
    "        # Update the weights\n",
    "        self.W = self.W - (self.learning_rate * dEdW)\n",
    "        self.W1 = self.W1 - (self.learning_rate * dEdW1)\n",
    "        \n",
    "#         print('\\n----BACKPROPAGATION STEP------')\n",
    "#         print('\\n Size of W and W1 is: ', self.W.shape, self.W1.shape)  \n",
    "        pass          \n",
    "    \n",
    "    def train(self, training_data, learning_rate = 0.01, epochs= 100):\n",
    "        '''Function to train the Word2Vec model'''\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weight matrices\n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "        \n",
    "#         print('\\n----INITIALIZE WEIGHTS------')\n",
    "#         print('\\n Size of W and W1 are: ', self.W.shape, self.W1.shape)\n",
    "        \n",
    "        # Loop through each epoch\n",
    "        for Pass in range(0, self.epochs):\n",
    "            \n",
    "            # Pick a random index\n",
    "            index = np.random.randint(len(training_data))\n",
    "            print('\\nIndex is:', index)\n",
    "            \n",
    "            # Pick a sample row from that index\n",
    "            td = np.reshape(training_data[index, :], (1,2))\n",
    "            \n",
    "            # Initialize Loss\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Loop through each training sample\n",
    "            for w_t, w_c in td:\n",
    "\n",
    "                # Feed-forward pass\n",
    "                y_pred, h, u = self.feed_forward(w_t)\n",
    "                \n",
    "                # Calculate error\n",
    "                EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
    "\n",
    "                # Backpropagation of error\n",
    "                self.backpropagate(EI, h, w_t)\n",
    "\n",
    "                # Calculate loss\n",
    "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "                #self.loss += -2*np.log(len(w_c)) -np.sum([u[word.index(1)] for word in w_c]) + (len(w_c) * np.log(np.sum(np.exp(u))))\n",
    "  \n",
    "            print( '\\nEPOCH:', Pass);\n",
    "            print( '\\nLOSS:', self.loss);\n",
    "            \n",
    "        pass\n",
    "            \n",
    "    def predict(self, word, number_of_predictions):\n",
    "        '''Function to predict context words using Word2Vec model'''\n",
    "        \n",
    "        # Check if word is contained in the dictionary\n",
    "        if word in self.words_list: \n",
    "            \n",
    "            index = self.word_index[word]\n",
    "            v_W = self.W[index]\n",
    "            \n",
    "            # Loop through words in vocabulary\n",
    "            word_sim = {}\n",
    "            for i in range(self.V):\n",
    "                v_W1 = self.W[i]\n",
    "                theta_num = np.dot(v_W, v_W1)\n",
    "                theta_den = np.linalg.norm(v_W) * np.linalg.norm(v_W1)\n",
    "                theta = theta_num / theta_den\n",
    "                \n",
    "                word = self.index_word[i]\n",
    "                word_sim[word] = theta\n",
    "                \n",
    "            words_sorted = sorted(word_sim.items(), key= lambda x:x[1], reverse=True)\n",
    "            \n",
    "            words_pred = []\n",
    "            \n",
    "            for word, sim in words_sorted[:number_of_predictions]:\n",
    "                words_pred.append(word)\n",
    "                \n",
    "            return words_pred\n",
    "        \n",
    "        else:\n",
    "            print(\"Error: Word not found in dicitonary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed sentence is: [['welcome', 'students', 'department', 'computer', 'science'], ['we', 'great', 'faculty', 'professors'], ['we', 'welcome', 'program', 'today'], []]\n",
      "\n",
      "------GENERATE TRANING DATA------\n",
      "\n",
      "Index is: 0\n",
      "\n",
      "EPOCH: 0\n",
      "\n",
      "LOSS: 4.9246952072504415\n",
      "\n",
      "Index is: 4\n",
      "\n",
      "EPOCH: 1\n",
      "\n",
      "LOSS: 4.718273080492166\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 2\n",
      "\n",
      "LOSS: 7.041954953983371\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 3\n",
      "\n",
      "LOSS: 7.120410173243705\n",
      "\n",
      "Index is: 12\n",
      "\n",
      "EPOCH: 4\n",
      "\n",
      "LOSS: 5.715571154118569\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 5\n",
      "\n",
      "LOSS: 8.33769325244503\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 6\n",
      "\n",
      "LOSS: 4.560888983393988\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 7\n",
      "\n",
      "LOSS: 7.09495999858318\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 8\n",
      "\n",
      "LOSS: 4.816363035972675\n",
      "\n",
      "Index is: 0\n",
      "\n",
      "EPOCH: 9\n",
      "\n",
      "LOSS: 4.886324803510533\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 10\n",
      "\n",
      "LOSS: 4.791230471608689\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 11\n",
      "\n",
      "LOSS: 4.54692544043471\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 12\n",
      "\n",
      "LOSS: 4.536418928065732\n",
      "\n",
      "Index is: 0\n",
      "\n",
      "EPOCH: 13\n",
      "\n",
      "LOSS: 4.841944630920873\n",
      "\n",
      "Index is: 12\n",
      "\n",
      "EPOCH: 14\n",
      "\n",
      "LOSS: 5.670577773193921\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 15\n",
      "\n",
      "LOSS: 4.510594529360361\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 16\n",
      "\n",
      "LOSS: 8.236564010418485\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 17\n",
      "\n",
      "LOSS: 4.5390620506996395\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 18\n",
      "\n",
      "LOSS: 7.008602152001849\n",
      "\n",
      "Index is: 1\n",
      "\n",
      "EPOCH: 19\n",
      "\n",
      "LOSS: 6.511604404557804\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 20\n",
      "\n",
      "LOSS: 4.772613152132984\n",
      "\n",
      "Index is: 0\n",
      "\n",
      "EPOCH: 21\n",
      "\n",
      "LOSS: 4.798131689221259\n",
      "\n",
      "Index is: 4\n",
      "\n",
      "EPOCH: 22\n",
      "\n",
      "LOSS: 4.678492058046674\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 23\n",
      "\n",
      "LOSS: 7.082403517476342\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 24\n",
      "\n",
      "LOSS: 7.057693827658542\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 25\n",
      "\n",
      "LOSS: 4.484235070691458\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 26\n",
      "\n",
      "LOSS: 8.137257864821754\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 27\n",
      "\n",
      "LOSS: 4.523973684814019\n",
      "\n",
      "Index is: 7\n",
      "\n",
      "EPOCH: 28\n",
      "\n",
      "LOSS: 8.437465951275009\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 29\n",
      "\n",
      "LOSS: 4.746698989209615\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 30\n",
      "\n",
      "LOSS: 4.721914215039989\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 31\n",
      "\n",
      "LOSS: 4.458032894540032\n",
      "\n",
      "Index is: 2\n",
      "\n",
      "EPOCH: 32\n",
      "\n",
      "LOSS: 9.70578938007073\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 33\n",
      "\n",
      "LOSS: 4.701581315884223\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 34\n",
      "\n",
      "LOSS: 8.066023513362474\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 35\n",
      "\n",
      "LOSS: 7.031752813780757\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 36\n",
      "\n",
      "LOSS: 7.975942146827588\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 37\n",
      "\n",
      "LOSS: 4.429294357079485\n",
      "\n",
      "Index is: 1\n",
      "\n",
      "EPOCH: 38\n",
      "\n",
      "LOSS: 6.501092520117853\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 39\n",
      "\n",
      "LOSS: 7.888453849153609\n",
      "\n",
      "Index is: 12\n",
      "\n",
      "EPOCH: 40\n",
      "\n",
      "LOSS: 5.631059477251307\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 41\n",
      "\n",
      "LOSS: 4.694337425674174\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 42\n",
      "\n",
      "LOSS: 4.669981636866957\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 43\n",
      "\n",
      "LOSS: 6.980435081455297\n",
      "\n",
      "Index is: 2\n",
      "\n",
      "EPOCH: 44\n",
      "\n",
      "LOSS: 9.672026315360748\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 45\n",
      "\n",
      "LOSS: 6.958261829527542\n",
      "\n",
      "Index is: 10\n",
      "\n",
      "EPOCH: 46\n",
      "\n",
      "LOSS: 7.244334356985564\n",
      "\n",
      "Index is: 12\n",
      "\n",
      "EPOCH: 47\n",
      "\n",
      "LOSS: 5.580502122104551\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 48\n",
      "\n",
      "LOSS: 7.820520312251254\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 49\n",
      "\n",
      "LOSS: 6.9358693270470955\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 50\n",
      "\n",
      "LOSS: 7.738552896581793\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 51\n",
      "\n",
      "LOSS: 4.508822835662686\n",
      "\n",
      "Index is: 7\n",
      "\n",
      "EPOCH: 52\n",
      "\n",
      "LOSS: 8.373098735448256\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 53\n",
      "\n",
      "LOSS: 6.996470101655586\n",
      "\n",
      "Index is: 0\n",
      "\n",
      "EPOCH: 54\n",
      "\n",
      "LOSS: 4.78257425384981\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 55\n",
      "\n",
      "LOSS: 6.977430461803403\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 56\n",
      "\n",
      "LOSS: 4.656287670268205\n",
      "\n",
      "Index is: 4\n",
      "\n",
      "EPOCH: 57\n",
      "\n",
      "LOSS: 4.638865821576603\n",
      "\n",
      "Index is: 10\n",
      "\n",
      "EPOCH: 58\n",
      "\n",
      "LOSS: 7.19768964172709\n",
      "\n",
      "Index is: 6\n",
      "\n",
      "EPOCH: 59\n",
      "\n",
      "LOSS: 7.67301909411099\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 60\n",
      "\n",
      "LOSS: 6.944860323589003\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 61\n",
      "\n",
      "LOSS: 4.4928089807170055\n",
      "\n",
      "Index is: 12\n",
      "\n",
      "EPOCH: 62\n",
      "\n",
      "LOSS: 5.533077698918968\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 63\n",
      "\n",
      "LOSS: 4.636999416615413\n",
      "\n",
      "Index is: 2\n",
      "\n",
      "EPOCH: 64\n",
      "\n",
      "LOSS: 9.642023096906641\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 65\n",
      "\n",
      "LOSS: 6.918525917865363\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 66\n",
      "\n",
      "LOSS: 6.910045465602033\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 67\n",
      "\n",
      "LOSS: 4.40742114461457\n",
      "\n",
      "Index is: 7\n",
      "\n",
      "EPOCH: 68\n",
      "\n",
      "LOSS: 8.308478566883869\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 69\n",
      "\n",
      "LOSS: 4.4757193899880665\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 70\n",
      "\n",
      "LOSS: 6.889163735111352\n",
      "\n",
      "Index is: 4\n",
      "\n",
      "EPOCH: 71\n",
      "\n",
      "LOSS: 4.6081439380998015\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 72\n",
      "\n",
      "LOSS: 4.459753739988751\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 73\n",
      "\n",
      "LOSS: 6.866713828814975\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 74\n",
      "\n",
      "LOSS: 6.8464870618684\n",
      "\n",
      "Index is: 7\n",
      "\n",
      "EPOCH: 75\n",
      "\n",
      "LOSS: 8.257424792412559\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 76\n",
      "\n",
      "LOSS: 4.3846195493737135\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 77\n",
      "\n",
      "LOSS: 4.360167794741251\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 78\n",
      "\n",
      "LOSS: 4.335769138974524\n",
      "\n",
      "Index is: 7\n",
      "\n",
      "EPOCH: 79\n",
      "\n",
      "LOSS: 8.200576395818443\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 80\n",
      "\n",
      "LOSS: 6.826417536911203\n",
      "\n",
      "Index is: 10\n",
      "\n",
      "EPOCH: 81\n",
      "\n",
      "LOSS: 7.1506258937572715\n",
      "\n",
      "Index is: 2\n",
      "\n",
      "EPOCH: 82\n",
      "\n",
      "LOSS: 9.61406627769761\n",
      "\n",
      "Index is: 12\n",
      "\n",
      "EPOCH: 83\n",
      "\n",
      "LOSS: 5.474863102233555\n",
      "\n",
      "Index is: 3\n",
      "\n",
      "EPOCH: 84\n",
      "\n",
      "LOSS: 6.80602056439519\n",
      "\n",
      "Index is: 9\n",
      "\n",
      "EPOCH: 85\n",
      "\n",
      "LOSS: 4.313967555375605\n",
      "\n",
      "Index is: 10\n",
      "\n",
      "EPOCH: 86\n",
      "\n",
      "LOSS: 7.111757210257078\n",
      "\n",
      "Index is: 11\n",
      "\n",
      "EPOCH: 87\n",
      "\n",
      "LOSS: 6.878937689679525\n",
      "\n",
      "Index is: 7\n",
      "\n",
      "EPOCH: 88\n",
      "\n",
      "LOSS: 8.150939571853002\n",
      "\n",
      "Index is: 7\n",
      "\n",
      "EPOCH: 89\n",
      "\n",
      "LOSS: 8.097975323925382\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 90\n",
      "\n",
      "LOSS: 4.4459794959710965\n",
      "\n",
      "Index is: 1\n",
      "\n",
      "EPOCH: 91\n",
      "\n",
      "LOSS: 6.495176338929126\n",
      "\n",
      "Index is: 12\n",
      "\n",
      "EPOCH: 92\n",
      "\n",
      "LOSS: 5.425030287078101\n",
      "\n",
      "Index is: 2\n",
      "\n",
      "EPOCH: 93\n",
      "\n",
      "LOSS: 9.579321837842794\n",
      "\n",
      "Index is: 2\n",
      "\n",
      "EPOCH: 94\n",
      "\n",
      "LOSS: 9.53479640364514\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 95\n",
      "\n",
      "LOSS: 4.60413945756599\n",
      "\n",
      "Index is: 1\n",
      "\n",
      "EPOCH: 96\n",
      "\n",
      "LOSS: 6.461830326183884\n",
      "\n",
      "Index is: 5\n",
      "\n",
      "EPOCH: 97\n",
      "\n",
      "LOSS: 4.4200344325875145\n",
      "\n",
      "Index is: 8\n",
      "\n",
      "EPOCH: 98\n",
      "\n",
      "LOSS: 4.580760422826703\n",
      "\n",
      "Index is: 4\n",
      "\n",
      "EPOCH: 99\n",
      "\n",
      "LOSS: 4.591865362310487\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0) \n",
    "\n",
    "# Get text data\n",
    "text = \"Welcome students to the Department of Computer Science. We have great faculty and professors. We will have a welcome program today.\"\n",
    "\n",
    "# Pre-process the data\n",
    "corpus = preprocessing(text)\n",
    "\n",
    "# Initialize Word2Vec model\n",
    "w2v = word2vec()\n",
    "\n",
    "# Generate training data\n",
    "training_data = w2v.generate_training_data(corpus)\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v.train(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The predicted context words are ['welcome', 'program', 'today', 'computer', 'department']\n"
     ]
    }
   ],
   "source": [
    "# Predict using model  \n",
    "print('\\nThe predicted context words are', w2v.predict(\"welcome\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
