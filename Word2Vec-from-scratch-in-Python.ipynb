{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x. This implementation provides better numerical stability.'''\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def softmax_1(x):\n",
    "    '''Function to compute the Softmax values for each sets of scores in x'''\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / np.sum(e_x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(self, word):\n",
    "    '''Function to covert a word to one-hot-encoded value'''\n",
    "    word_vec = [0 for i in range(0, self.v_count)]\n",
    "    word_index = self.word_index[word]\n",
    "    word_vec[word_index] = 1\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    '''Implementation of Skip-Gram Word2Vec model'''\n",
    "    def __init__(self):\n",
    "        self.N = 10\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        \n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.001\n",
    "        \n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "    \n",
    "    def initialize(self, V, data):\n",
    "        '''Function to initialze the neural network'''\n",
    "        self.V = V\n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "            \n",
    "    def feed_forward(self, X):\n",
    "        '''Function for feed-forward step'''\n",
    "        self.h = np.dot(self.W.T, X).reshape(self.N, 1) \n",
    "        self.u = np.dot(self.W1.T, self.h)\n",
    "        self.y = softmax(self.u)   \n",
    "        return self.y\n",
    "    \n",
    "    def backpropagate(self, x, t):\n",
    "        '''Function for back propagation using Stochastic Gradient Descent step'''\n",
    "        e = self.y - np.asarray(t).reshape(self.V, 1) #e.shape is V X 1\n",
    "        # Calculate partial derivative of loss function wrt W1\n",
    "        dEdW1 = np.dot(self.h, e.T)\n",
    "        \n",
    "        X = np.array(x).reshape(self.V, 1)\n",
    "        # Calculate partial derivative of loss function wrt W\n",
    "        dEdW = np.dot(X, np.dot(self.W1, e).T)\n",
    "        \n",
    "        # Update the weights\n",
    "        self.W1 = self.W1 - self.alpha * dEdW1\n",
    "        self.W = self.W - self.alpha * dEdW\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        '''Function to train the Word2Vec model'''\n",
    "        \n",
    "        # Generate Training data\n",
    "        generate_training_data(training_data,w2v)\n",
    "\n",
    "        # Loop through each epoch\n",
    "        for x in range(1,epochs):\n",
    "            \n",
    "            # Initialize Loss\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Loop through each training sample\n",
    "            for j in range(len(self.X_train)):\n",
    "                \n",
    "                # Forward Pass\n",
    "                self.feed_forward(self.X_train[j])\n",
    "                \n",
    "                # Backpropagation\n",
    "                self.backpropagate(self.X_train[j],self.y_train[j])\n",
    "                \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                        \n",
    "                # Calculate Loss        \n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u)))\n",
    "                \n",
    "            print(\"Epoch: \", x, \" Loss: \", self.loss)\n",
    "            self.alpha *= 1/((1+self.alpha*x))\n",
    "            \n",
    "    def predict(self, word, number_of_predictions):\n",
    "        '''Function to predict context words using Word2Vec model'''\n",
    "        # Check if word is contained in the dictionary\n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X)\n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i\n",
    "                \n",
    "            # Sort top context words in the output    \n",
    "            sorted_output = [] \n",
    "            for k in sorted(output, reverse=True): \n",
    "                sorted_output.append(self.words[output[k]]) \n",
    "                if(len(sorted_output)>=number_of_predictions): \n",
    "                    break\n",
    "            return sorted_output \n",
    "        else: \n",
    "            print(\"Error: Word not found in dicitonary\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    '''Function for data preprocessing'''\n",
    "    processed = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    # Split text corpus into sentences\n",
    "    sentences = corpus.split(\".\")\n",
    "    # Loop through each sentence\n",
    "    for i in range(len(sentences)):\n",
    "        # Remove leading and trailing characters\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        # Split sentence into list of words\n",
    "        sentence = sentences[i].split()\n",
    "        # Remove punctuations\n",
    "        x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "        # Convert to lower case\n",
    "        x = [word.lower() for word in x]\n",
    "        processed.append(x) \n",
    "    return processed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sentences, w2v):\n",
    "    '''Function to generate training data for Word2Vec'''\n",
    "    data = {}\n",
    "    \n",
    "    # Loop throuch each sentence\n",
    "    for sentence in sentences:\n",
    "        # Loop through each word\n",
    "        for word in sentence:\n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) # Size of Vocabulary\n",
    "    data = sorted(list(data.keys()))\n",
    "    \n",
    "    vocab = {}\n",
    "    # Store words into vocabulary\n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "    \n",
    "    print('\\nVocabulary:', vocab, '\\n')\n",
    "    \n",
    "    # Loop through each sentence \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)] \n",
    "            # Get the context words from the sliding window  \n",
    "            for j in range(i-w2v.window_size, i+w2v.window_size): \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "                    \n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context)\n",
    "            \n",
    "    w2v.initialize(V, data)\n",
    "    return w2v.X_train, w2v.y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary: {'computer': 0, 'department': 1, 'faculty': 2, 'great': 3, 'professors': 4, 'program': 5, 'science': 6, 'students': 7, 'today': 8, 'we': 9, 'welcome': 10} \n",
      "\n",
      "Epoch:  1  Loss:  72.69769323019779\n",
      "Epoch:  2  Loss:  72.5930390857227\n",
      "Epoch:  3  Loss:  72.48883010829633\n",
      "Epoch:  4  Loss:  72.38516577226034\n",
      "Epoch:  5  Loss:  72.28214303588062\n",
      "Epoch:  6  Loss:  72.17985580507238\n",
      "Epoch:  7  Loss:  72.07839443648926\n",
      "Epoch:  8  Loss:  71.9778452862734\n",
      "Epoch:  9  Loss:  71.87829030964802\n",
      "Epoch:  10  Loss:  71.77980671532326\n",
      "Epoch:  11  Loss:  71.68246667742335\n",
      "Epoch:  12  Loss:  71.58633710637427\n",
      "Epoch:  13  Loss:  71.4914794789621\n",
      "Epoch:  14  Loss:  71.39794972661537\n",
      "Epoch:  15  Loss:  71.30579817991847\n",
      "Epoch:  16  Loss:  71.2150695664476\n",
      "Epoch:  17  Loss:  71.12580305825394\n",
      "Epoch:  18  Loss:  71.0380323647145\n",
      "Epoch:  19  Loss:  70.95178586602292\n",
      "Epoch:  20  Loss:  70.8670867823065\n",
      "Epoch:  21  Loss:  70.78395337321435\n",
      "Epoch:  22  Loss:  70.702399162815\n",
      "Epoch:  23  Loss:  70.6224331847533\n",
      "Epoch:  24  Loss:  70.54406024282437\n",
      "Epoch:  25  Loss:  70.46728118241322\n",
      "Epoch:  26  Loss:  70.39209316859593\n",
      "Epoch:  27  Loss:  70.31848996709085\n",
      "Epoch:  28  Loss:  70.2464622246646\n",
      "Epoch:  29  Loss:  70.17599774602616\n",
      "Epoch:  30  Loss:  70.10708176466757\n",
      "Epoch:  31  Loss:  70.0396972055252\n",
      "Epoch:  32  Loss:  69.97382493772803\n",
      "Epoch:  33  Loss:  69.90944401606842\n",
      "Epoch:  34  Loss:  69.8465319101673\n",
      "Epoch:  35  Loss:  69.78506472060927\n",
      "Epoch:  36  Loss:  69.72501738159413\n",
      "Epoch:  37  Loss:  69.66636384988563\n",
      "Epoch:  38  Loss:  69.60907728004044\n",
      "Epoch:  39  Loss:  69.55313018607171\n",
      "Epoch:  40  Loss:  69.49849458984056\n",
      "Epoch:  41  Loss:  69.44514215658386\n",
      "Epoch:  42  Loss:  69.39304431807446\n",
      "Epoch:  43  Loss:  69.34217238397797\n",
      "Epoch:  44  Loss:  69.29249764201728\n",
      "Epoch:  45  Loss:  69.24399144758799\n",
      "Epoch:  46  Loss:  69.19662530348467\n",
      "Epoch:  47  Loss:  69.15037093040372\n",
      "Epoch:  48  Loss:  69.10520032888368\n",
      "Epoch:  49  Loss:  69.06108583333193\n",
      "Epoch:  50  Loss:  69.0180001587677\n",
      "Epoch:  51  Loss:  68.97591644088847\n",
      "Epoch:  52  Loss:  68.93480827003907\n",
      "Epoch:  53  Loss:  68.89464971963416\n",
      "Epoch:  54  Loss:  68.85541536955283\n",
      "Epoch:  55  Loss:  68.81708032499274\n",
      "Epoch:  56  Loss:  68.77962023123867\n",
      "Epoch:  57  Loss:  68.74301128476829\n",
      "Epoch:  58  Loss:  68.70723024108742\n",
      "Epoch:  59  Loss:  68.67225441965537\n",
      "Epoch:  60  Loss:  68.63806170623361\n",
      "Epoch:  61  Loss:  68.60463055296086\n",
      "Epoch:  62  Loss:  68.57193997643444\n",
      "Epoch:  63  Loss:  68.53996955404918\n",
      "Epoch:  64  Loss:  68.5086994188258\n",
      "Epoch:  65  Loss:  68.4781102529357\n",
      "Epoch:  66  Loss:  68.44818328011124\n",
      "Epoch:  67  Loss:  68.41890025711031\n",
      "Epoch:  68  Loss:  68.39024346438806\n",
      "Epoch:  69  Loss:  68.362195696112\n",
      "Epoch:  70  Loss:  68.33474024964224\n",
      "Epoch:  71  Loss:  68.30786091458614\n",
      "Epoch:  72  Loss:  68.28154196152276\n",
      "Epoch:  73  Loss:  68.25576813048409\n",
      "Epoch:  74  Loss:  68.23052461926721\n",
      "Epoch:  75  Loss:  68.20579707164472\n",
      "Epoch:  76  Loss:  68.18157156553113\n",
      "Epoch:  77  Loss:  68.15783460115621\n",
      "Epoch:  78  Loss:  68.13457308928949\n",
      "Epoch:  79  Loss:  68.11177433955383\n",
      "Epoch:  80  Loss:  68.08942604886124\n",
      "Epoch:  81  Loss:  68.06751628999825\n",
      "Epoch:  82  Loss:  68.04603350038522\n",
      "Epoch:  83  Loss:  68.02496647102879\n",
      "Epoch:  84  Loss:  68.00430433568437\n",
      "Epoch:  85  Loss:  67.9840365602415\n",
      "Epoch:  86  Loss:  67.96415293234341\n",
      "Epoch:  87  Loss:  67.94464355124845\n",
      "Epoch:  88  Loss:  67.92549881794015\n",
      "Epoch:  89  Loss:  67.90670942548999\n",
      "Epoch:  90  Loss:  67.8882663496757\n",
      "Epoch:  91  Loss:  67.87016083985671\n",
      "Epoch:  92  Loss:  67.85238441010645\n",
      "Epoch:  93  Loss:  67.83492883060131\n",
      "Epoch:  94  Loss:  67.81778611926353\n",
      "Epoch:  95  Loss:  67.80094853365647\n",
      "Epoch:  96  Loss:  67.78440856312801\n",
      "Epoch:  97  Loss:  67.76815892119932\n",
      "Epoch:  98  Loss:  67.75219253819363\n",
      "Epoch:  99  Loss:  67.73650255410122\n",
      "Epoch:  100  Loss:  67.72108231167554\n",
      "Epoch:  101  Loss:  67.70592534975445\n",
      "Epoch:  102  Loss:  67.69102539680243\n",
      "Epoch:  103  Loss:  67.67637636466729\n",
      "Epoch:  104  Loss:  67.6619723425458\n",
      "Epoch:  105  Loss:  67.6478075911533\n",
      "Epoch:  106  Loss:  67.63387653709012\n",
      "Epoch:  107  Loss:  67.62017376740087\n",
      "Epoch:  108  Loss:  67.60669402431908\n",
      "Epoch:  109  Loss:  67.59343220019284\n",
      "Epoch:  110  Loss:  67.58038333258509\n",
      "Epoch:  111  Loss:  67.56754259954315\n",
      "Epoch:  112  Loss:  67.55490531503249\n",
      "Epoch:  113  Loss:  67.5424669245283\n",
      "Epoch:  114  Loss:  67.53022300076104\n",
      "Epoch:  115  Loss:  67.51816923960943\n",
      "Epoch:  116  Loss:  67.50630145613694\n",
      "Epoch:  117  Loss:  67.49461558076618\n",
      "Epoch:  118  Loss:  67.4831076555867\n",
      "Epoch:  119  Loss:  67.4717738307918\n",
      "Epoch:  120  Loss:  67.460610361239\n",
      "Epoch:  121  Loss:  67.4496136031311\n",
      "Epoch:  122  Loss:  67.43878001081212\n",
      "Epoch:  123  Loss:  67.4281061336754\n",
      "Epoch:  124  Loss:  67.41758861317847\n",
      "Epoch:  125  Loss:  67.40722417996223\n",
      "Epoch:  126  Loss:  67.39700965106978\n",
      "Epoch:  127  Loss:  67.38694192726122\n",
      "Epoch:  128  Loss:  67.37701799042188\n",
      "Epoch:  129  Loss:  67.36723490105952\n",
      "Epoch:  130  Loss:  67.35758979588789\n",
      "Epoch:  131  Loss:  67.34807988549328\n",
      "Epoch:  132  Loss:  67.33870245208126\n",
      "Epoch:  133  Loss:  67.32945484730031\n",
      "Epoch:  134  Loss:  67.32033449013996\n",
      "Epoch:  135  Loss:  67.31133886490028\n",
      "Epoch:  136  Loss:  67.30246551923051\n",
      "Epoch:  137  Loss:  67.29371206223429\n",
      "Epoch:  138  Loss:  67.28507616263843\n",
      "Epoch:  139  Loss:  67.27655554702392\n",
      "Epoch:  140  Loss:  67.26814799811604\n",
      "Epoch:  141  Loss:  67.259851353132\n",
      "Epoch:  142  Loss:  67.25166350218352\n",
      "Epoch:  143  Loss:  67.24358238673275\n",
      "Epoch:  144  Loss:  67.23560599809956\n",
      "Epoch:  145  Loss:  67.227732376018\n",
      "Epoch:  146  Loss:  67.21995960724064\n",
      "Epoch:  147  Loss:  67.21228582418851\n",
      "Epoch:  148  Loss:  67.20470920364562\n",
      "Epoch:  149  Loss:  67.19722796549593\n",
      "Epoch:  150  Loss:  67.1898403715014\n",
      "Epoch:  151  Loss:  67.18254472411992\n",
      "Epoch:  152  Loss:  67.17533936536125\n",
      "Epoch:  153  Loss:  67.16822267567993\n",
      "Epoch:  154  Loss:  67.16119307290373\n",
      "Epoch:  155  Loss:  67.15424901119644\n",
      "Epoch:  156  Loss:  67.14738898005358\n",
      "Epoch:  157  Loss:  67.14061150333025\n",
      "Epoch:  158  Loss:  67.13391513829959\n",
      "Epoch:  159  Loss:  67.12729847474104\n",
      "Epoch:  160  Loss:  67.12076013405708\n",
      "Epoch:  161  Loss:  67.11429876841808\n",
      "Epoch:  162  Loss:  67.10791305993314\n",
      "Epoch:  163  Loss:  67.10160171984742\n",
      "Epoch:  164  Loss:  67.09536348776386\n",
      "Epoch:  165  Loss:  67.0891971308891\n",
      "Epoch:  166  Loss:  67.08310144330258\n",
      "Epoch:  167  Loss:  67.07707524524795\n",
      "Epoch:  168  Loss:  67.0711173824462\n",
      "Epoch:  169  Loss:  67.06522672542965\n",
      "Epoch:  170  Loss:  67.05940216889604\n",
      "Epoch:  171  Loss:  67.05364263108224\n",
      "Epoch:  172  Loss:  67.04794705315658\n",
      "Epoch:  173  Loss:  67.04231439862966\n",
      "Epoch:  174  Loss:  67.03674365278239\n",
      "Epoch:  175  Loss:  67.03123382211146\n",
      "Epoch:  176  Loss:  67.02578393379073\n",
      "Epoch:  177  Loss:  67.020393035149\n",
      "Epoch:  178  Loss:  67.01506019316265\n",
      "Epoch:  179  Loss:  67.00978449396354\n",
      "Epoch:  180  Loss:  67.00456504236098\n",
      "Epoch:  181  Loss:  66.99940096137776\n",
      "Epoch:  182  Loss:  66.99429139179946\n",
      "Epoch:  183  Loss:  66.98923549173679\n",
      "Epoch:  184  Loss:  66.98423243620066\n",
      "Epoch:  185  Loss:  66.97928141668908\n",
      "Epoch:  186  Loss:  66.97438164078615\n",
      "Epoch:  187  Loss:  66.96953233177234\n",
      "Epoch:  188  Loss:  66.96473272824556\n",
      "Epoch:  189  Loss:  66.95998208375327\n",
      "Epoch:  190  Loss:  66.95527966643466\n",
      "Epoch:  191  Loss:  66.95062475867292\n",
      "Epoch:  192  Loss:  66.9460166567571\n",
      "Epoch:  193  Loss:  66.94145467055363\n",
      "Epoch:  194  Loss:  66.9369381231865\n",
      "Epoch:  195  Loss:  66.93246635072663\n",
      "Epoch:  196  Loss:  66.92803870188945\n",
      "Epoch:  197  Loss:  66.92365453774094\n",
      "Epoch:  198  Loss:  66.91931323141169\n",
      "Epoch:  199  Loss:  66.9150141678185\n",
      "Epoch:  200  Loss:  66.91075674339375\n",
      "Epoch:  201  Loss:  66.9065403658218\n",
      "Epoch:  202  Loss:  66.90236445378282\n",
      "Epoch:  203  Loss:  66.89822843670291\n",
      "Epoch:  204  Loss:  66.89413175451158\n",
      "Epoch:  205  Loss:  66.89007385740491\n",
      "Epoch:  206  Loss:  66.88605420561551\n",
      "Epoch:  207  Loss:  66.88207226918837\n",
      "Epoch:  208  Loss:  66.87812752776249\n",
      "Epoch:  209  Loss:  66.87421947035823\n",
      "Epoch:  210  Loss:  66.8703475951704\n",
      "Epoch:  211  Loss:  66.86651140936642\n",
      "Epoch:  212  Loss:  66.86271042888988\n",
      "Epoch:  213  Loss:  66.85894417826913\n",
      "Epoch:  214  Loss:  66.85521219043058\n",
      "Epoch:  215  Loss:  66.85151400651723\n",
      "Epoch:  216  Loss:  66.84784917571122\n",
      "Epoch:  217  Loss:  66.84421725506138\n",
      "Epoch:  218  Loss:  66.84061780931481\n",
      "Epoch:  219  Loss:  66.8370504107529\n",
      "Epoch:  220  Loss:  66.83351463903128\n",
      "Epoch:  221  Loss:  66.830010081024\n",
      "Epoch:  222  Loss:  66.82653633067135\n",
      "Epoch:  223  Loss:  66.82309298883156\n",
      "Epoch:  224  Loss:  66.81967966313624\n",
      "Epoch:  225  Loss:  66.81629596784921\n",
      "Epoch:  226  Loss:  66.81294152372891\n",
      "Epoch:  227  Loss:  66.8096159578942\n",
      "Epoch:  228  Loss:  66.80631890369322\n",
      "Epoch:  229  Loss:  66.80305000057585\n",
      "Epoch:  230  Loss:  66.79980889396865\n",
      "Epoch:  231  Loss:  66.79659523515335\n",
      "Epoch:  232  Loss:  66.79340868114808\n",
      "Epoch:  233  Loss:  66.79024889459134\n",
      "Epoch:  234  Loss:  66.78711554362883\n",
      "Epoch:  235  Loss:  66.78400830180286\n",
      "Epoch:  236  Loss:  66.78092684794466\n",
      "Epoch:  237  Loss:  66.77787086606881\n",
      "Epoch:  238  Loss:  66.77484004527058\n",
      "Epoch:  239  Loss:  66.77183407962534\n",
      "Epoch:  240  Loss:  66.7688526680904\n",
      "Epoch:  241  Loss:  66.7658955144094\n",
      "Epoch:  242  Loss:  66.76296232701843\n",
      "Epoch:  243  Loss:  66.76005281895469\n",
      "Epoch:  244  Loss:  66.75716670776727\n",
      "Epoch:  245  Loss:  66.7543037154296\n",
      "Epoch:  246  Loss:  66.75146356825428\n",
      "Epoch:  247  Loss:  66.74864599680976\n",
      "Epoch:  248  Loss:  66.74585073583879\n",
      "Epoch:  249  Loss:  66.74307752417884\n",
      "Epoch:  250  Loss:  66.74032610468421\n",
      "Epoch:  251  Loss:  66.73759622415002\n",
      "Epoch:  252  Loss:  66.73488763323776\n",
      "Epoch:  253  Loss:  66.73220008640257\n",
      "Epoch:  254  Loss:  66.72953334182219\n",
      "Epoch:  255  Loss:  66.7268871613274\n",
      "Epoch:  256  Loss:  66.72426131033399\n",
      "Epoch:  257  Loss:  66.7216555577763\n",
      "Epoch:  258  Loss:  66.71906967604221\n",
      "Epoch:  259  Loss:  66.71650344090945\n",
      "Epoch:  260  Loss:  66.71395663148341\n",
      "Epoch:  261  Loss:  66.71142903013624\n",
      "Epoch:  262  Loss:  66.70892042244729\n",
      "Epoch:  263  Loss:  66.7064305971449\n",
      "Epoch:  264  Loss:  66.70395934604922\n",
      "Epoch:  265  Loss:  66.7015064640166\n",
      "Epoch:  266  Loss:  66.69907174888479\n",
      "Epoch:  267  Loss:  66.69665500141969\n",
      "Epoch:  268  Loss:  66.69425602526282\n",
      "Epoch:  269  Loss:  66.69187462688029\n",
      "Epoch:  270  Loss:  66.6895106155126\n",
      "Epoch:  271  Loss:  66.68716380312557\n",
      "Epoch:  272  Loss:  66.68483400436222\n",
      "Epoch:  273  Loss:  66.68252103649594\n",
      "Epoch:  274  Loss:  66.68022471938417\n",
      "Epoch:  275  Loss:  66.67794487542348\n",
      "Epoch:  276  Loss:  66.67568132950527\n",
      "Epoch:  277  Loss:  66.67343390897257\n",
      "Epoch:  278  Loss:  66.67120244357766\n",
      "Epoch:  279  Loss:  66.6689867654406\n",
      "Epoch:  280  Loss:  66.6667867090084\n",
      "Epoch:  281  Loss:  66.66460211101545\n",
      "Epoch:  282  Loss:  66.66243281044427\n",
      "Epoch:  283  Loss:  66.66027864848732\n",
      "Epoch:  284  Loss:  66.65813946850969\n",
      "Epoch:  285  Loss:  66.65601511601216\n",
      "Epoch:  286  Loss:  66.6539054385954\n",
      "Epoch:  287  Loss:  66.65181028592463\n",
      "Epoch:  288  Loss:  66.64972950969502\n",
      "Epoch:  289  Loss:  66.64766296359797\n",
      "Epoch:  290  Loss:  66.64561050328778\n",
      "Epoch:  291  Loss:  66.6435719863491\n",
      "Epoch:  292  Loss:  66.64154727226514\n",
      "Epoch:  293  Loss:  66.63953622238628\n",
      "Epoch:  294  Loss:  66.63753869989944\n",
      "Epoch:  295  Loss:  66.63555456979803\n",
      "Epoch:  296  Loss:  66.63358369885239\n",
      "Epoch:  297  Loss:  66.63162595558103\n",
      "Epoch:  298  Loss:  66.62968121022203\n",
      "Epoch:  299  Loss:  66.6277493347055\n",
      "Epoch:  300  Loss:  66.62583020262612\n",
      "Epoch:  301  Loss:  66.6239236892164\n",
      "Epoch:  302  Loss:  66.62202967132053\n",
      "Epoch:  303  Loss:  66.62014802736856\n",
      "Epoch:  304  Loss:  66.61827863735114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  305  Loss:  66.61642138279477\n",
      "Epoch:  306  Loss:  66.61457614673746\n",
      "Epoch:  307  Loss:  66.61274281370493\n",
      "Epoch:  308  Loss:  66.61092126968713\n",
      "Epoch:  309  Loss:  66.60911140211529\n",
      "Epoch:  310  Loss:  66.60731309983943\n",
      "Epoch:  311  Loss:  66.60552625310612\n",
      "Epoch:  312  Loss:  66.60375075353689\n",
      "Epoch:  313  Loss:  66.60198649410688\n",
      "Epoch:  314  Loss:  66.60023336912384\n",
      "Epoch:  315  Loss:  66.59849127420772\n",
      "Epoch:  316  Loss:  66.59676010627035\n",
      "Epoch:  317  Loss:  66.59503976349585\n",
      "Epoch:  318  Loss:  66.593330145321\n",
      "Epoch:  319  Loss:  66.59163115241626\n",
      "Epoch:  320  Loss:  66.58994268666707\n",
      "Epoch:  321  Loss:  66.58826465115538\n",
      "Epoch:  322  Loss:  66.58659695014155\n",
      "Epoch:  323  Loss:  66.58493948904675\n",
      "Epoch:  324  Loss:  66.5832921744354\n",
      "Epoch:  325  Loss:  66.58165491399814\n",
      "Epoch:  326  Loss:  66.58002761653498\n",
      "Epoch:  327  Loss:  66.57841019193874\n",
      "Epoch:  328  Loss:  66.57680255117896\n",
      "Epoch:  329  Loss:  66.5752046062859\n",
      "Epoch:  330  Loss:  66.57361627033487\n",
      "Epoch:  331  Loss:  66.5720374574308\n",
      "Epoch:  332  Loss:  66.57046808269332\n",
      "Epoch:  333  Loss:  66.5689080622417\n",
      "Epoch:  334  Loss:  66.5673573131804\n",
      "Epoch:  335  Loss:  66.5658157535847\n",
      "Epoch:  336  Loss:  66.56428330248653\n",
      "Epoch:  337  Loss:  66.56275987986086\n",
      "Epoch:  338  Loss:  66.56124540661173\n",
      "Epoch:  339  Loss:  66.55973980455933\n",
      "Epoch:  340  Loss:  66.55824299642639\n",
      "Epoch:  341  Loss:  66.55675490582566\n",
      "Epoch:  342  Loss:  66.55527545724694\n",
      "Epoch:  343  Loss:  66.55380457604474\n",
      "Epoch:  344  Loss:  66.55234218842594\n",
      "Epoch:  345  Loss:  66.55088822143786\n",
      "Epoch:  346  Loss:  66.54944260295612\n",
      "Epoch:  347  Loss:  66.54800526167328\n",
      "Epoch:  348  Loss:  66.54657612708726\n",
      "Epoch:  349  Loss:  66.54515512948993\n",
      "Epoch:  350  Loss:  66.5437421999562\n",
      "Epoch:  351  Loss:  66.54233727033316\n",
      "Epoch:  352  Loss:  66.54094027322913\n",
      "Epoch:  353  Loss:  66.53955114200326\n",
      "Epoch:  354  Loss:  66.53816981075524\n",
      "Epoch:  355  Loss:  66.53679621431486\n",
      "Epoch:  356  Loss:  66.53543028823228\n",
      "Epoch:  357  Loss:  66.53407196876788\n",
      "Epoch:  358  Loss:  66.53272119288265\n",
      "Epoch:  359  Loss:  66.53137789822881\n",
      "Epoch:  360  Loss:  66.53004202314014\n",
      "Epoch:  361  Loss:  66.5287135066229\n",
      "Epoch:  362  Loss:  66.52739228834668\n",
      "Epoch:  363  Loss:  66.52607830863553\n",
      "Epoch:  364  Loss:  66.52477150845903\n",
      "Epoch:  365  Loss:  66.52347182942371\n",
      "Epoch:  366  Loss:  66.52217921376456\n",
      "Epoch:  367  Loss:  66.52089360433655\n",
      "Epoch:  368  Loss:  66.51961494460637\n",
      "Epoch:  369  Loss:  66.51834317864447\n",
      "Epoch:  370  Loss:  66.5170782511168\n",
      "Epoch:  371  Loss:  66.51582010727722\n",
      "Epoch:  372  Loss:  66.51456869295953\n",
      "Epoch:  373  Loss:  66.51332395457\n",
      "Epoch:  374  Loss:  66.51208583907973\n",
      "Epoch:  375  Loss:  66.51085429401739\n",
      "Epoch:  376  Loss:  66.50962926746188\n",
      "Epoch:  377  Loss:  66.5084107080351\n",
      "Epoch:  378  Loss:  66.50719856489513\n",
      "Epoch:  379  Loss:  66.50599278772904\n",
      "Epoch:  380  Loss:  66.50479332674615\n",
      "Epoch:  381  Loss:  66.50360013267135\n",
      "Epoch:  382  Loss:  66.50241315673844\n",
      "Epoch:  383  Loss:  66.50123235068358\n",
      "Epoch:  384  Loss:  66.50005766673885\n",
      "Epoch:  385  Loss:  66.49888905762609\n",
      "Epoch:  386  Loss:  66.49772647655043\n",
      "Epoch:  387  Loss:  66.4965698771943\n",
      "Epoch:  388  Loss:  66.49541921371137\n",
      "Epoch:  389  Loss:  66.49427444072056\n",
      "Epoch:  390  Loss:  66.49313551330025\n",
      "Epoch:  391  Loss:  66.49200238698243\n",
      "Epoch:  392  Loss:  66.49087501774704\n",
      "Epoch:  393  Loss:  66.48975336201637\n",
      "Epoch:  394  Loss:  66.48863737664959\n",
      "Epoch:  395  Loss:  66.48752701893723\n",
      "Epoch:  396  Loss:  66.4864222465958\n",
      "Epoch:  397  Loss:  66.48532301776271\n",
      "Epoch:  398  Loss:  66.48422929099085\n",
      "Epoch:  399  Loss:  66.48314102524353\n",
      "Epoch:  400  Loss:  66.48205817988953\n",
      "Epoch:  401  Loss:  66.48098071469799\n",
      "Epoch:  402  Loss:  66.47990858983361\n",
      "Epoch:  403  Loss:  66.47884176585183\n",
      "Epoch:  404  Loss:  66.47778020369384\n",
      "Epoch:  405  Loss:  66.47672386468231\n",
      "Epoch:  406  Loss:  66.4756727105164\n",
      "Epoch:  407  Loss:  66.4746267032674\n",
      "Epoch:  408  Loss:  66.47358580537413\n",
      "Epoch:  409  Loss:  66.47254997963859\n",
      "Epoch:  410  Loss:  66.47151918922155\n",
      "Epoch:  411  Loss:  66.47049339763838\n",
      "Epoch:  412  Loss:  66.46947256875453\n",
      "Epoch:  413  Loss:  66.4684566667817\n",
      "Epoch:  414  Loss:  66.4674456562735\n",
      "Epoch:  415  Loss:  66.46643950212143\n",
      "Epoch:  416  Loss:  66.4654381695509\n",
      "Epoch:  417  Loss:  66.46444162411728\n",
      "Epoch:  418  Loss:  66.46344983170206\n",
      "Epoch:  419  Loss:  66.46246275850889\n",
      "Epoch:  420  Loss:  66.46148037105993\n",
      "Epoch:  421  Loss:  66.460502636192\n",
      "Epoch:  422  Loss:  66.45952952105313\n",
      "Epoch:  423  Loss:  66.45856099309856\n",
      "Epoch:  424  Loss:  66.45759702008755\n",
      "Epoch:  425  Loss:  66.4566375700796\n",
      "Epoch:  426  Loss:  66.4556826114311\n",
      "Epoch:  427  Loss:  66.45473211279189\n",
      "Epoch:  428  Loss:  66.45378604310179\n",
      "Epoch:  429  Loss:  66.45284437158736\n",
      "Epoch:  430  Loss:  66.45190706775857\n",
      "Epoch:  431  Loss:  66.45097410140556\n",
      "Epoch:  432  Loss:  66.45004544259551\n",
      "Epoch:  433  Loss:  66.44912106166933\n",
      "Epoch:  434  Loss:  66.4482009292387\n",
      "Epoch:  435  Loss:  66.44728501618292\n",
      "Epoch:  436  Loss:  66.44637329364586\n",
      "Epoch:  437  Loss:  66.44546573303309\n",
      "Epoch:  438  Loss:  66.4445623060088\n",
      "Epoch:  439  Loss:  66.44366298449292\n",
      "Epoch:  440  Loss:  66.44276774065835\n",
      "Epoch:  441  Loss:  66.441876546928\n",
      "Epoch:  442  Loss:  66.44098937597205\n",
      "Epoch:  443  Loss:  66.44010620070522\n",
      "Epoch:  444  Loss:  66.439226994284\n",
      "Epoch:  445  Loss:  66.43835173010402\n",
      "Epoch:  446  Loss:  66.43748038179739\n",
      "Epoch:  447  Loss:  66.43661292322994\n",
      "Epoch:  448  Loss:  66.43574932849897\n",
      "Epoch:  449  Loss:  66.43488957193041\n",
      "Epoch:  450  Loss:  66.43403362807638\n",
      "Epoch:  451  Loss:  66.43318147171279\n",
      "Epoch:  452  Loss:  66.43233307783683\n",
      "Epoch:  453  Loss:  66.43148842166458\n",
      "Epoch:  454  Loss:  66.43064747862864\n",
      "Epoch:  455  Loss:  66.42981022437569\n",
      "Epoch:  456  Loss:  66.4289766347643\n",
      "Epoch:  457  Loss:  66.42814668586254\n",
      "Epoch:  458  Loss:  66.4273203539458\n",
      "Epoch:  459  Loss:  66.42649761549448\n",
      "Epoch:  460  Loss:  66.4256784471918\n",
      "Epoch:  461  Loss:  66.42486282592169\n",
      "Epoch:  462  Loss:  66.42405072876657\n",
      "Epoch:  463  Loss:  66.4232421330053\n",
      "Epoch:  464  Loss:  66.42243701611096\n",
      "Epoch:  465  Loss:  66.42163535574895\n",
      "Epoch:  466  Loss:  66.42083712977481\n",
      "Epoch:  467  Loss:  66.42004231623233\n",
      "Epoch:  468  Loss:  66.41925089335143\n",
      "Epoch:  469  Loss:  66.4184628395463\n",
      "Epoch:  470  Loss:  66.4176781334134\n",
      "Epoch:  471  Loss:  66.4168967537296\n",
      "Epoch:  472  Loss:  66.41611867945024\n",
      "Epoch:  473  Loss:  66.41534388970727\n",
      "Epoch:  474  Loss:  66.4145723638074\n",
      "Epoch:  475  Loss:  66.41380408123038\n",
      "Epoch:  476  Loss:  66.41303902162701\n",
      "Epoch:  477  Loss:  66.41227716481764\n",
      "Epoch:  478  Loss:  66.41151849078994\n",
      "Epoch:  479  Loss:  66.41076297969788\n",
      "Epoch:  480  Loss:  66.4100106118593\n",
      "Epoch:  481  Loss:  66.40926136775468\n",
      "Epoch:  482  Loss:  66.40851522802532\n",
      "Epoch:  483  Loss:  66.40777217347163\n",
      "Epoch:  484  Loss:  66.40703218505165\n",
      "Epoch:  485  Loss:  66.40629524387937\n",
      "Epoch:  486  Loss:  66.40556133122305\n",
      "Epoch:  487  Loss:  66.40483042850386\n",
      "Epoch:  488  Loss:  66.40410251729406\n",
      "Epoch:  489  Loss:  66.40337757931576\n",
      "Epoch:  490  Loss:  66.40265559643913\n",
      "Epoch:  491  Loss:  66.40193655068114\n",
      "Epoch:  492  Loss:  66.40122042420396\n",
      "Epoch:  493  Loss:  66.40050719931345\n",
      "Epoch:  494  Loss:  66.39979685845788\n",
      "Epoch:  495  Loss:  66.39908938422633\n",
      "Epoch:  496  Loss:  66.39838475934751\n",
      "Epoch:  497  Loss:  66.39768296668807\n",
      "Epoch:  498  Loss:  66.39698398925142\n",
      "Epoch:  499  Loss:  66.39628781017646\n",
      "Epoch:  500  Loss:  66.39559441273593\n",
      "Epoch:  501  Loss:  66.39490378033541\n",
      "Epoch:  502  Loss:  66.39421589651182\n",
      "Epoch:  503  Loss:  66.39353074493219\n",
      "Epoch:  504  Loss:  66.39284830939235\n",
      "Epoch:  505  Loss:  66.39216857381565\n",
      "Epoch:  506  Loss:  66.39149152225183\n",
      "Epoch:  507  Loss:  66.39081713887565\n",
      "Epoch:  508  Loss:  66.39014540798566\n",
      "Epoch:  509  Loss:  66.38947631400319\n",
      "Epoch:  510  Loss:  66.38880984147087\n",
      "Epoch:  511  Loss:  66.38814597505174\n",
      "Epoch:  512  Loss:  66.38748469952782\n",
      "Epoch:  513  Loss:  66.38682599979919\n",
      "Epoch:  514  Loss:  66.38616986088267\n",
      "Epoch:  515  Loss:  66.38551626791082\n",
      "Epoch:  516  Loss:  66.3848652061308\n",
      "Epoch:  517  Loss:  66.38421666090316\n",
      "Epoch:  518  Loss:  66.38357061770097\n",
      "Epoch:  519  Loss:  66.38292706210855\n",
      "Epoch:  520  Loss:  66.38228597982045\n",
      "Epoch:  521  Loss:  66.38164735664051\n",
      "Epoch:  522  Loss:  66.38101117848072\n",
      "Epoch:  523  Loss:  66.38037743136019\n",
      "Epoch:  524  Loss:  66.37974610140418\n",
      "Epoch:  525  Loss:  66.37911717484309\n",
      "Epoch:  526  Loss:  66.37849063801143\n",
      "Epoch:  527  Loss:  66.37786647734688\n",
      "Epoch:  528  Loss:  66.37724467938939\n",
      "Epoch:  529  Loss:  66.37662523077996\n",
      "Epoch:  530  Loss:  66.37600811826009\n",
      "Epoch:  531  Loss:  66.37539332867044\n",
      "Epoch:  532  Loss:  66.37478084895022\n",
      "Epoch:  533  Loss:  66.3741706661361\n",
      "Epoch:  534  Loss:  66.37356276736128\n",
      "Epoch:  535  Loss:  66.37295713985479\n",
      "Epoch:  536  Loss:  66.37235377094035\n",
      "Epoch:  537  Loss:  66.37175264803572\n",
      "Epoch:  538  Loss:  66.3711537586517\n",
      "Epoch:  539  Loss:  66.37055709039126\n",
      "Epoch:  540  Loss:  66.36996263094876\n",
      "Epoch:  541  Loss:  66.36937036810922\n",
      "Epoch:  542  Loss:  66.36878028974711\n",
      "Epoch:  543  Loss:  66.36819238382597\n",
      "Epoch:  544  Loss:  66.36760663839736\n",
      "Epoch:  545  Loss:  66.36702304160009\n",
      "Epoch:  546  Loss:  66.36644158165947\n",
      "Epoch:  547  Loss:  66.36586224688651\n",
      "Epoch:  548  Loss:  66.36528502567705\n",
      "Epoch:  549  Loss:  66.36470990651114\n",
      "Epoch:  550  Loss:  66.36413687795226\n",
      "Epoch:  551  Loss:  66.36356592864644\n",
      "Epoch:  552  Loss:  66.36299704732164\n",
      "Epoch:  553  Loss:  66.36243022278695\n",
      "Epoch:  554  Loss:  66.36186544393195\n",
      "Epoch:  555  Loss:  66.36130269972584\n",
      "Epoch:  556  Loss:  66.36074197921684\n",
      "Epoch:  557  Loss:  66.36018327153154\n",
      "Epoch:  558  Loss:  66.35962656587401\n",
      "Epoch:  559  Loss:  66.35907185152531\n",
      "Epoch:  560  Loss:  66.35851911784269\n",
      "Epoch:  561  Loss:  66.35796835425893\n",
      "Epoch:  562  Loss:  66.35741955028176\n",
      "Epoch:  563  Loss:  66.35687269549298\n",
      "Epoch:  564  Loss:  66.35632777954818\n",
      "Epoch:  565  Loss:  66.35578479217567\n",
      "Epoch:  566  Loss:  66.35524372317609\n",
      "Epoch:  567  Loss:  66.35470456242183\n",
      "Epoch:  568  Loss:  66.35416729985616\n",
      "Epoch:  569  Loss:  66.35363192549283\n",
      "Epoch:  570  Loss:  66.35309842941537\n",
      "Epoch:  571  Loss:  66.3525668017764\n",
      "Epoch:  572  Loss:  66.35203703279727\n",
      "Epoch:  573  Loss:  66.35150911276715\n",
      "Epoch:  574  Loss:  66.35098303204273\n",
      "Epoch:  575  Loss:  66.35045878104748\n",
      "Epoch:  576  Loss:  66.34993635027102\n",
      "Epoch:  577  Loss:  66.34941573026879\n",
      "Epoch:  578  Loss:  66.34889691166117\n",
      "Epoch:  579  Loss:  66.34837988513317\n",
      "Epoch:  580  Loss:  66.3478646414338\n",
      "Epoch:  581  Loss:  66.34735117137542\n",
      "Epoch:  582  Loss:  66.34683946583343\n",
      "Epoch:  583  Loss:  66.34632951574542\n",
      "Epoch:  584  Loss:  66.34582131211098\n",
      "Epoch:  585  Loss:  66.34531484599088\n",
      "Epoch:  586  Loss:  66.34481010850672\n",
      "Epoch:  587  Loss:  66.34430709084042\n",
      "Epoch:  588  Loss:  66.34380578423358\n",
      "Epoch:  589  Loss:  66.34330617998708\n",
      "Epoch:  590  Loss:  66.34280826946059\n",
      "Epoch:  591  Loss:  66.34231204407202\n",
      "Epoch:  592  Loss:  66.34181749529705\n",
      "Epoch:  593  Loss:  66.34132461466868\n",
      "Epoch:  594  Loss:  66.34083339377669\n",
      "Epoch:  595  Loss:  66.34034382426715\n",
      "Epoch:  596  Loss:  66.33985589784211\n",
      "Epoch:  597  Loss:  66.33936960625893\n",
      "Epoch:  598  Loss:  66.33888494132994\n",
      "Epoch:  599  Loss:  66.33840189492194\n",
      "Epoch:  600  Loss:  66.3379204589558\n",
      "Epoch:  601  Loss:  66.33744062540592\n",
      "Epoch:  602  Loss:  66.33696238629982\n",
      "Epoch:  603  Loss:  66.33648573371782\n",
      "Epoch:  604  Loss:  66.33601065979248\n",
      "Epoch:  605  Loss:  66.33553715670813\n",
      "Epoch:  606  Loss:  66.33506521670057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  607  Loss:  66.33459483205661\n",
      "Epoch:  608  Loss:  66.33412599511357\n",
      "Epoch:  609  Loss:  66.33365869825899\n",
      "Epoch:  610  Loss:  66.33319293393018\n",
      "Epoch:  611  Loss:  66.3327286946137\n",
      "Epoch:  612  Loss:  66.33226597284518\n",
      "Epoch:  613  Loss:  66.33180476120864\n",
      "Epoch:  614  Loss:  66.3313450523365\n",
      "Epoch:  615  Loss:  66.33088683890868\n",
      "Epoch:  616  Loss:  66.33043011365268\n",
      "Epoch:  617  Loss:  66.3299748693429\n",
      "Epoch:  618  Loss:  66.32952109880038\n",
      "Epoch:  619  Loss:  66.3290687948924\n",
      "Epoch:  620  Loss:  66.32861795053212\n",
      "Epoch:  621  Loss:  66.32816855867821\n",
      "Epoch:  622  Loss:  66.32772061233453\n",
      "Epoch:  623  Loss:  66.32727410454964\n",
      "Epoch:  624  Loss:  66.32682902841657\n",
      "Epoch:  625  Loss:  66.3263853770724\n",
      "Epoch:  626  Loss:  66.32594314369796\n",
      "Epoch:  627  Loss:  66.3255023215174\n",
      "Epoch:  628  Loss:  66.32506290379801\n",
      "Epoch:  629  Loss:  66.32462488384967\n",
      "Epoch:  630  Loss:  66.32418825502461\n",
      "Epoch:  631  Loss:  66.32375301071713\n",
      "Epoch:  632  Loss:  66.32331914436323\n",
      "Epoch:  633  Loss:  66.3228866494402\n",
      "Epoch:  634  Loss:  66.32245551946642\n",
      "Epoch:  635  Loss:  66.32202574800097\n",
      "Epoch:  636  Loss:  66.32159732864328\n",
      "Epoch:  637  Loss:  66.32117025503297\n",
      "Epoch:  638  Loss:  66.32074452084929\n",
      "Epoch:  639  Loss:  66.32032011981109\n",
      "Epoch:  640  Loss:  66.31989704567621\n",
      "Epoch:  641  Loss:  66.31947529224152\n",
      "Epoch:  642  Loss:  66.31905485334227\n",
      "Epoch:  643  Loss:  66.318635722852\n",
      "Epoch:  644  Loss:  66.31821789468228\n",
      "Epoch:  645  Loss:  66.31780136278223\n",
      "Epoch:  646  Loss:  66.31738612113844\n",
      "Epoch:  647  Loss:  66.31697216377447\n",
      "Epoch:  648  Loss:  66.31655948475074\n",
      "Epoch:  649  Loss:  66.3161480781642\n",
      "Epoch:  650  Loss:  66.31573793814802\n",
      "Epoch:  651  Loss:  66.31532905887128\n",
      "Epoch:  652  Loss:  66.31492143453879\n",
      "Epoch:  653  Loss:  66.31451505939071\n",
      "Epoch:  654  Loss:  66.31410992770243\n",
      "Epoch:  655  Loss:  66.31370603378416\n",
      "Epoch:  656  Loss:  66.31330337198071\n",
      "Epoch:  657  Loss:  66.31290193667125\n",
      "Epoch:  658  Loss:  66.31250172226903\n",
      "Epoch:  659  Loss:  66.31210272322109\n",
      "Epoch:  660  Loss:  66.31170493400818\n",
      "Epoch:  661  Loss:  66.31130834914418\n",
      "Epoch:  662  Loss:  66.3109129631762\n",
      "Epoch:  663  Loss:  66.31051877068401\n",
      "Epoch:  664  Loss:  66.31012576628012\n",
      "Epoch:  665  Loss:  66.30973394460929\n",
      "Epoch:  666  Loss:  66.30934330034833\n",
      "Epoch:  667  Loss:  66.30895382820597\n",
      "Epoch:  668  Loss:  66.30856552292256\n",
      "Epoch:  669  Loss:  66.30817837926973\n",
      "Epoch:  670  Loss:  66.30779239205036\n",
      "Epoch:  671  Loss:  66.30740755609816\n",
      "Epoch:  672  Loss:  66.30702386627755\n",
      "Epoch:  673  Loss:  66.30664131748343\n",
      "Epoch:  674  Loss:  66.30625990464092\n",
      "Epoch:  675  Loss:  66.30587962270513\n",
      "Epoch:  676  Loss:  66.30550046666099\n",
      "Epoch:  677  Loss:  66.30512243152289\n",
      "Epoch:  678  Loss:  66.3047455123347\n",
      "Epoch:  679  Loss:  66.30436970416946\n",
      "Epoch:  680  Loss:  66.30399500212891\n",
      "Epoch:  681  Loss:  66.30362140134368\n",
      "Epoch:  682  Loss:  66.30324889697292\n",
      "Epoch:  683  Loss:  66.30287748420389\n",
      "Epoch:  684  Loss:  66.30250715825217\n",
      "Epoch:  685  Loss:  66.30213791436101\n",
      "Epoch:  686  Loss:  66.30176974780146\n",
      "Epoch:  687  Loss:  66.30140265387206\n",
      "Epoch:  688  Loss:  66.3010366278985\n",
      "Epoch:  689  Loss:  66.30067166523372\n",
      "Epoch:  690  Loss:  66.30030776125743\n",
      "Epoch:  691  Loss:  66.29994491137609\n",
      "Epoch:  692  Loss:  66.2995831110226\n",
      "Epoch:  693  Loss:  66.29922235565621\n",
      "Epoch:  694  Loss:  66.29886264076232\n",
      "Epoch:  695  Loss:  66.29850396185222\n",
      "Epoch:  696  Loss:  66.29814631446293\n",
      "Epoch:  697  Loss:  66.29778969415705\n",
      "Epoch:  698  Loss:  66.29743409652264\n",
      "Epoch:  699  Loss:  66.29707951717286\n",
      "Epoch:  700  Loss:  66.29672595174588\n",
      "Epoch:  701  Loss:  66.29637339590477\n",
      "Epoch:  702  Loss:  66.29602184533728\n",
      "Epoch:  703  Loss:  66.29567129575561\n",
      "Epoch:  704  Loss:  66.2953217428963\n",
      "Epoch:  705  Loss:  66.29497318251997\n",
      "Epoch:  706  Loss:  66.29462561041133\n",
      "Epoch:  707  Loss:  66.29427902237882\n",
      "Epoch:  708  Loss:  66.29393341425452\n",
      "Epoch:  709  Loss:  66.29358878189402\n",
      "Epoch:  710  Loss:  66.29324512117618\n",
      "Epoch:  711  Loss:  66.29290242800303\n",
      "Epoch:  712  Loss:  66.2925606982996\n",
      "Epoch:  713  Loss:  66.29221992801364\n",
      "Epoch:  714  Loss:  66.29188011311571\n",
      "Epoch:  715  Loss:  66.29154124959878\n",
      "Epoch:  716  Loss:  66.29120333347817\n",
      "Epoch:  717  Loss:  66.29086636079144\n",
      "Epoch:  718  Loss:  66.29053032759813\n",
      "Epoch:  719  Loss:  66.29019522997974\n",
      "Epoch:  720  Loss:  66.28986106403941\n",
      "Epoch:  721  Loss:  66.28952782590194\n",
      "Epoch:  722  Loss:  66.28919551171353\n",
      "Epoch:  723  Loss:  66.28886411764171\n",
      "Epoch:  724  Loss:  66.28853363987503\n",
      "Epoch:  725  Loss:  66.28820407462321\n",
      "Epoch:  726  Loss:  66.28787541811668\n",
      "Epoch:  727  Loss:  66.28754766660664\n",
      "Epoch:  728  Loss:  66.28722081636487\n",
      "Epoch:  729  Loss:  66.2868948636835\n",
      "Epoch:  730  Loss:  66.28656980487503\n",
      "Epoch:  731  Loss:  66.28624563627206\n",
      "Epoch:  732  Loss:  66.28592235422721\n",
      "Epoch:  733  Loss:  66.28559995511297\n",
      "Epoch:  734  Loss:  66.28527843532159\n",
      "Epoch:  735  Loss:  66.2849577912649\n",
      "Epoch:  736  Loss:  66.28463801937421\n",
      "Epoch:  737  Loss:  66.2843191161002\n",
      "Epoch:  738  Loss:  66.28400107791269\n",
      "Epoch:  739  Loss:  66.28368390130068\n",
      "Epoch:  740  Loss:  66.28336758277197\n",
      "Epoch:  741  Loss:  66.28305211885339\n",
      "Epoch:  742  Loss:  66.28273750609024\n",
      "Epoch:  743  Loss:  66.28242374104659\n",
      "Epoch:  744  Loss:  66.28211082030487\n",
      "Epoch:  745  Loss:  66.28179874046579\n",
      "Epoch:  746  Loss:  66.28148749814837\n",
      "Epoch:  747  Loss:  66.28117708998963\n",
      "Epoch:  748  Loss:  66.28086751264452\n",
      "Epoch:  749  Loss:  66.28055876278592\n",
      "Epoch:  750  Loss:  66.2802508371044\n",
      "Epoch:  751  Loss:  66.27994373230814\n",
      "Epoch:  752  Loss:  66.27963744512277\n",
      "Epoch:  753  Loss:  66.27933197229126\n",
      "Epoch:  754  Loss:  66.279027310574\n",
      "Epoch:  755  Loss:  66.27872345674828\n",
      "Epoch:  756  Loss:  66.27842040760866\n",
      "Epoch:  757  Loss:  66.27811815996644\n",
      "Epoch:  758  Loss:  66.27781671064983\n",
      "Epoch:  759  Loss:  66.27751605650369\n",
      "Epoch:  760  Loss:  66.27721619438941\n",
      "Epoch:  761  Loss:  66.27691712118502\n",
      "Epoch:  762  Loss:  66.27661883378475\n",
      "Epoch:  763  Loss:  66.27632132909918\n",
      "Epoch:  764  Loss:  66.276024604055\n",
      "Epoch:  765  Loss:  66.27572865559502\n",
      "Epoch:  766  Loss:  66.27543348067792\n",
      "Epoch:  767  Loss:  66.27513907627824\n",
      "Epoch:  768  Loss:  66.27484543938628\n",
      "Epoch:  769  Loss:  66.27455256700803\n",
      "Epoch:  770  Loss:  66.27426045616487\n",
      "Epoch:  771  Loss:  66.27396910389373\n",
      "Epoch:  772  Loss:  66.27367850724684\n",
      "Epoch:  773  Loss:  66.27338866329173\n",
      "Epoch:  774  Loss:  66.27309956911094\n",
      "Epoch:  775  Loss:  66.27281122180214\n",
      "Epoch:  776  Loss:  66.27252361847793\n",
      "Epoch:  777  Loss:  66.27223675626574\n",
      "Epoch:  778  Loss:  66.27195063230778\n",
      "Epoch:  779  Loss:  66.2716652437609\n",
      "Epoch:  780  Loss:  66.27138058779654\n",
      "Epoch:  781  Loss:  66.27109666160058\n",
      "Epoch:  782  Loss:  66.27081346237325\n",
      "Epoch:  783  Loss:  66.27053098732918\n",
      "Epoch:  784  Loss:  66.27024923369709\n",
      "Epoch:  785  Loss:  66.26996819871982\n",
      "Epoch:  786  Loss:  66.26968787965427\n",
      "Epoch:  787  Loss:  66.26940827377126\n",
      "Epoch:  788  Loss:  66.26912937835543\n",
      "Epoch:  789  Loss:  66.26885119070516\n",
      "Epoch:  790  Loss:  66.26857370813252\n",
      "Epoch:  791  Loss:  66.26829692796318\n",
      "Epoch:  792  Loss:  66.26802084753625\n",
      "Epoch:  793  Loss:  66.2677454642043\n",
      "Epoch:  794  Loss:  66.26747077533317\n",
      "Epoch:  795  Loss:  66.26719677830202\n",
      "Epoch:  796  Loss:  66.26692347050306\n",
      "Epoch:  797  Loss:  66.2666508493417\n",
      "Epoch:  798  Loss:  66.26637891223623\n",
      "Epoch:  799  Loss:  66.26610765661792\n",
      "Epoch:  800  Loss:  66.26583707993086\n",
      "Epoch:  801  Loss:  66.2655671796319\n",
      "Epoch:  802  Loss:  66.26529795319051\n",
      "Epoch:  803  Loss:  66.26502939808881\n",
      "Epoch:  804  Loss:  66.26476151182146\n",
      "Epoch:  805  Loss:  66.26449429189549\n",
      "Epoch:  806  Loss:  66.26422773583027\n",
      "Epoch:  807  Loss:  66.26396184115757\n",
      "Epoch:  808  Loss:  66.26369660542129\n",
      "Epoch:  809  Loss:  66.26343202617748\n",
      "Epoch:  810  Loss:  66.26316810099424\n",
      "Epoch:  811  Loss:  66.2629048274516\n",
      "Epoch:  812  Loss:  66.26264220314165\n",
      "Epoch:  813  Loss:  66.26238022566821\n",
      "Epoch:  814  Loss:  66.26211889264687\n",
      "Epoch:  815  Loss:  66.2618582017049\n",
      "Epoch:  816  Loss:  66.26159815048126\n",
      "Epoch:  817  Loss:  66.26133873662644\n",
      "Epoch:  818  Loss:  66.26107995780234\n",
      "Epoch:  819  Loss:  66.26082181168233\n",
      "Epoch:  820  Loss:  66.26056429595116\n",
      "Epoch:  821  Loss:  66.26030740830473\n",
      "Epoch:  822  Loss:  66.26005114645028\n",
      "Epoch:  823  Loss:  66.25979550810607\n",
      "Epoch:  824  Loss:  66.25954049100152\n",
      "Epoch:  825  Loss:  66.259286092877\n",
      "Epoch:  826  Loss:  66.25903231148382\n",
      "Epoch:  827  Loss:  66.2587791445842\n",
      "Epoch:  828  Loss:  66.25852658995106\n",
      "Epoch:  829  Loss:  66.25827464536819\n",
      "Epoch:  830  Loss:  66.25802330862997\n",
      "Epoch:  831  Loss:  66.25777257754139\n",
      "Epoch:  832  Loss:  66.25752244991804\n",
      "Epoch:  833  Loss:  66.25727292358593\n",
      "Epoch:  834  Loss:  66.2570239963815\n",
      "Epoch:  835  Loss:  66.25677566615161\n",
      "Epoch:  836  Loss:  66.25652793075335\n",
      "Epoch:  837  Loss:  66.25628078805407\n",
      "Epoch:  838  Loss:  66.25603423593125\n",
      "Epoch:  839  Loss:  66.25578827227258\n",
      "Epoch:  840  Loss:  66.25554289497572\n",
      "Epoch:  841  Loss:  66.25529810194833\n",
      "Epoch:  842  Loss:  66.25505389110802\n",
      "Epoch:  843  Loss:  66.25481026038227\n",
      "Epoch:  844  Loss:  66.25456720770842\n",
      "Epoch:  845  Loss:  66.25432473103348\n",
      "Epoch:  846  Loss:  66.25408282831425\n",
      "Epoch:  847  Loss:  66.25384149751712\n",
      "Epoch:  848  Loss:  66.25360073661808\n",
      "Epoch:  849  Loss:  66.25336054360264\n",
      "Epoch:  850  Loss:  66.25312091646583\n",
      "Epoch:  851  Loss:  66.25288185321202\n",
      "Epoch:  852  Loss:  66.252643351855\n",
      "Epoch:  853  Loss:  66.25240541041788\n",
      "Epoch:  854  Loss:  66.25216802693296\n",
      "Epoch:  855  Loss:  66.25193119944183\n",
      "Epoch:  856  Loss:  66.25169492599515\n",
      "Epoch:  857  Loss:  66.25145920465273\n",
      "Epoch:  858  Loss:  66.25122403348337\n",
      "Epoch:  859  Loss:  66.2509894105649\n",
      "Epoch:  860  Loss:  66.25075533398402\n",
      "Epoch:  861  Loss:  66.25052180183641\n",
      "Epoch:  862  Loss:  66.25028881222653\n",
      "Epoch:  863  Loss:  66.25005636326762\n",
      "Epoch:  864  Loss:  66.24982445308166\n",
      "Epoch:  865  Loss:  66.2495930797993\n",
      "Epoch:  866  Loss:  66.24936224155982\n",
      "Epoch:  867  Loss:  66.24913193651108\n",
      "Epoch:  868  Loss:  66.24890216280951\n",
      "Epoch:  869  Loss:  66.24867291861997\n",
      "Epoch:  870  Loss:  66.24844420211579\n",
      "Epoch:  871  Loss:  66.24821601147866\n",
      "Epoch:  872  Loss:  66.24798834489859\n",
      "Epoch:  873  Loss:  66.24776120057398\n",
      "Epoch:  874  Loss:  66.24753457671132\n",
      "Epoch:  875  Loss:  66.24730847152541\n",
      "Epoch:  876  Loss:  66.24708288323917\n",
      "Epoch:  877  Loss:  66.24685781008361\n",
      "Epoch:  878  Loss:  66.24663325029776\n",
      "Epoch:  879  Loss:  66.24640920212877\n",
      "Epoch:  880  Loss:  66.24618566383164\n",
      "Epoch:  881  Loss:  66.24596263366935\n",
      "Epoch:  882  Loss:  66.24574010991273\n",
      "Epoch:  883  Loss:  66.24551809084046\n",
      "Epoch:  884  Loss:  66.24529657473897\n",
      "Epoch:  885  Loss:  66.24507555990247\n",
      "Epoch:  886  Loss:  66.2448550446329\n",
      "Epoch:  887  Loss:  66.24463502723974\n",
      "Epoch:  888  Loss:  66.24441550604018\n",
      "Epoch:  889  Loss:  66.24419647935896\n",
      "Epoch:  890  Loss:  66.24397794552834\n",
      "Epoch:  891  Loss:  66.24375990288806\n",
      "Epoch:  892  Loss:  66.2435423497853\n",
      "Epoch:  893  Loss:  66.24332528457467\n",
      "Epoch:  894  Loss:  66.24310870561807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  895  Loss:  66.24289261128477\n",
      "Epoch:  896  Loss:  66.24267699995139\n",
      "Epoch:  897  Loss:  66.24246187000163\n",
      "Epoch:  898  Loss:  66.24224721982648\n",
      "Epoch:  899  Loss:  66.2420330478241\n",
      "Epoch:  900  Loss:  66.24181935239972\n",
      "Epoch:  901  Loss:  66.24160613196568\n",
      "Epoch:  902  Loss:  66.24139338494135\n",
      "Epoch:  903  Loss:  66.24118110975309\n",
      "Epoch:  904  Loss:  66.24096930483421\n",
      "Epoch:  905  Loss:  66.240757968625\n",
      "Epoch:  906  Loss:  66.24054709957258\n",
      "Epoch:  907  Loss:  66.24033669613095\n",
      "Epoch:  908  Loss:  66.24012675676084\n",
      "Epoch:  909  Loss:  66.23991727992987\n",
      "Epoch:  910  Loss:  66.23970826411235\n",
      "Epoch:  911  Loss:  66.23949970778921\n",
      "Epoch:  912  Loss:  66.23929160944816\n",
      "Epoch:  913  Loss:  66.23908396758347\n",
      "Epoch:  914  Loss:  66.23887678069599\n",
      "Epoch:  915  Loss:  66.23867004729314\n",
      "Epoch:  916  Loss:  66.23846376588887\n",
      "Epoch:  917  Loss:  66.2382579350036\n",
      "Epoch:  918  Loss:  66.23805255316421\n",
      "Epoch:  919  Loss:  66.23784761890391\n",
      "Epoch:  920  Loss:  66.23764313076242\n",
      "Epoch:  921  Loss:  66.23743908728571\n",
      "Epoch:  922  Loss:  66.23723548702603\n",
      "Epoch:  923  Loss:  66.237032328542\n",
      "Epoch:  924  Loss:  66.23682961039843\n",
      "Epoch:  925  Loss:  66.23662733116633\n",
      "Epoch:  926  Loss:  66.23642548942286\n",
      "Epoch:  927  Loss:  66.23622408375138\n",
      "Epoch:  928  Loss:  66.2360231127413\n",
      "Epoch:  929  Loss:  66.2358225749881\n",
      "Epoch:  930  Loss:  66.23562246909339\n",
      "Epoch:  931  Loss:  66.2354227936646\n",
      "Epoch:  932  Loss:  66.2352235473154\n",
      "Epoch:  933  Loss:  66.23502472866514\n",
      "Epoch:  934  Loss:  66.23482633633928\n",
      "Epoch:  935  Loss:  66.23462836896906\n",
      "Epoch:  936  Loss:  66.23443082519152\n",
      "Epoch:  937  Loss:  66.23423370364968\n",
      "Epoch:  938  Loss:  66.23403700299222\n",
      "Epoch:  939  Loss:  66.23384072187359\n",
      "Epoch:  940  Loss:  66.233644858954\n",
      "Epoch:  941  Loss:  66.2334494128993\n",
      "Epoch:  942  Loss:  66.23325438238113\n",
      "Epoch:  943  Loss:  66.23305976607656\n",
      "Epoch:  944  Loss:  66.23286556266847\n",
      "Epoch:  945  Loss:  66.23267177084517\n",
      "Epoch:  946  Loss:  66.23247838930062\n",
      "Epoch:  947  Loss:  66.23228541673424\n",
      "Epoch:  948  Loss:  66.23209285185091\n",
      "Epoch:  949  Loss:  66.23190069336103\n",
      "Epoch:  950  Loss:  66.2317089399804\n",
      "Epoch:  951  Loss:  66.23151759043019\n",
      "Epoch:  952  Loss:  66.231326643437\n",
      "Epoch:  953  Loss:  66.23113609773279\n",
      "Epoch:  954  Loss:  66.23094595205477\n",
      "Epoch:  955  Loss:  66.23075620514543\n",
      "Epoch:  956  Loss:  66.23056685575256\n",
      "Epoch:  957  Loss:  66.23037790262926\n",
      "Epoch:  958  Loss:  66.23018934453368\n",
      "Epoch:  959  Loss:  66.23000118022927\n",
      "Epoch:  960  Loss:  66.22981340848455\n",
      "Epoch:  961  Loss:  66.22962602807324\n",
      "Epoch:  962  Loss:  66.22943903777411\n",
      "Epoch:  963  Loss:  66.22925243637104\n",
      "Epoch:  964  Loss:  66.22906622265285\n",
      "Epoch:  965  Loss:  66.22888039541358\n",
      "Epoch:  966  Loss:  66.22869495345205\n",
      "Epoch:  967  Loss:  66.2285098955722\n",
      "Epoch:  968  Loss:  66.22832522058282\n",
      "Epoch:  969  Loss:  66.2281409272977\n",
      "Epoch:  970  Loss:  66.2279570145354\n",
      "Epoch:  971  Loss:  66.22777348111951\n",
      "Epoch:  972  Loss:  66.22759032587834\n",
      "Epoch:  973  Loss:  66.22740754764503\n",
      "Epoch:  974  Loss:  66.22722514525753\n",
      "Epoch:  975  Loss:  66.22704311755861\n",
      "Epoch:  976  Loss:  66.2268614633957\n",
      "Epoch:  977  Loss:  66.22668018162096\n",
      "Epoch:  978  Loss:  66.22649927109133\n",
      "Epoch:  979  Loss:  66.2263187306683\n",
      "Epoch:  980  Loss:  66.22613855921807\n",
      "Epoch:  981  Loss:  66.2259587556115\n",
      "Epoch:  982  Loss:  66.22577931872395\n",
      "Epoch:  983  Loss:  66.2256002474355\n",
      "Epoch:  984  Loss:  66.22542154063056\n",
      "Epoch:  985  Loss:  66.22524319719835\n",
      "Epoch:  986  Loss:  66.22506521603238\n",
      "Epoch:  987  Loss:  66.2248875960307\n",
      "Epoch:  988  Loss:  66.22471033609591\n",
      "Epoch:  989  Loss:  66.22453343513492\n",
      "Epoch:  990  Loss:  66.22435689205913\n",
      "Epoch:  991  Loss:  66.22418070578432\n",
      "Epoch:  992  Loss:  66.22400487523068\n",
      "Epoch:  993  Loss:  66.22382939932268\n",
      "Epoch:  994  Loss:  66.22365427698912\n",
      "Epoch:  995  Loss:  66.22347950716319\n",
      "Epoch:  996  Loss:  66.22330508878225\n",
      "Epoch:  997  Loss:  66.22313102078806\n",
      "Epoch:  998  Loss:  66.22295730212649\n",
      "Epoch:  999  Loss:  66.22278393174774\n",
      "\n",
      "The predicted context words are ['computer', 'professors', 'program', 'great', 'department']\n"
     ]
    }
   ],
   "source": [
    "text_corpus = \"Welcome students to the Department of Computer Science. We have great faculty and professors. We will have a welcome program today.\"\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Data preprocessing  \n",
    "training_data = preprocessing(text_corpus)\n",
    "\n",
    "# Word2Vec\n",
    "w2v = word2vec()\n",
    "\n",
    "# Train the model\n",
    "w2v.train(epochs)\n",
    "\n",
    "# Predict using model  \n",
    "print('\\nThe predicted context words are', w2v.predict(\"welcome\", 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
